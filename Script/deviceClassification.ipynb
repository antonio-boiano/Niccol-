{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and plotlib kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import warnings\n",
    "import Levenshtein as lev\n",
    "import tsfresh\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import mpldatacursor\n",
    "mpldatacursor.datacursor()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tsfresh\n",
    "tsfresh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tikzplotlib\n",
    "\n",
    "def save_tikzplotlib(fig,path):\n",
    "    def tikzplotlib_fix_ncols(obj):\n",
    "        \"\"\"\n",
    "        workaround for matplotlib 3.6 renamed legend's _ncol to _ncols, which breaks tikzplotlib\n",
    "        \"\"\"\n",
    "        if hasattr(obj, \"_ncols\"):\n",
    "            obj._ncol = obj._ncols\n",
    "        for child in obj.get_children():\n",
    "            tikzplotlib_fix_ncols(child)\n",
    "    tikzplotlib_fix_ncols(fig)\n",
    "    tikzplotlib.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping(version):\n",
    "    \n",
    "    global device_name_mapping \n",
    "    global device_type_mapping \n",
    "    global devices_name    \n",
    "    global devices\n",
    "    \n",
    "    if version == 2:\n",
    "\n",
    "        devices = ['0x0000', '0x265e', '0x3181', '0x0a79', '0xb815', '0x4e52', '0x9989', '0x5db6', '0x772c', '0x61af', '0xe011', '0xe694','0x6e5f', '0xc8f0', '0x09ac', '0x82eb', '0x054f', '0xebe5',  '0xe5c4', '0x482d', '0xa209', '0x2cae'];\n",
    "\n",
    "        devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', \n",
    "                        'Aqara Button', 'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb',  'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "\n",
    "        device_type_mapping = {\n",
    "            '0x0000' : 'Coordinator',\n",
    "            '0x09ac' : 'Socket',\n",
    "            '0x5db6' : 'Door',\n",
    "            '0xe694' : 'Socket',\n",
    "            '0x772c' : 'Door',\n",
    "            '0x61af' : 'Vibration',\n",
    "            '0xe011' : 'Button',\n",
    "            '0x9989' : 'Motion',\n",
    "            '0x265e' : 'Temperature',\n",
    "            '0xb815' : 'Motion',\n",
    "            '0x4e52' : 'Motion',\n",
    "            '0x3181' : 'Door',\n",
    "            '0x0a79' : 'Door',\n",
    "            '0x82eb' : 'Socket',\n",
    "            '0x6e5f' : 'Socket',\n",
    "            '0xc8f0' : 'Socket',\n",
    "            '0xebe5' : 'Bulb',\n",
    "            '0x054f' : 'Bulb',\n",
    "            '0x482d' : 'Bulb',\n",
    "            '0xa209' : 'Bulb',\n",
    "            '0x2cae' : 'Motion',\n",
    "            '0xe5c4' : 'Bulb'\n",
    "        }\n",
    "\n",
    "        device_name_mapping = {\n",
    "            '0x0000' : 'Coordinator',\n",
    "            '0x09ac' : 'Ledvance Z3 Plug',\n",
    "            '0x5db6' : 'Aqara Door',\n",
    "            '0xe694' : 'Smart Socket',\n",
    "            '0x772c' : 'Aqara Door',\n",
    "            '0x61af' : 'Aqara Vibration',\n",
    "            '0xe011' : 'Aqara Button',\n",
    "            '0x9989' : 'Aqara Motion',\n",
    "            '0x265e' : 'Sonoff Temperature',\n",
    "            '0xb815' : 'Sonoff Motion',\n",
    "            '0x4e52' : 'Sonoff Motion',\n",
    "            '0x3181' : 'Sonoff Door',\n",
    "            '0x0a79' : 'Sonoff Door',\n",
    "            '0x82eb' : 'Ledvance Smart+ Plug',\n",
    "            '0x6e5f' : 'Power Plug',\n",
    "            '0xc8f0' : 'Power Plug',\n",
    "            '0xebe5' : 'Moes Bulb',\n",
    "            '0x054f' : 'Ledvance Bulb',\n",
    "            '0x482d' : 'Philips Lamp',\n",
    "            '0xa209' : 'Philips Lamp',\n",
    "            '0x2cae' : 'Philips Lamp',\n",
    "            '0xe5c4' : 'Philips Motion'\n",
    "        }\n",
    "    else:\n",
    "        devices = ['0x0000', '0x4615', '0x946e', '0x7b10', '0xd0bb', '0x1f29', '0x27d7', '0x907b', '0xe01d', '0x187a', '0xc31c',\n",
    "        '0xe1a6', '0x3d95', '0xa706', '0x4e11', '0x0112', '0xec7f', '0x1e15', '0x1cd8', '0x5bb9', '0x711c', '0x059b']\n",
    "\n",
    "        devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', 'Aqara Button',\n",
    "                        'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb', 'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "        device_type_mapping = {\n",
    "            '0x0000': 'Coordinator',\n",
    "            '0x4615': 'Temperature',\n",
    "            '0x946e': 'Door',\n",
    "            '0x7b10': 'Door',\n",
    "            '0xd0bb': 'Motion',\n",
    "            '0x1f29': 'Motion',\n",
    "            '0x27d7': 'Motion',\n",
    "            '0x907b': 'Door',\n",
    "            '0xe01d': 'Door',\n",
    "            '0x187a': 'Vibration',\n",
    "            '0xc31c': 'Button',\n",
    "            '0xe1a6': 'Socket',\n",
    "            '0x3d95': 'Socket',\n",
    "            '0xa706': 'Socket',\n",
    "            '0x4e11': 'Socket',\n",
    "            '0x0112': 'Socket',\n",
    "            '0xec7f': 'Bulb',\n",
    "            '0x1e15': 'Bulb',\n",
    "            '0x1cd8': 'Bulb',\n",
    "            '0x5bb9': 'Bulb',\n",
    "            '0x711c': 'Bulb',\n",
    "            '0x059b': 'Motion'    \n",
    "        }\n",
    "\n",
    "        device_name_mapping = {\n",
    "            '0x0000': 'Coordinator',\n",
    "            '0x4615': 'Sonoff Temperature',\n",
    "            '0x946e': 'Sonoff Door',\n",
    "            '0x7b10': 'Sonoff Door',\n",
    "            '0xd0bb': 'Sonoff Motion',\n",
    "            '0x1f29': 'Sonoff Motion',\n",
    "            '0x27d7': 'Aqara Motion',\n",
    "            '0x907b': 'Aqara Door',\n",
    "            '0xe01d': 'Aqara Door',\n",
    "            '0x187a': 'Aqara Vibration',\n",
    "            '0xc31c': 'Aqara Button',\n",
    "            '0xe1a6': 'Smart Socket',\n",
    "            '0x3d95': 'Power Plug',\n",
    "            '0xa706': 'Power Plug',\n",
    "            '0x4e11': 'Ledvance Z3 Plug',\n",
    "            '0x0112': 'Ledvance Smart+ Plug',\n",
    "            '0xec7f': 'Ledvance Bulb',\n",
    "            '0x1e15': 'Moes Bulb',\n",
    "            '0x1cd8': 'Philips Lamp',\n",
    "            '0x5bb9': 'Philips Lamp',\n",
    "            '0x711c': 'Philips Lamp',\n",
    "            '0x059b': 'Philips Motion' \n",
    "        }\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority ranking for commands\n",
    "\n",
    "command_priorities = {\n",
    "    'Sonoff Temperature': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Sonoff Door': {\n",
    "        'Zone Status Change Notification (0x00)': 3,\n",
    "        'Report Attributes (0x0a)': 2,\n",
    "    },\n",
    "\n",
    "    'Sonoff Motion': {\n",
    "        'Zone Status Change Notification (0x00)': 7,\n",
    "        'Report Attributes (0x0a)': 6,\n",
    "        'Zone Enroll Request (0x01)': 5,\n",
    "        'Rejoin Request (0x06)': 4,\n",
    "        'zdp': 3,\n",
    "        'Ack': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Aqara Motion': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Aqara Door': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Aqara Vibration': {\n",
    "        'Report Attributes (0x0a)': 5,\n",
    "        'Rejoin Request (0x06)': 4,\n",
    "        'zdp': 3,\n",
    "        'Ack': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Aqara Button': {\n",
    "        'Report Attributes (0x0a)': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Smart Socket': {\n",
    "        'Report Attributes (0x0a)': 9,\n",
    "        'Read Attributes Response (0x01)': 9,\n",
    "        'Read Attributes (0x00)': 8,\n",
    "        'Default Response (0x0b)': 7,\n",
    "        'APS: Ack': 6,\n",
    "        'Ack': 5,\n",
    "        'Rejoin Response (0x07)': 4,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'zdp': 2,\n",
    "        'Link Status': 1,\n",
    "    },\n",
    "\n",
    "    'Power Plug': {\n",
    "        'Read Attribute Response (0x01)': 9,\n",
    "        'Report Attributes (0x0a)': 9,\n",
    "        'Read Attributes (0x00)': 8,\n",
    "        'Default Response (0x0b)': 7,\n",
    "        'APS: Ack': 6,\n",
    "        'Ack': 5,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'zdp': 2,\n",
    "        'Link Status': 1,\n",
    "    },\n",
    "\n",
    "    'Ledvance Z3 Plug': {\n",
    "        'Link Status': 1,\n",
    "        'Route Record (0x05)': 6,\n",
    "        'Ack': 7,\n",
    "        'Report Attributes (0x0a)': 10,\n",
    "        'APS: Ack': 8,\n",
    "        'zdp': 2,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Get Group Membership Response (0x02)': 5,\n",
    "    },\n",
    "\n",
    "    'Ledvance Smart+ Plug': {\n",
    "        'Link Status': 2,\n",
    "        'Report Attributes (0x0a)': 10,\n",
    "        'Ack': 7,\n",
    "        'zdp': 3,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 4,\n",
    "        '---': 1,\n",
    "        'Get Group Membership Response (0x02)': 6,\n",
    "        'APS: Ack': 8,\n",
    "        'Rejoin Response (0x07)': 5,\n",
    "    },\n",
    "\n",
    "    'Ledvance Bulb': {\n",
    "        'Link Status': 1,\n",
    "        'Ack': 7,\n",
    "        'Route Record (0x05)': 6,\n",
    "        'APS: Ack': 8,\n",
    "        'zdp': 2,\n",
    "        'Report Attributes (0x0a)': 11,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Get Group Membership Response (0x02)': 5,\n",
    "    },\n",
    "\n",
    "    'Moes Bulb': {\n",
    "        'Link Status': 1,\n",
    "        'Ack': 4,\n",
    "        'Default Response (0x0b)': 8,\n",
    "        'Read Attribute Response (0x01)': 6,\n",
    "        'Report Attributes (0x0a)': 7,\n",
    "        'APS: Ack': 5,\n",
    "        'zdp': 2,\n",
    "        'Route Reply (0x02)': 3,\n",
    "    },\n",
    "\n",
    "    'Philips Lamp': {\n",
    "        'Link Status': 2,\n",
    "        'Route Record (0x05)': 7,\n",
    "        'zdp': 3,\n",
    "        'Ack': 8,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Report Attributes (0x0a)': 11,\n",
    "        'APS: Ack': 9,\n",
    "        'Default Response (0x0b)': 12,\n",
    "        'Get Group Membership Response (0x02)': 6,\n",
    "        'Route Reply (0x02)': 5,\n",
    "        'Rejoin Response (0x07)': 4,\n",
    "        '---': 1,\n",
    "    },\n",
    "\n",
    "    'Philips Motion': {\n",
    "        'Report Attributes (0x0a)': 6,\n",
    "        'APS: Ack': 5,\n",
    "        'Route Record (0x05)': 2,\n",
    "        'Ack': 4,\n",
    "        'zdp': 1,\n",
    "        'Rejoin Request (0x06)': 3,\n",
    "    },\n",
    "\n",
    "    'Coordinator': {\n",
    "        'Read Attributes (0x00)': 11,\n",
    "        'Ack': 7,\n",
    "        'APS: Ack': 8,\n",
    "        'Link Status': 1,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'zdp': 2,\n",
    "        'On (0x01)': 12,\n",
    "        'Move To Level with OnOff (0x04)': 14,\n",
    "        'Off (0x00)': 12,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Move To Color (0x07)': 13,\n",
    "        'Get Group Membership (0x02)': 4,\n",
    "        'Move To Color Temperature (0x0a)': 14,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Configure Reporting (0x06)': 5,\n",
    "        'Color Loop Set (0x44)': 15,\n",
    "        'Rejoin Response (0x07)': 6,\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_highest_priority_action(device_name, action_list):\n",
    "    if device_name in command_priorities:\n",
    "        # Get the priorities for the specified device\n",
    "        device_priorities = command_priorities[device_name]\n",
    "\n",
    "        # Filter the action list to include only those present in the priorities\n",
    "        valid_actions = [action for action in action_list if action in device_priorities]\n",
    "\n",
    "        if valid_actions:\n",
    "            # Find the action with the highest priority\n",
    "            highest_priority_action = max(valid_actions, key=lambda action: device_priorities[action])\n",
    "            return highest_priority_action\n",
    "        else:\n",
    "            return '---'  # No valid actions found in the priorities\n",
    "    else:\n",
    "        return None  # Device not found in the command_priorities dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load acquisition 2\n",
    "\n",
    "df = pd.read_csv(r'../final_dataFrame_v2_anto.csv')\n",
    "\n",
    "ground_truth_df = pd.read_csv(r'../ground_truth_v2.csv')\n",
    "ground_truth_df = ground_truth_df.rename(columns={'0': 'Action'})\n",
    "if 'Unnamed: 0' in ground_truth_df.columns:\n",
    "    ground_truth_df = ground_truth_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)\n",
    "\n",
    "devices = ['0x0000', '0x265e', '0x3181', '0x0a79', '0xb815', '0x4e52', '0x9989', '0x5db6', '0x772c', '0x61af', '0xe011', '0xe694',\n",
    "            '0x6e5f', '0xc8f0', '0x09ac', '0x82eb', '0x054f', '0xebe5',  '0xe5c4', '0x482d', '0xa209', '0x2cae']\n",
    "\n",
    "devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', \n",
    "                'Aqara Button', 'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb',  'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "device_type_mapping = {\n",
    "    '0x0000' : 'Coordinator',\n",
    "    '0x09ac' : 'Socket',\n",
    "    '0x5db6' : 'Door',\n",
    "    '0xe694' : 'Socket',\n",
    "    '0x772c' : 'Door',\n",
    "    '0x61af' : 'Vibration',\n",
    "    '0xe011' : 'Button',\n",
    "    '0x9989' : 'Motion',\n",
    "    '0x265e' : 'Temperature',\n",
    "    '0xb815' : 'Motion',\n",
    "    '0x4e52' : 'Motion',\n",
    "    '0x3181' : 'Door',\n",
    "    '0x0a79' : 'Door',\n",
    "    '0x82eb' : 'Socket',\n",
    "    '0x6e5f' : 'Socket',\n",
    "    '0xc8f0' : 'Socket',\n",
    "    '0xebe5' : 'Bulb',\n",
    "    '0x054f' : 'Bulb',\n",
    "    '0x482d' : 'Bulb',\n",
    "    '0xa209' : 'Bulb',\n",
    "    '0x2cae' : 'Motion',\n",
    "    '0xe5c4' : 'Bulb'\n",
    "}\n",
    "\n",
    "device_name_mapping = {\n",
    "    '0x0000' : 'Coordinator',\n",
    "    '0x09ac' : 'Ledvance Z3 Plug',\n",
    "    '0x5db6' : 'Aqara Door',\n",
    "    '0xe694' : 'Smart Socket',\n",
    "    '0x772c' : 'Aqara Door',\n",
    "    '0x61af' : 'Aqara Vibration',\n",
    "    '0xe011' : 'Aqara Button',\n",
    "    '0x9989' : 'Aqara Motion',\n",
    "    '0x265e' : 'Sonoff Temperature',\n",
    "    '0xb815' : 'Sonoff Motion',\n",
    "    '0x4e52' : 'Sonoff Motion',\n",
    "    '0x3181' : 'Sonoff Door',\n",
    "    '0x0a79' : 'Sonoff Door',\n",
    "    '0x82eb' : 'Ledvance Smart+ Plug',\n",
    "    '0x6e5f' : 'Power Plug',\n",
    "    '0xc8f0' : 'Power Plug',\n",
    "    '0xebe5' : 'Moes Bulb',\n",
    "    '0x054f' : 'Ledvance Bulb',\n",
    "    '0x482d' : 'Philips Lamp',\n",
    "    '0xa209' : 'Philips Lamp',\n",
    "    '0x2cae' : 'Philips Lamp',\n",
    "    '0xe5c4' : 'Philips Motion'\n",
    "}\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=71482)\n",
    "threshold_float = 0.071482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Extended Source'])\n",
    "y = df['Extended Source']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.to_csv('train.csv', index=False)\n",
    "X_test.to_csv('test.csv', index=False)\n",
    "y_train.to_csv('train_labels.csv', index=False)\n",
    "y_test.to_csv('test_labels.csv', index=False)\n",
    "\n",
    "X_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "y_test = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add source zigbee and destination zigbee to ack packets\n",
    "seq_no = df.loc[0,'Sequence Number']\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    seq_no1 = df.loc[i,'Sequence Number']\n",
    "    if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "        df.loc[i,'Source Zigbee'] = df.loc[i-1,'Destination Zigbee']\n",
    "        df.loc[i,'Destination Zigbee'] = df.loc[i-1,'Source Zigbee']\n",
    "    seq_no = seq_no1\n",
    "    if(np.isnan(df.loc[i, 'Payload Length'])):\n",
    "        df.loc[i, 'Payload Length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_dataFrame_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframes 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load acquisition 4\n",
    "\n",
    "df = pd.read_csv(r'../final_dataFrame_v4_anto.csv')\n",
    "\n",
    "ground_truth_df = pd.read_csv(r'../ground_truth_v4.csv')\n",
    "ground_truth_df = ground_truth_df.rename(columns={'0': 'Action'})\n",
    "if 'Unnamed: 0' in ground_truth_df.columns:\n",
    "    ground_truth_df = ground_truth_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)\n",
    "\n",
    "devices = ['0x0000', '0x4615', '0x946e', '0x7b10', '0xd0bb', '0x1f29', '0x27d7', '0x907b', '0xe01d', '0x187a', '0xc31c',\n",
    "           '0xe1a6', '0x3d95', '0xa706', '0x4e11', '0x0112', '0xec7f', '0x1e15', '0x1cd8', '0x5bb9', '0x711c', '0x059b']\n",
    "\n",
    "devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', 'Aqara Button',\n",
    "                'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb', 'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "device_type_mapping = {\n",
    "    '0x0000': 'Coordinator',\n",
    "    '0x4615': 'Temperature',\n",
    "    '0x946e': 'Door',\n",
    "    '0x7b10': 'Door',\n",
    "    '0xd0bb': 'Motion',\n",
    "    '0x1f29': 'Motion',\n",
    "    '0x27d7': 'Motion',\n",
    "    '0x907b': 'Door',\n",
    "    '0xe01d': 'Door',\n",
    "    '0x187a': 'Vibration',\n",
    "    '0xc31c': 'Button',\n",
    "    '0xe1a6': 'Socket',\n",
    "    '0x3d95': 'Socket',\n",
    "    '0xa706': 'Socket',\n",
    "    '0x4e11': 'Socket',\n",
    "    '0x0112': 'Socket',\n",
    "    '0xec7f': 'Bulb',\n",
    "    '0x1e15': 'Bulb',\n",
    "    '0x1cd8': 'Bulb',\n",
    "    '0x5bb9': 'Bulb',\n",
    "    '0x711c': 'Bulb',\n",
    "    '0x059b': 'Motion'    \n",
    "}\n",
    "\n",
    "device_name_mapping = {\n",
    "    '0x0000': 'Coordinator',\n",
    "    '0x4615': 'Sonoff Temperature',\n",
    "    '0x946e': 'Sonoff Door',\n",
    "    '0x7b10': 'Sonoff Door',\n",
    "    '0xd0bb': 'Sonoff Motion',\n",
    "    '0x1f29': 'Sonoff Motion',\n",
    "    '0x27d7': 'Aqara Motion',\n",
    "    '0x907b': 'Aqara Door',\n",
    "    '0xe01d': 'Aqara Door',\n",
    "    '0x187a': 'Aqara Vibration',\n",
    "    '0xc31c': 'Aqara Button',\n",
    "    '0xe1a6': 'Smart Socket',\n",
    "    '0x3d95': 'Power Plug',\n",
    "    '0xa706': 'Power Plug',\n",
    "    '0x4e11': 'Ledvance Z3 Plug',\n",
    "    '0x0112': 'Ledvance Smart+ Plug',\n",
    "    '0xec7f': 'Ledvance Bulb',\n",
    "    '0x1e15': 'Moes Bulb',\n",
    "    '0x1cd8': 'Philips Lamp',\n",
    "    '0x5bb9': 'Philips Lamp',\n",
    "    '0x711c': 'Philips Lamp',\n",
    "    '0x059b': 'Philips Motion' \n",
    "}\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=63446)\n",
    "threshold_float = 0.063446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Extended Source'])\n",
    "y = df['Extended Source']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.to_csv('train.csv', index=False)\n",
    "X_test.to_csv('test.csv', index=False)\n",
    "y_train.to_csv('train_labels.csv', index=False)\n",
    "y_test.to_csv('test_labels.csv', index=False)\n",
    "\n",
    "X_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "y_test = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add source zigbee and destination zigbee to ack packets\n",
    "seq_no = df.loc[0,'Sequence Number']\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    seq_no1 = df.loc[i,'Sequence Number']\n",
    "    if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "        df.loc[i,'Source Zigbee'] = df.loc[i-1,'Destination Zigbee']\n",
    "        df.loc[i,'Destination Zigbee'] = df.loc[i-1,'Source Zigbee']\n",
    "    seq_no = seq_no1\n",
    "    if(np.isnan(df.loc[i, 'Payload Length'])):\n",
    "        df.loc[i, 'Payload Length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_dataFrame_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAT Density Plot V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>Delta Time</th>\n",
       "      <th>Length</th>\n",
       "      <th>Source IEEE</th>\n",
       "      <th>Destination IEEE</th>\n",
       "      <th>FCF IEEE</th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Source Zigbee</th>\n",
       "      <th>Destination Zigbee</th>\n",
       "      <th>FCF Zigbee</th>\n",
       "      <th>Payload Length</th>\n",
       "      <th>Extended Source</th>\n",
       "      <th>File</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34068</td>\n",
       "      <td>Oct 12, 2023 17:16:36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77</td>\n",
       "      <td>0x482d</td>\n",
       "      <td>0xffff</td>\n",
       "      <td>0x8841</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0x482d</td>\n",
       "      <td>0xfffc</td>\n",
       "      <td>0x1209</td>\n",
       "      <td>32.0</td>\n",
       "      <td>00:17:88:01:06:e3:0a:f3</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Ack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34069</td>\n",
       "      <td>Oct 12, 2023 17:16:38.715568</td>\n",
       "      <td>2.715568</td>\n",
       "      <td>12</td>\n",
       "      <td>0x2cae</td>\n",
       "      <td>0x482d</td>\n",
       "      <td>0x8863</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Read Attributes (0x00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34070</td>\n",
       "      <td>Oct 12, 2023 17:16:38.716335</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x0002</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Ack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34071</td>\n",
       "      <td>Oct 12, 2023 17:16:40.084308</td>\n",
       "      <td>1.367973</td>\n",
       "      <td>68</td>\n",
       "      <td>0xebe5</td>\n",
       "      <td>0xffff</td>\n",
       "      <td>0x8841</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0xebe5</td>\n",
       "      <td>0xfffc</td>\n",
       "      <td>0x1209</td>\n",
       "      <td>23.0</td>\n",
       "      <td>a4:c1:38:86:85:3b:80:7a</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Read Attributes Response (0x01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34072</td>\n",
       "      <td>Oct 12, 2023 17:16:41.091075</td>\n",
       "      <td>1.006767</td>\n",
       "      <td>77</td>\n",
       "      <td>0x82eb</td>\n",
       "      <td>0xffff</td>\n",
       "      <td>0x8841</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0x82eb</td>\n",
       "      <td>0xfffc</td>\n",
       "      <td>0x1209</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7c:b0:3e:aa:0a:08:56:75</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Ack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          Time  Delta Time  Length Source IEEE  \\\n",
       "0       34068  Oct 12, 2023 17:16:36.000000    0.000000      77      0x482d   \n",
       "1       34069  Oct 12, 2023 17:16:38.715568    2.715568      12      0x2cae   \n",
       "2       34070  Oct 12, 2023 17:16:38.716335    0.000767       5         NaN   \n",
       "3       34071  Oct 12, 2023 17:16:40.084308    1.367973      68      0xebe5   \n",
       "4       34072  Oct 12, 2023 17:16:41.091075    1.006767      77      0x82eb   \n",
       "\n",
       "  Destination IEEE FCF IEEE  Sequence Number Source Zigbee Destination Zigbee  \\\n",
       "0           0xffff   0x8841             79.0        0x482d             0xfffc   \n",
       "1           0x482d   0x8863            225.0           NaN                NaN   \n",
       "2              NaN   0x0002            225.0           NaN                NaN   \n",
       "3           0xffff   0x8841             79.0        0xebe5             0xfffc   \n",
       "4           0xffff   0x8841             37.0        0x82eb             0xfffc   \n",
       "\n",
       "  FCF Zigbee  Payload Length          Extended Source          File  \\\n",
       "0     0x1209            32.0  00:17:88:01:06:e3:0a:f3  idle2.pcapng   \n",
       "1        NaN             0.0                      NaN  idle2.pcapng   \n",
       "2        NaN             0.0                      NaN  idle2.pcapng   \n",
       "3     0x1209            23.0  a4:c1:38:86:85:3b:80:7a  idle2.pcapng   \n",
       "4     0x1209            32.0  7c:b0:3e:aa:0a:08:56:75  idle2.pcapng   \n",
       "\n",
       "                            Action  \n",
       "0                              Ack  \n",
       "1           Read Attributes (0x00)  \n",
       "2                              Ack  \n",
       "3  Read Attributes Response (0x01)  \n",
       "4                              Ack  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df = df[df['File'].str.contains('idle')]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95th percentile value is 2505.41 microseconds\n",
      "The total number of Ack in the DataFrame is 15018\n"
     ]
    }
   ],
   "source": [
    "## Calculate the time delta between packets with the same sequence number (packet and corresponding ack)\n",
    "df.sort_values(by=['Time'])\n",
    "seq_no = df.loc[0,'Sequence Number']\n",
    "date_string = df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    seq_no1 = df.loc[i,'Sequence Number']\n",
    "    date_string = df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1\n",
    "    seq_no = seq_no1\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=20000)\n",
    "second_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=0)\n",
    "filtered_timedelta = [td for td in list_of_timedelta if td < threshold]\n",
    "filtered_timedelta = [td for td in filtered_timedelta if td > second_threshold]\n",
    "filtered_timedelta_micro = [td.microseconds for td in filtered_timedelta]\n",
    "\n",
    "plt.hist(filtered_timedelta_micro, bins=100, edgecolor='k')\n",
    "plt.xlabel('Time in Microseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values for Ack Packets')\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_micro, bins=100)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "percentile_value = bin_edges[percentile_bin + 1]\n",
    "print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "number_of_ack = len(df[df['FCF IEEE'] == '0x0002'])\n",
    "print(f\"The total number of Ack in the DataFrame is {number_of_ack}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95th percentile value is 76711.66313333334 Milliseconds\n",
      "Mean: 8262.730636601005, Std: 78476.8103939541\n"
     ]
    }
   ],
   "source": [
    "## Calculate the time delta between packets\n",
    "df.sort_values(by=['Time'])\n",
    "date_string = df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "SOURCE = 'Source IEEE' #Source IEEE Zigbee\n",
    "\n",
    "for dev in df[SOURCE].unique():\n",
    "#    print(dev)\n",
    "    df_dev = df[df[SOURCE] == dev]\n",
    "    if len(df_dev) >0:\n",
    "        df_dev.reset_index(drop=True, inplace=True)\n",
    "        date_string = df_dev.loc[0]['Time']\n",
    "        date_object = datetime.strptime(date_string, date_format)\n",
    "        for i in range(1, len(df_dev)):\n",
    "            date_string = df_dev.iloc[i]['Time']\n",
    "            date_object1 = datetime.strptime(date_string, date_format)\n",
    "            list_of_timedelta.append(date_object1 - date_object)\n",
    "            date_object = date_object1\n",
    "\n",
    "threshold = timedelta(days=0, hours=10, seconds=40, microseconds=150000)\n",
    "second_threshold = timedelta(days=0)\n",
    "\n",
    "filtered_timedelta = [td for td in list_of_timedelta if td < threshold and td > second_threshold]\n",
    "\n",
    "filtered_timedelta_milli = [td.total_seconds()*1000 for td in filtered_timedelta]\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.hist(filtered_timedelta_milli, bins=60, edgecolor='k')\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values')\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_milli, bins=60)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "percentile_bin = np.where(cdf > 0.97)[0][0]\n",
    "\n",
    "percentile_value = bin_edges[percentile_bin + 1]\n",
    "print(f\"The 95th percentile value is {percentile_value} Milliseconds\")\n",
    "\n",
    "import statistics\n",
    "mean = statistics.mean(filtered_timedelta_milli)\n",
    "std = statistics.stdev(filtered_timedelta_milli)\n",
    "\n",
    "print (f\"Mean: {mean}, Std: {std}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "## Density plot\n",
    "\n",
    "sns.kdeplot(filtered_timedelta_milli, bw_method=0.1)\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Timedelta Values')\n",
    "#save_tikzplotlib(plt.gcf(), \"threshold.tex\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 packets for device 0x265e\n",
      "The 95th percentile value is 8727.75 microseconds\n"
     ]
    }
   ],
   "source": [
    "## Calculate the time delta between packets ingoing or outgoing for a specific device\n",
    "\n",
    "device_address = '0x265e'\n",
    "\n",
    "device_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "print(f\"{len(device_df)} packets for device {device_address}\")\n",
    "\n",
    "device_df.sort_values(by=['Time'])\n",
    "device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "date_string = device_df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(device_df)):\n",
    "    date_string = device_df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=150000)\n",
    "second_threshold = timedelta(days=0)\n",
    "filtered_timedelta_ledvance = [td for td in list_of_timedelta if td < threshold]\n",
    "filtered_timedelta_ledvance = [td for td in filtered_timedelta_ledvance if td > second_threshold]\n",
    "filtered_timedelta_micro_ledvance = [td.microseconds for td in filtered_timedelta_ledvance]\n",
    "\n",
    "plt.hist(filtered_timedelta_micro_ledvance, bins=60, edgecolor='k')\n",
    "plt.xlabel('Time in Microseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values')\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_micro_ledvance, bins=60)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "percentile_value = bin_edges[percentile_bin + 1]\n",
    "print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x0000\n",
      "The dataframe is made up by 7547 packets\n",
      "Filtered dataframe is made up by 5516 packets\n",
      "66.972 microseconds\n",
      "\n",
      "0x265e\n",
      "The dataframe is made up by 50 packets\n",
      "Filtered dataframe is made up by 4 packets\n",
      "22.3 microseconds\n",
      "\n",
      "0x3181\n",
      "The dataframe is made up by 3 packets\n",
      "Filtered dataframe is made up by 1 packets\n",
      "3.861 microseconds\n",
      "\n",
      "0x0a79\n",
      "The dataframe is made up by 7 packets\n",
      "Filtered dataframe is made up by 5 packets\n",
      "10.645 microseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1566110/1513759939.py:41: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address, color=colormap(devices.index(device_address)))\n",
      "/tmp/ipykernel_1566110/1513759939.py:41: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address, color=colormap(devices.index(device_address)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0xb815\n",
      "The dataframe is made up by 7 packets\n",
      "Filtered dataframe is made up by 3 packets\n",
      "5.736 microseconds\n",
      "\n",
      "0x4e52\n",
      "The dataframe is made up by 13 packets\n",
      "Filtered dataframe is made up by 1 packets\n",
      "3.799 microseconds\n",
      "\n",
      "0x9989\n",
      "The dataframe is made up by 23 packets\n",
      "Filtered dataframe is made up by 2 packets\n",
      "8.236 microseconds\n",
      "\n",
      "0x5db6\n",
      "The dataframe is made up by 14 packets\n",
      "Filtered dataframe is made up by 4 packets\n",
      "5.208 microseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1566110/1513759939.py:41: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address, color=colormap(devices.index(device_address)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0x772c\n",
      "The dataframe is made up by 14 packets\n",
      "Filtered dataframe is made up by 3 packets\n",
      "6.936 microseconds\n",
      "\n",
      "0x61af\n",
      "The dataframe is made up by 31 packets\n",
      "Filtered dataframe is made up by 4 packets\n",
      "3.144 microseconds\n",
      "\n",
      "       Unnamed: 0                          Time  Delta Time  Length  \\\n",
      "7477        41545  Oct 12, 2023 17:48:43.629088    0.675263      80   \n",
      "7481        41549  Oct 12, 2023 17:48:44.138432    0.499502      12   \n",
      "19857       54178  Oct 12, 2023 18:38:53.448040    2.921897      80   \n",
      "19874       54195  Oct 12, 2023 18:38:53.957783    0.114662      12   \n",
      "19875       54196  Oct 12, 2023 18:38:53.960927    0.003144      12   \n",
      "37846       75228  Oct 12, 2023 16:58:37.186016    0.652240      80   \n",
      "37850       75232  Oct 12, 2023 16:58:37.695972    0.499890      12   \n",
      "\n",
      "      Source IEEE Destination IEEE FCF IEEE  Sequence Number Source Zigbee  \\\n",
      "7477       0xe011           0xe694   0x8861             92.0        0xe011   \n",
      "7481       0xe011           0xe694   0x8863             93.0           NaN   \n",
      "19857      0xe011           0xe694   0x8861             94.0        0xe011   \n",
      "19874      0xe011           0xe694   0x8863             95.0           NaN   \n",
      "19875      0xe011           0xe694   0x8863             95.0           NaN   \n",
      "37846      0xe011           0xe694   0x8861             90.0        0xe011   \n",
      "37850      0xe011           0xe694   0x8863             91.0           NaN   \n",
      "\n",
      "      Destination Zigbee FCF Zigbee  Payload Length          Extended Source  \\\n",
      "7477              0x0000     0x0248            43.0  00:15:8d:00:09:76:41:64   \n",
      "7481                 NaN        NaN             0.0                      NaN   \n",
      "19857             0x0000     0x0248            43.0  00:15:8d:00:09:76:41:64   \n",
      "19874                NaN        NaN             0.0                      NaN   \n",
      "19875                NaN        NaN             0.0                      NaN   \n",
      "37846             0x0000     0x0248            43.0  00:15:8d:00:09:76:41:64   \n",
      "37850                NaN        NaN             0.0                      NaN   \n",
      "\n",
      "               File    Action  \n",
      "7477   idle2.pcapng       Ack  \n",
      "7481   idle2.pcapng  APS: Ack  \n",
      "19857  idle3.pcapng       Ack  \n",
      "19874  idle3.pcapng  APS: Ack  \n",
      "19875  idle3.pcapng       Ack  \n",
      "37846  idle1.pcapng       Ack  \n",
      "37850  idle1.pcapng       Ack  \n",
      "0xe011\n",
      "The dataframe is made up by 7 packets\n",
      "Filtered dataframe is made up by 1 packets\n",
      "3.144 microseconds\n",
      "\n",
      "0xe694\n",
      "The dataframe is made up by 2423 packets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1566110/1513759939.py:41: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address, color=colormap(devices.index(device_address)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe is made up by 792 packets\n",
      "66.968 microseconds\n",
      "\n",
      "0x6e5f\n",
      "The dataframe is made up by 2098 packets\n",
      "Filtered dataframe is made up by 774 packets\n",
      "66.956 microseconds\n",
      "\n",
      "0xc8f0\n",
      "The dataframe is made up by 1987 packets\n",
      "Filtered dataframe is made up by 823 packets\n",
      "66.561 microseconds\n",
      "\n",
      "0x09ac\n",
      "The dataframe is made up by 714 packets\n",
      "Filtered dataframe is made up by 13 packets\n",
      "58.919 microseconds\n",
      "\n",
      "0x82eb\n",
      "The dataframe is made up by 801 packets\n",
      "Filtered dataframe is made up by 22 packets\n",
      "66.912 microseconds\n",
      "\n",
      "0x054f\n",
      "The dataframe is made up by 1568 packets\n",
      "Filtered dataframe is made up by 518 packets\n",
      "66.873 microseconds\n",
      "\n",
      "0xebe5\n",
      "The dataframe is made up by 873 packets\n",
      "Filtered dataframe is made up by 74 packets\n",
      "64.579 microseconds\n",
      "\n",
      "0xe5c4\n",
      "The dataframe is made up by 1042 packets\n",
      "Filtered dataframe is made up by 206 packets\n",
      "66.938 microseconds\n",
      "\n",
      "0x482d\n",
      "The dataframe is made up by 1944 packets\n",
      "Filtered dataframe is made up by 830 packets\n",
      "66.964 microseconds\n",
      "\n",
      "0xa209\n",
      "The dataframe is made up by 2474 packets\n",
      "Filtered dataframe is made up by 1240 packets\n",
      "66.935 microseconds\n",
      "\n",
      "0x2cae\n",
      "The dataframe is made up by 2402 packets\n",
      "Filtered dataframe is made up by 282 packets\n",
      "62.074 microseconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Density plot based on inter arrival time\n",
    "\n",
    "colormap = plt.cm.get_cmap('tab20', len(devices))\n",
    "\n",
    "devices_hist_v2 = []\n",
    "for device_address in devices:\n",
    "    if device_address == '0xe011':\n",
    "        device_df = df[(df['Source IEEE'] == device_address)]\n",
    "        print(device_df)\n",
    "    \n",
    "\n",
    "    #device_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_df = df[(df['Source IEEE'] == device_address)]\n",
    "    device_df = device_df.sort_values(by=['Time'])\n",
    "    print(device_address)\n",
    "    print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "    device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    date_string = device_df.loc[0,'Time']\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(device_df)):\n",
    "        date_string = device_df.loc[i,'Time']\n",
    "        date_object1 = datetime.strptime(date_string, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=0, microseconds=67000) and td > timedelta(days=0)]\n",
    "    print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "    filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "    if len(filtered_timedelta_micro_device) > 0:\n",
    "        print(f\"{max(filtered_timedelta_micro_device)} microseconds\")\n",
    "\n",
    "\n",
    "    #plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "    sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address, color=colormap(devices.index(device_address)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time in Milliseconds')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f\"Histogram of Timedelta Values\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=100, range=(0,100))\n",
    "    devices_hist_v2.append(hist)\n",
    "    '''\n",
    "    cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "    percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "    percentile_value = bin_edges[percentile_bin + 1]\n",
    "    print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "    '''\n",
    "    print('')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"device_density.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Bulb\n",
      "Device address: 0x482d, Number of packets filtered: 1803, Number of packets: 1943, MEAN: 170206.37104444444\n",
      "Device address: 0xebe5, Number of packets filtered: 835, Number of packets: 872, MEAN: 170206.37104444444\n",
      "Device address: 0x054f, Number of packets filtered: 1462, Number of packets: 1567, MEAN: 170206.37104444444\n",
      "Device address: 0xa209, Number of packets filtered: 2353, Number of packets: 2473, MEAN: 170206.37104444444\n",
      "Device address: 0xe5c4, Number of packets filtered: 889, Number of packets: 1041, MEAN: 170206.37104444444\n",
      "Number of packets: 7342\n",
      "16172.709100000002\n",
      "Device type: Motion\n",
      "Device address: 0x2cae, Number of packets filtered: 2395, Number of packets: 2401, MEAN: 170206.37104444444\n",
      "Device address: 0xb815, Number of packets filtered: 5, Number of packets: 6, MEAN: 170206.37104444444\n",
      "Device address: 0x9989, Number of packets filtered: 14, Number of packets: 22, MEAN: 170206.37104444444\n",
      "Device address: 0x4e52, Number of packets filtered: 7, Number of packets: 12, MEAN: 170206.37104444444\n",
      "Number of packets: 2421\n",
      "5064.384\n",
      "Device type: nan\n",
      "Number of packets: 0\n",
      "nan\n",
      "Device type: Socket\n",
      "Device address: 0x82eb, Number of packets filtered: 775, Number of packets: 800, MEAN: 170206.37104444444\n",
      "Device address: 0x6e5f, Number of packets filtered: 2058, Number of packets: 2097, MEAN: 170206.37104444444\n",
      "Device address: 0xe694, Number of packets filtered: 2414, Number of packets: 2422, MEAN: 170206.37104444444\n",
      "Device address: 0xc8f0, Number of packets filtered: 1885, Number of packets: 1986, MEAN: 170206.37104444444\n",
      "Device address: 0x09ac, Number of packets filtered: 567, Number of packets: 713, MEAN: 170206.37104444444\n",
      "Number of packets: 7699\n",
      "15074.4964\n",
      "Device type: Door\n",
      "Device address: 0x772c, Number of packets filtered: 9, Number of packets: 13, MEAN: 170206.37104444444\n",
      "Device address: 0x3181, Number of packets filtered: 2, Number of packets: 2, MEAN: 170206.37104444444\n",
      "Device address: 0x5db6, Number of packets filtered: 9, Number of packets: 13, MEAN: 170206.37104444444\n",
      "Device address: 0x0a79, Number of packets filtered: 6, Number of packets: 6, MEAN: 170206.37104444444\n",
      "Number of packets: 26\n",
      "4058.40875\n",
      "Device type: Vibration\n",
      "Device address: 0x61af, Number of packets filtered: 12, Number of packets: 30, MEAN: 170206.37104444444\n",
      "Number of packets: 12\n",
      "1224.2083\n",
      "Device type: Temperature\n",
      "Device address: 0x265e, Number of packets filtered: 26, Number of packets: 49, MEAN: 170206.37104444444\n",
      "Number of packets: 26\n",
      "510.6805\n",
      "Device type: Button\n",
      "Device address: 0xe011, Number of packets filtered: 4, Number of packets: 6, MEAN: 170206.37104444444\n",
      "Number of packets: 4\n",
      "509.92404999999997\n",
      "Median 95 Q 4058.40875\n",
      "Mean 95 Q 6087.830157142857\n",
      "Qraw 95 Q 15510.375999999998\n"
     ]
    }
   ],
   "source": [
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df['Device Type'] = df['Source IEEE'].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "\n",
    "quant_list = list()\n",
    "quant_raw_list = list()\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    if device_type == 'Coordinator':\n",
    "        continue\n",
    "\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_milli_device = []\n",
    "    \n",
    "    print(f\"Device type: {device_type}\")\n",
    "\n",
    "    for device_address in subset_df['Source IEEE'].unique():\n",
    "        device_df = subset_df[subset_df['Source IEEE'] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=17.160, microseconds=0) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_milli_device = [td.total_seconds()*1000 for td in filtered_timedelta_device]\n",
    "        \n",
    "        print(f\"Device address: {device_address}, Number of packets filtered: {len(filtered_timedelta_milli_device)}, Number of packets: {len(list_of_timedelta)}, MEAN: {statistics.mean(filtered_timedelta_micro_device)}\")\n",
    "        \n",
    "\n",
    "        \n",
    "        all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "\n",
    "\n",
    "    # if len(all_timedelta_milli_device) < 10:\n",
    "    #     continue\n",
    "    \n",
    "    print(f\"Number of packets: {len(all_timedelta_milli_device)}\")\n",
    "    \n",
    "    quant_raw_list.extend(all_timedelta_milli_device)\n",
    "    all_timedelta_micro_device_pd = pd.Series(all_timedelta_milli_device)\n",
    "    quant = all_timedelta_micro_device_pd.quantile(0.95)\n",
    "    if quant is not None and not np.isnan(quant):\n",
    "        quant_list.append(quant)\n",
    "        \n",
    "    print(all_timedelta_micro_device_pd.quantile(0.95))\n",
    "    \n",
    "    \n",
    "    \n",
    "    sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "    #sns.histplot(all_timedelta_micro_device, bins='auto',stat='count', kde=True, color=color, label=device_type,alpha=0.5)\n",
    "    #sns.displot(all_timedelta_micro_device, kind=\"kde\",color=color, label=device_type)\n",
    "\n",
    "import statistics\n",
    "\n",
    "print(f\"Median 95 Q {statistics.median(quant_list)}\")\n",
    "print(f\"Mean 95 Q {statistics.mean(quant_list)}\")\n",
    "q_raw = pd.Series(quant_raw_list).quantile(0.95)\n",
    "print(f\"Qraw 95 Q {q_raw}\")\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "#save_tikzplotlib(plt.gcf(), \"category_density_idle_67ms_no_coordinator_ieee_and_zb.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAT Density Plot V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the time delta between packets\n",
    "\n",
    "df = df4\n",
    "load_mapping(4)\n",
    "\n",
    "df.sort_values(by=['Time'])\n",
    "date_string = df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    date_string = df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    if True:\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=200000)\n",
    "second_threshold = timedelta(days=0)\n",
    "filtered_timedelta_no_ack = [td for td in list_of_timedelta if td < threshold]\n",
    "filtered_timedelta_no_ack = [td for td in filtered_timedelta_no_ack if td > second_threshold]\n",
    "filtered_timedelta_micro_no_ack = [td.microseconds for td in filtered_timedelta_no_ack]\n",
    "\n",
    "# plt.hist(filtered_timedelta_micro_no_ack, bins=60, edgecolor='k')\n",
    "# plt.xlabel('Time in Microseconds')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Timedelta Values')\n",
    "# plt.grid(True)\n",
    "\n",
    "# hist, bin_edges = np.histogram(filtered_timedelta_micro_no_ack, bins=60)\n",
    "# cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "# percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "# percentile_value = bin_edges[percentile_bin + 1]\n",
    "# print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "## Density plot\n",
    "\n",
    "sns.kdeplot(filtered_timedelta_micro_no_ack, bw_method=0.1)\n",
    "plt.xlabel('Time in Microseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Timedelta Values')\n",
    "#save_tikzplotlib(plt.gcf(), \"threshold.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x0000\n",
      "The dataframe is made up by 65512 packets\n",
      "Filtered dataframe is made up by 57820 packets\n",
      "99.562 microseconds\n",
      "\n",
      "0x4615\n",
      "The dataframe is made up by 263 packets\n",
      "Filtered dataframe is made up by 117 packets\n",
      "30.147 microseconds\n",
      "\n",
      "0x946e\n",
      "The dataframe is made up by 72 packets\n",
      "Filtered dataframe is made up by 5 packets\n",
      "6.448 microseconds\n",
      "\n",
      "0x7b10\n",
      "The dataframe is made up by 61 packets\n",
      "Filtered dataframe is made up by 6 packets\n",
      "28.151 microseconds\n",
      "\n",
      "0xd0bb\n",
      "The dataframe is made up by 198 packets\n",
      "Filtered dataframe is made up by 139 packets\n",
      "83.109 microseconds\n",
      "\n",
      "0x1f29\n",
      "The dataframe is made up by 22 packets\n",
      "Filtered dataframe is made up by 1 packets\n",
      "5.391 microseconds\n",
      "\n",
      "0x27d7\n",
      "The dataframe is made up by 203 packets\n",
      "Filtered dataframe is made up by 3 packets\n",
      "12.768 microseconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1250042/896754407.py:40: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address)\n",
      "/tmp/ipykernel_1250042/896754407.py:40: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0x907b\n",
      "The dataframe is made up by 55 packets\n",
      "Filtered dataframe is made up by 1 packets\n",
      "4.424 microseconds\n",
      "\n",
      "0xe01d\n",
      "The dataframe is made up by 283 packets\n",
      "Filtered dataframe is made up by 196 packets\n",
      "27.45 microseconds\n",
      "\n",
      "0x187a\n",
      "The dataframe is made up by 117 packets\n",
      "Filtered dataframe is made up by 61 packets\n",
      "53.682 microseconds\n",
      "\n",
      "0xc31c\n",
      "The dataframe is made up by 404 packets\n",
      "Filtered dataframe is made up by 279 packets\n",
      "68.512 microseconds\n",
      "\n",
      "0xe1a6\n",
      "The dataframe is made up by 8834 packets\n",
      "Filtered dataframe is made up by 5808 packets\n",
      "99.924 microseconds\n",
      "\n",
      "0x3d95\n",
      "The dataframe is made up by 11311 packets\n",
      "Filtered dataframe is made up by 8561 packets\n",
      "99.372 microseconds\n",
      "\n",
      "0xa706\n",
      "The dataframe is made up by 9766 packets\n",
      "Filtered dataframe is made up by 7843 packets\n",
      "99.828 microseconds\n",
      "\n",
      "0x4e11\n",
      "The dataframe is made up by 2157 packets\n",
      "Filtered dataframe is made up by 768 packets\n",
      "96.94 microseconds\n",
      "\n",
      "0x0112\n",
      "The dataframe is made up by 1159 packets\n",
      "Filtered dataframe is made up by 77 packets\n",
      "91.897 microseconds\n",
      "\n",
      "0xec7f\n",
      "The dataframe is made up by 5248 packets\n",
      "Filtered dataframe is made up by 3348 packets\n",
      "99.068 microseconds\n",
      "\n",
      "0x1e15\n",
      "The dataframe is made up by 2693 packets\n",
      "Filtered dataframe is made up by 1101 packets\n",
      "99.968 microseconds\n",
      "\n",
      "0x1cd8\n",
      "The dataframe is made up by 3302 packets\n",
      "Filtered dataframe is made up by 1837 packets\n",
      "98.651 microseconds\n",
      "\n",
      "0x5bb9\n",
      "The dataframe is made up by 2133 packets\n",
      "Filtered dataframe is made up by 1265 packets\n",
      "98.947 microseconds\n",
      "\n",
      "0x711c\n",
      "The dataframe is made up by 2525 packets\n",
      "Filtered dataframe is made up by 1085 packets\n",
      "98.06 microseconds\n",
      "\n",
      "0x059b\n",
      "The dataframe is made up by 1827 packets\n",
      "Filtered dataframe is made up by 1401 packets\n",
      "96.493 microseconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Density plot based on inter arrival time per device\n",
    "\n",
    "df = df4\n",
    "load_mapping(4)\n",
    "\n",
    "colormap = plt.cm.get_cmap('tab20', len(devices))\n",
    "\n",
    "devices_hist_v4 = []\n",
    "for device_address in devices:\n",
    "\n",
    "    #device_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_df = df[(df['Source Zigbee'] == device_address)]\n",
    "    device_df = device_df.sort_values(by=['Time'])\n",
    "    print(device_address)\n",
    "    print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "    device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    date_string = device_df.loc[0,'Time']\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(device_df)):\n",
    "        date_string = device_df.loc[i,'Time']\n",
    "        date_object1 = datetime.strptime(date_string, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=0, microseconds=100000) and td > timedelta(days=0)]\n",
    "    print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "    filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "    if len(filtered_timedelta_micro_device) > 0:\n",
    "        print(f\"{max(filtered_timedelta_micro_device)} microseconds\")\n",
    "\n",
    "\n",
    "    #plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "    sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time in Milliseconds')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f\"Histogram of Timedelta Values\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=100, range=(0,100))\n",
    "    devices_hist_v4.append(hist)\n",
    "    '''\n",
    "    cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "    percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "    percentile_value = bin_edges[percentile_bin + 1]\n",
    "    print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "    '''\n",
    "    print('')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"device_density.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df = df4\n",
    "load_mapping(4)\n",
    "\n",
    "df['Device Type'] = df['Source Zigbee'].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_micro_device = []\n",
    "\n",
    "    for device_address in subset_df['Source Zigbee'].unique():\n",
    "        device_df = subset_df[subset_df['Source Zigbee'] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=20, microseconds=10000) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "\n",
    "        all_timedelta_micro_device.extend(filtered_timedelta_micro_device)\n",
    "\n",
    "    sns.kdeplot(all_timedelta_micro_device, bw_method=0.1, label=device_type, color=color)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "#save_tikzplotlib(plt.gcf(), \"category_density_updated.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lev Distance Prob Dist Idel State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract 300 bins from the kde plot\n",
    "def extract_bins (df):\n",
    "\n",
    "    df['Device Type'] = df['Source IEEE'].map(device_type_mapping)\n",
    "    \n",
    "    areas_per_device = {}\n",
    "    kde_list = []\n",
    "    # Density plot based on inter-arrival time for each category\n",
    "    colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "    for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "        if device_type == 'Coordinator' or device_type == 'nan' or device_type is None or device_type is np.nan:\n",
    "            continue\n",
    "\n",
    "        subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "        all_timedelta_milli_device = []\n",
    "        \n",
    "        print(f\"Device type: {device_type}\")\n",
    "\n",
    "        for device_address in subset_df['Source IEEE'].unique():\n",
    "            device_df = subset_df[subset_df['Source IEEE'] == device_address]\n",
    "            device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            list_of_timedelta = []\n",
    "            for i in range(1, len(device_df)):\n",
    "                date_string = device_df.loc[i, 'Time']\n",
    "                date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "                list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "            filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=17.160, microseconds=0) and td > timedelta(days=0)]\n",
    "\n",
    "            filtered_timedelta_milli_device = [td.total_seconds()*1000 for td in filtered_timedelta_device]\n",
    "            \n",
    "            \n",
    "            all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "            \n",
    "        \n",
    "        if len(all_timedelta_milli_device) <= 5:\n",
    "            continue\n",
    "        \n",
    "        plt.figure()\n",
    "        kde_plot = sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "        kde_list.append({'device':device_type,'kde':kde_plot})\n",
    "    \n",
    "    glob_min = 0\n",
    "    glob_max = 0\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        # Extract KDE Data:\n",
    "        kde = elem['kde']\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = x_values.min(), x_values.max()\n",
    "        if glob_max < x_max:\n",
    "            glob_max = x_max\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        kde = elem['kde']\n",
    "        # Extract KDE Data:\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = glob_min,glob_max\n",
    "        bin_width = (x_max - x_min) / 300\n",
    "        bins = np.linspace(x_min, x_max, 301)  # 301 points for 300 intervals\n",
    "\n",
    "        # Area Calculation:\n",
    "        areas = []\n",
    "        for i in range(len(bins) - 1):\n",
    "            bin_start, bin_end = bins[i], bins[i + 1]\n",
    "            bin_midpoint = (bin_start + bin_end) / 2\n",
    "\n",
    "            # Use Kernel Density Estimation Function Directly (no need for interpolation):\n",
    "            density = kde_line.get_ydata()[np.argmin(np.abs(x_values - bin_midpoint))]\n",
    "            area = density * bin_width\n",
    "            areas.append(area)\n",
    "\n",
    "        areas_per_device[elem['device']] = areas\n",
    "        \n",
    "    for device_type, areas in areas_per_device.items():\n",
    "        plt.figure()\n",
    "        plt.bar(bins[:-1], areas, width=bin_width, align='edge', edgecolor='k', alpha=0.5)\n",
    "        plt.xlabel('Time Interval (ms)')\n",
    "        plt.ylabel('Area')\n",
    "        plt.title(f'Areas under KDE for {device_type}')\n",
    "        plt.show()\n",
    "    \n",
    "    return areas_per_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Bulb\n",
      "Device type: Motion\n",
      "Device type: Socket\n",
      "Device type: Door\n",
      "Device type: Vibration\n",
      "Device type: Temperature\n",
      "Device type: Button\n"
     ]
    }
   ],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hist2 = extract_bins(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Bulb\n",
      "Device type: Socket\n",
      "Device type: Temperature\n",
      "Device type: Motion\n",
      "Device type: Button\n",
      "Device type: Door\n"
     ]
    }
   ],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "hist4 = extract_bins(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m distance_matrix_total \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(hist4), \u001b[38;5;28mlen\u001b[39m(hist2)))  \u001b[38;5;66;03m# Use float dtype for EMD\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Compute pairwise EMD between histograms of the same device type\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (dev_type1, hist1) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mhist2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, (dev_type2, hist2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hist4\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dev_type1 \u001b[38;5;241m==\u001b[39m dev_type2:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist4), len(hist2)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "        if dev_type1 == dev_type2:\n",
    "            distance_matrix_total[j, i] = wasserstein_distance(hist1_val, hist2_val)  # Correct indexing\n",
    "\n",
    "print(distance_matrix_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEVENST Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_distance_matrix_v2 = [[0 for x in range(len(devices_hist_v2))] for y in range(len(devices_hist_v2))]\n",
    "\n",
    "for i in range(len(devices_hist_v2)):\n",
    "    for j in range(len(devices_hist_v2)):\n",
    "        lev_distance_matrix_v2[i][j] = lev.distance(str(devices_hist_v2[i]), str(devices_hist_v2[j]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'devices_hist_v4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lev_distance_matrix_v4 \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(devices_hist_v4))] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdevices_hist_v4\u001b[49m))]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(devices_hist_v4)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(devices_hist_v4)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'devices_hist_v4' is not defined"
     ]
    }
   ],
   "source": [
    "lev_distance_matrix_v4 = [[0 for x in range(len(devices_hist_v4))] for y in range(len(devices_hist_v4))]\n",
    "\n",
    "for i in range(len(devices_hist_v4)):\n",
    "    for j in range(len(devices_hist_v4)):\n",
    "        lev_distance_matrix_v4[i][j] = lev.distance(str(devices_hist_v4[i]), str(devices_hist_v4[j]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_devices_hist = devices_hist_v2 + devices_hist_v4\n",
    "\n",
    "lev_distance_matrix_total = np.zeros((len(total_devices_hist), len(total_devices_hist)), dtype=int)\n",
    "\n",
    "for i in range(len(total_devices_hist)):\n",
    "    for j in range(len(total_devices_hist)):\n",
    "        lev_distance_matrix_total[i][j] = lev.distance(str(total_devices_hist[i]), str(total_devices_hist[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lev_distance_matrix_v2, xticklabels=devices_name, yticklabels=devices_name, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lev_distance_matrix_v4, xticklabels=devices_name, yticklabels=devices_name)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lev_distance_matrix_total[0:22, 22:45], xticklabels=devices_name_v4, yticklabels=devices_name_v2, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix in between Acquisition 2 and 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "idle_df = df[df['File'].str.contains('idle')]\n",
    "\n",
    "idle_df.head()\n",
    "\n",
    "sns.kdeplot(df['Length'], bw_method=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df['Device Type'] = df['Source Zigbee'].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_micro_device = []\n",
    "\n",
    "    for device_address in subset_df['Source Zigbee'].unique():\n",
    "        device_df = subset_df[subset_df['Source Zigbee'] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=50, microseconds=10000) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "\n",
    "        all_timedelta_micro_device.extend(filtered_timedelta_micro_device)\n",
    "\n",
    "    sns.kdeplot(all_timedelta_micro_device, bw_method=0.1, label=device_type, color=color)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "#save_tikzplotlib(plt.gcf(), \"category_density_updated.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x0a79\n",
      "The dataframe is made up by 75 packets\n",
      "Filtered dataframe is made up by 56 packets\n",
      "8.6792 seconds\n"
     ]
    }
   ],
   "source": [
    "# Density function for specific device\n",
    "\n",
    "device_address = '0x0a79'\n",
    "\n",
    "device_df = df[(df['Source Zigbee'] == device_address)]\n",
    "device_df = device_df.sort_values(by=['Time'])\n",
    "print(device_address)\n",
    "print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "date_string = device_df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(device_df)):\n",
    "    date_string = device_df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1\n",
    "\n",
    "filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=20, microseconds=0) and td > timedelta(days=0)]\n",
    "print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) / 1e6 for td in filtered_timedelta_device]\n",
    "if len(filtered_timedelta_micro_device) > 0:\n",
    "    print(f\"{max(filtered_timedelta_micro_device)} seconds\")\n",
    "\n",
    "\n",
    "#plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address)\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Seconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f\"Histogram of Timedelta Values\")\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=100, range=(0, 20))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating mean and standard deviation from the total dataframe\n",
    "inter_arrival_time = []\n",
    "df = df.sort_values(by=['Time'])\n",
    "\n",
    "date_string = df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    date_string = df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    iat = date_object1 - date_object\n",
    "    if(iat > timedelta(days=0, hours=0, seconds=0, microseconds=0) and\n",
    "       iat < timedelta(days=0, hours=0, seconds=0, microseconds=150000)):\n",
    "        inter_arrival_time.append(iat)\n",
    "    date_object = date_object1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is 0:00:00.010561\n",
      "Standard deviation is 0.02030713200671125 seconds\n",
      "Threshold is 0.07148239602013376 seconds\n"
     ]
    }
   ],
   "source": [
    "iat_mean = np.mean(inter_arrival_time)\n",
    "\n",
    "micro_iat = np.array([td.total_seconds() for td in inter_arrival_time])\n",
    "iat_stdev = np.std(micro_iat)\n",
    "\n",
    "print(f\"Mean is {iat_mean}\")\n",
    "print(f\"Standard deviation is {iat_stdev} seconds\")\n",
    "\n",
    "iat_threshold = iat_mean.total_seconds() + 3 * iat_stdev\n",
    "print(f\"Threshold is {iat_threshold} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe for device 0x310d is made up by 0 packets\n"
     ]
    }
   ],
   "source": [
    "## Calculate the time delta between packets from a specific device grouped by destination zigbee\n",
    "\n",
    "device_address = '0x310d'\n",
    "\n",
    "device_flux_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "device_flux_df = device_flux_df.sort_values(by=['Time'])\n",
    "print(f\"The dataframe for device {device_address} is made up by {len(device_flux_df)} packets\")\n",
    "\n",
    "device_flux_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#group by destination zigbee\n",
    "device_flux_df = device_flux_df.groupby('Destination Zigbee')\n",
    "\n",
    "legend_labels = []\n",
    "\n",
    "for destination_address, group_data in device_flux_df:\n",
    "    #if destination_address != device_address:\n",
    "    group = device_flux_df.get_group(destination_address)\n",
    "    group.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    date_string = group.loc[0,'Time']\n",
    "    date_string_without_zeros1 = date_string[:-3]\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    date_object = datetime.strptime(date_string_without_zeros1, date_format)\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(group)):\n",
    "        date_string = group.loc[i,'Time']\n",
    "        date_string_without_zeros = date_string[:-3]\n",
    "        date_object1 = datetime.strptime(date_string_without_zeros, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    threshold = timedelta(days=0, hours=0, seconds=0, microseconds=100000)\n",
    "    filtered = [td for td in list_of_timedelta if td < threshold]\n",
    "    list_micro = [td.microseconds + 1000000 * td.seconds for td in filtered]\n",
    "\n",
    "    if len(list_micro) > 1:\n",
    "        mean = np.mean(list_micro)\n",
    "        std = np.std(list_micro)\n",
    "\n",
    "        print(f\"Mean for destination {destination_address} is: {mean}\")\n",
    "        print(f\"Standard deviation for destination {destination_address} is: {std}\")\n",
    "        print(\"Calculated on {} packets\".format(len(list_micro)))\n",
    "        print(\"\")\n",
    "        legend_labels.append(destination_address)\n",
    "        plt.hist(list_micro, bins=100, edgecolor='k', alpha=0.7, label=destination_address)\n",
    "    \n",
    "    plt.legend(legend_labels, title='Destination Address')\n",
    "    plt.xlabel('Time in Microseconds')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Timedelta Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the time delta between a flux of two specific devices\n",
    "\n",
    "device_address = '0x0000'\n",
    "\n",
    "device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "device_packets = device_packets.sort_values(by=['Time'])\n",
    "device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "communicating_device = device_packets['Destination Zigbee'].unique()\n",
    "mask = communicating_device != device_address\n",
    "communicating_device = communicating_device[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0xfffc' '0xe694' '0x6e5f' '0xc8f0' '0xebe5' '0x4e52' '0xa209' '0x2cae'\n",
      " '0xe5c4' '0x054f' '0x482d' '0x265e' '0x61af' '0x9989' '0x5db6' '0x772c'\n",
      " '0x82eb' '0xb815' '0x09ac' '0xe011' '0x3181' '0x0a79' '0x4a30']\n"
     ]
    }
   ],
   "source": [
    "print(communicating_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_device = '0xaea8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication = device_packets[(device_packets['Destination Zigbee'] == other_device) | (device_packets['Source Zigbee'] == other_device)]\n",
    "communication.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexes/range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m date_string \u001b[38;5;241m=\u001b[39m \u001b[43mcommunication\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m date_string_without_zeros1 \u001b[38;5;241m=\u001b[39m date_string[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      3\u001b[0m date_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexing.py:1183\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/frame.py:4221\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4215\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[1;32m   4217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   4218\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   4219\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   4220\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[0;32m-> 4221\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[1;32m   4224\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[1;32m   4225\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexes/range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "date_string = communication.loc[0,'Time']\n",
    "date_string_without_zeros1 = date_string[:-3]\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string_without_zeros1, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(communication)):\n",
    "    date_string = communication.loc[i,'Time']\n",
    "    date_string_without_zeros = date_string[:-3]\n",
    "    date_object1 = datetime.strptime(date_string_without_zeros, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_of_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=0)\n",
    "max_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=50000)\n",
    "filtered = [td for td in list_of_timedelta if td > min_threshold]\n",
    "filtered = [td for td in filtered if td < max_threshold]\n",
    "list_micro = [td.microseconds + 1000000 * td.seconds for td in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_labels = []\n",
    "\n",
    "if len(list_micro) > 1:\n",
    "    mean = np.mean(list_micro)\n",
    "    std = np.std(list_micro)\n",
    "\n",
    "    print(f\"Mean for destination {other_device} is: {mean}\")\n",
    "    print(f\"Standard deviation for destination {other_device} is: {std}\")\n",
    "    print(\"Calculated on {} packets\".format(len(list_micro)))\n",
    "    print(\"\")\n",
    "\n",
    "plt.hist(list_micro, bins=500, edgecolor='k')\n",
    "legend_labels.append(other_device)\n",
    "plt.legend(legend_labels, title='Device in Communication')\n",
    "plt.xlabel('Time in Microseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values for device {}'.format(device_address))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the dataframe into trains of packets according to the threshold\n",
    "\n",
    "# for each device i will have a list of list: a list of trains and each train is a list of packets\n",
    "# there will be the same train for both the devices in communication\n",
    "# from the packets in the train i will extract the features (delta time, packet length, packet type, payload, fcf, ecc)\n",
    "\n",
    "train_of_packets = []\n",
    "all_trains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device_address in devices:\n",
    "\n",
    "    device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2:\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    trains.to_csv(f'trains_{device_address}.csv', index=False)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    train_of_packets = []\n",
    "    all_trains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bin_edges) - 1):\n",
    "    print(f\"Bin {i + 1}: {bin_edges[i]:.2f} to {bin_edges[i + 1]:.2f} --> Count: {int(hist[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Possible MAC addresses for each device\n",
    "\n",
    "mac_addresses = df.groupby('Source Zigbee')['Extended Source'].unique().reset_index()\n",
    "\n",
    "for source, extended in zip(mac_addresses['Source Zigbee'], mac_addresses['Extended Source']):\n",
    "    if source not in devices:\n",
    "        mac_addresses.drop(mac_addresses[mac_addresses['Source Zigbee'] == source].index, inplace=True)\n",
    "    else:\n",
    "        mac_addresses.loc[mac_addresses['Source Zigbee'] == source, 'Source Zigbee'] = device_name_mapping[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------------------------------------- RANDOM FOREST ON DENSITY FUNCTION ---------------------------------------- ###\n",
    "\n",
    "## Matrix composed by the hystograms of the time delta between packets for each device removing acks and link status\n",
    "\n",
    "number_of_features = 100\n",
    "total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "train_of_packets = []\n",
    "\n",
    "\n",
    "for device_address in devices:\n",
    "\n",
    "    \n",
    "    # CREATE THE TRAINS OF PACKETS\n",
    "    all_trains = []\n",
    "    \n",
    "    device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2 and train_of_packets[0]['Source Zigbee'] == device_address:\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    # REMOVE DUPLICATES\n",
    "    cleaned_df = []\n",
    "    for i in range(len(trains)):\n",
    "        train = trains.loc[i]\n",
    "        seen_packets = set()\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                packet_tuple = (\n",
    "                    packet['Length'],\n",
    "                    packet['FCF IEEE'],\n",
    "                    packet['Sequence Number'],\n",
    "                    packet['Source Zigbee'],\n",
    "                    packet['Destination Zigbee'],\n",
    "                    packet['Payload Length']\n",
    "                )\n",
    "                if packet_tuple not in seen_packets:\n",
    "                    seen_packets.add(packet_tuple)\n",
    "                    result.append(packet)\n",
    "        cleaned_df.append(result)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "    # REMOVE ACK PACKETS AND LINK STATUS\n",
    "    no_ack_df = []\n",
    "    for i in range(len(cleaned_df)):\n",
    "        train = cleaned_df.loc[i]\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                result.append(packet)\n",
    "        no_ack_df.append(result)\n",
    "\n",
    "    no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "    # CREATE THE HISTOGRAMS\n",
    "    histograms = []\n",
    "    for i in range(len(no_ack_df)):\n",
    "        train = no_ack_df.loc[i]\n",
    "        list_of_timedelta = []\n",
    "        for j in range(1, len(train)):\n",
    "            if train[j] is not None:\n",
    "                date_string = train[j]['Time']\n",
    "                date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                date_object = datetime.strptime(date_string, date_format)\n",
    "                date_string1 = train[j-1]['Time']\n",
    "                date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                list_of_timedelta.append(date_object - date_object1)\n",
    "        list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "        hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "        histograms.append(hist)\n",
    "\n",
    "    # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "    histogram_matrix = np.array(histograms)\n",
    "    last_column = np.repeat(device_address, len(histogram_matrix))\n",
    "    matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "    # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "    total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Classifier on the total matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = total_matrix[:, :-1]\n",
    "y = total_matrix[:, -1]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "max_depths = range(15, 25)\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth)\n",
    "    f1score = []\n",
    "    accuracy = []\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index] \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        #f1score.append(f1_score(y_test, y_pred, average='macro'))\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "    #f1_scores.append(np.mean(f1score))\n",
    "    accuracies.append(np.mean(accuracy))\n",
    "\n",
    "plt.plot(max_depths, accuracies)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Max Depth')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = total_matrix[:, :-1]\n",
    "y = total_matrix[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix composed by the hystograms of the time delta between outgoing packets for each device removing acks and link status\n",
    "\n",
    "number_of_features = 100\n",
    "total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "train_of_packets = []\n",
    "\n",
    "\n",
    "for device_address in devices:\n",
    "    \n",
    "    # CREATE THE TRAINS OF PACKETS\n",
    "    all_trains = []\n",
    "    \n",
    "    device_packets = df[(df['Source Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2 or device_address == '0x907b':\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    # REMOVE DUPLICATES\n",
    "    cleaned_df = []\n",
    "    for i in range(len(trains)):\n",
    "        train = trains.loc[i]\n",
    "        seen_packets = set()\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                packet_tuple = (\n",
    "                    packet['Length'],\n",
    "                    packet['FCF IEEE'],\n",
    "                    packet['Sequence Number'],\n",
    "                    packet['Source Zigbee'],\n",
    "                    packet['Destination Zigbee'],\n",
    "                    packet['Payload Length']\n",
    "                )\n",
    "                if packet_tuple not in seen_packets:\n",
    "                    seen_packets.add(packet_tuple)\n",
    "                    result.append(packet)\n",
    "        cleaned_df.append(result)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "    # REMOVE ACK PACKETS AND LINK STATUS\n",
    "    no_ack_df = []\n",
    "    for i in range(len(cleaned_df)):\n",
    "        train = cleaned_df.loc[i]\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                result.append(packet)\n",
    "        no_ack_df.append(result)\n",
    "\n",
    "    no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "    # CREATE THE HISTOGRAMS\n",
    "    histograms = []\n",
    "    for i in range(len(no_ack_df)):\n",
    "        train = no_ack_df.loc[i]\n",
    "        list_of_timedelta = []\n",
    "        for j in range(1, len(train)):\n",
    "            if train[j] is not None:\n",
    "                date_string = train[j]['Time']\n",
    "                date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                date_object = datetime.strptime(date_string, date_format)\n",
    "                date_string1 = train[j-1]['Time']\n",
    "                date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                list_of_timedelta.append(date_object - date_object1)\n",
    "        list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "        hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "        histograms.append(hist)\n",
    "\n",
    "    # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "    histogram_matrix = np.array(histograms)\n",
    "    last_column = np.repeat(device_address, len(histogram_matrix))\n",
    "    matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "    # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "    total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix composed by the hystograms of the time delta between outgoing packets for each type of device removing acks and link status\n",
    "\n",
    "number_of_features = 30\n",
    "total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "train_of_packets = []\n",
    "\n",
    "\n",
    "for device_address in devices:\n",
    "    \n",
    "    # CREATE THE TRAINS OF PACKETS\n",
    "    all_trains = []\n",
    "    \n",
    "    device_packets = df[(df['Source Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2 or device_address == '0x907b':\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    # REMOVE DUPLICATES\n",
    "    cleaned_df = []\n",
    "    for i in range(len(trains)):\n",
    "        train = trains.loc[i]\n",
    "        seen_packets = set()\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                packet_tuple = (\n",
    "                    packet['Length'],\n",
    "                    packet['FCF IEEE'],\n",
    "                    packet['Sequence Number'],\n",
    "                    packet['Source Zigbee'],\n",
    "                    packet['Destination Zigbee'],\n",
    "                    packet['Payload Length']\n",
    "                )\n",
    "                if packet_tuple not in seen_packets:\n",
    "                    seen_packets.add(packet_tuple)\n",
    "                    result.append(packet)\n",
    "        cleaned_df.append(result)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "    # REMOVE ACK PACKETS AND LINK STATUS\n",
    "    no_ack_df = []\n",
    "    for i in range(len(cleaned_df)):\n",
    "        train = cleaned_df.loc[i]\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                result.append(packet)\n",
    "        no_ack_df.append(result)\n",
    "\n",
    "    no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "    # CREATE THE HISTOGRAMS\n",
    "    histograms = []\n",
    "    for i in range(len(no_ack_df)):\n",
    "        train = no_ack_df.loc[i]\n",
    "        list_of_timedelta = []\n",
    "        for j in range(1, len(train)):\n",
    "            if train[j] is not None:\n",
    "                date_string = train[j]['Time']\n",
    "                date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                date_object = datetime.strptime(date_string, date_format)\n",
    "                date_string1 = train[j-1]['Time']\n",
    "                date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                list_of_timedelta.append(date_object - date_object1)\n",
    "        list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "        hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "        histograms.append(hist)\n",
    "\n",
    "    # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "    histogram_matrix = np.array(histograms)\n",
    "    last_column = np.repeat(device_type_mapping[device_address], len(histogram_matrix))\n",
    "    matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "    # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "    total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- TSFRESH -------------------------------------------------- ###\n",
    "\n",
    "# Dataframe with train of packets without acks and link status\n",
    "\n",
    "train_of_packets = []\n",
    "no_ack_df = []\n",
    "\n",
    "for device_address in devices:\n",
    "    \n",
    "    # CREATE THE TRAINS OF PACKETS\n",
    "    all_trains = []\n",
    "    \n",
    "    device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2:\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    # REMOVE DUPLICATES\n",
    "    cleaned_df = []\n",
    "    for i in range(len(trains)):\n",
    "        train = trains.loc[i]\n",
    "        seen_packets = set()\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                packet_tuple = (\n",
    "                    packet['Length'],\n",
    "                    packet['FCF IEEE'],\n",
    "                    packet['Sequence Number'],\n",
    "                    packet['Source Zigbee'],\n",
    "                    packet['Destination Zigbee'],\n",
    "                    packet['Payload Length']\n",
    "                )\n",
    "                if packet_tuple not in seen_packets:\n",
    "                    seen_packets.add(packet_tuple)\n",
    "                    result.append(packet)\n",
    "        cleaned_df.append(result)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "    # REMOVE ACK PACKETS AND LINK STATUS\n",
    "    for i in range(len(cleaned_df)):\n",
    "        train = cleaned_df.loc[i]\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                result.append(packet)\n",
    "        if(len(result) > 0 and result[0]['Source Zigbee'] in devices):\n",
    "            no_ack_df.append(result)\n",
    "\n",
    "no_ack_df = pd.DataFrame(no_ack_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single DataFrame with all the packets and 'id' column to identify the train of packets\n",
    "\n",
    "all_packets = list()\n",
    "\n",
    "for i in range(len(no_ack_df)):\n",
    "    train = no_ack_df.loc[i]\n",
    "    for packet in train:\n",
    "        if packet is not None:\n",
    "            all_packets.append({\n",
    "                    'id': i,\n",
    "                    'Time': packet['Time'],\n",
    "                    'Delta Time': packet['Delta Time'],\n",
    "                    'Length': packet['Length'],\n",
    "                    'Sequence Number': packet['Sequence Number'],\n",
    "                    'Payload Length': packet['Payload Length'] if not np.isnan(packet['Payload Length']) else 0})\n",
    "\n",
    "result_df = pd.DataFrame(all_packets)\n",
    "\n",
    "# Set Delta Time of the first packet of each train to 0\n",
    "id_changes = result_df['id'] != result_df['id'].shift(1)\n",
    "result_df.loc[id_changes, 'Delta Time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tsfresh used to get features from trains of packets\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters, ComprehensiveFCParameters\n",
    "\n",
    "result_df['Time'] = pd.to_datetime(result_df['Time'], format=\"%b %d, %Y %H:%M:%S.%f\")\n",
    "\n",
    "# Extract features from the dataframe\n",
    "extracted_features = extract_features(result_df, column_id=\"id\", column_sort=\"Time\", default_fc_parameters=MinimalFCParameters())\n",
    "\n",
    "# Impute the extracted features\n",
    "impute(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the device address to the dataframe of features to be able to classify the packets\n",
    "for i in range(len(no_ack_df)):\n",
    "    extracted_features.loc[i, 'Source Zigbee'] = no_ack_df.loc[i].loc[0].loc['Source Zigbee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to every row of extracted_features the number of packets that fall inside the windows of threshold / 10\n",
    "\n",
    "N = 10\n",
    "for i in range(len(extracted_features)):\n",
    "    mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    deltas = result_df[result_df['id'] == i]['Delta Time']\n",
    "    for n in range(N):\n",
    "        lower_bound = n * (threshold_float / N)\n",
    "        upper_bound = (n + 1) * (threshold_float / N)\n",
    "        count = ((deltas >= lower_bound) & \n",
    "                 (deltas < upper_bound)).sum()\n",
    "\n",
    "        # Add the count to the extracted features\n",
    "        match n:\n",
    "            case 0:\n",
    "                extracted_features.loc[i, 'Window 1'] = count\n",
    "            case 1:\n",
    "                extracted_features.loc[i, 'Window 2'] = count\n",
    "            case 2:\n",
    "                extracted_features.loc[i, 'Window 3'] = count\n",
    "            case 3:\n",
    "                extracted_features.loc[i, 'Window 4'] = count\n",
    "            case 4:\n",
    "                extracted_features.loc[i, 'Window 5'] = count\n",
    "            case 5:\n",
    "                extracted_features.loc[i, 'Window 6'] = count\n",
    "            case 6:\n",
    "                extracted_features.loc[i, 'Window 7'] = count\n",
    "            case 7:\n",
    "                extracted_features.loc[i, 'Window 8'] = count\n",
    "            case 8:\n",
    "                extracted_features.loc[i, 'Window 9'] = count\n",
    "            case 9:\n",
    "                extracted_features.loc[i, 'Window 10'] = count\n",
    "            case _:\n",
    "                print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "#plot_confusion_matrix(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=devices)\n",
    "disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "plt.show()\n",
    "#plt.figure(figsize=(10, 10))\n",
    "#sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.ylabel('Actual')\n",
    "#plt.title('Confusion Matrix')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the RandomForestClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the features with low importance\n",
    "feature_threshold = 0.00000\n",
    "selected_features = np.where(clf.feature_importances_ > feature_threshold)[0]\n",
    "selected_features = X_train.columns[selected_features]\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to every row of extracted_features the number of packets that fall inside the window {-2, -2 + i(4/N)}\n",
    "\n",
    "N = 5\n",
    "max_batches = 0\n",
    "id_max_batches = 0\n",
    "\n",
    "for i in range(len(extracted_features)):\n",
    "    batches = 0\n",
    "    mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    maximum = extracted_features.loc[i, 'Delta Time__maximum']\n",
    "\n",
    "    print(f\"Batches for train {i}\")\n",
    "    print(f\"Mean: {mu}, std dev: {sigma}\")\n",
    "    print(f\"Maximum: {maximum}\")\n",
    "    if(sigma == 0):\n",
    "        continue\n",
    "    for n in range(0,10):\n",
    "        batch = mu - 2 * sigma + n * (4 * sigma / N)\n",
    "        if(batch < 0):\n",
    "            continue\n",
    "        if(batch > maximum):\n",
    "            break\n",
    "        batches += 1\n",
    "        print(batch)\n",
    "    if(batches > max_batches):\n",
    "        max_batches = batches\n",
    "        id_max_batches = i\n",
    "    print('')\n",
    "\n",
    "print(f\"Max batches: {max_batches} for train {id_max_batches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(N):\n",
    "    lower_bound = n * (threshold_float / N)\n",
    "    upper_bound = (n + 1) * (threshold_float / N)\n",
    "    print(f\"{lower_bound} -> {upper_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = result_df.loc[12694,'Delta Time']\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a / threshold_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x_points = np.linspace(mu - 2 * sigma, mu + 2 * sigma, 100)\n",
    "y_points = norm.pdf(x_points, mu, sigma)\n",
    "\n",
    "plt.plot(x_points, y_points, color='black', label=f\"Mean: {mu:.2f}, Std Dev: {sigma:.2f}\")\n",
    "plt.title('Graph of the Probability Density Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost classifiers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(y))}\n",
    "target = y.map(class_mapping)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2)\n",
    "\n",
    "# Instantiate the XGBoostClassifier\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=22, random_state=42)\n",
    "\n",
    "# Train the XGBoostClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the XGBoostClassifier on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBClassifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'extracted_features' is your DataFrame\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "target = y.map(class_mapping)\n",
    "\n",
    "# Instantiate the XGBoostClassifier\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=22, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, target):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Train the XGBoostClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- COORDINATOR DATAFRAME -------------------------------------------------- ###\n",
    "\n",
    "# Dataframe with train of packets without acks and link status\n",
    "\n",
    "train_of_packets = []\n",
    "coordinator_final_df = []\n",
    "\n",
    "device_address = '0x0000'\n",
    "    \n",
    "# CREATE THE TRAINS OF PACKETS\n",
    "all_trains = []\n",
    "\n",
    "coordinator_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "coordinator_packets = coordinator_packets.sort_values(by=['Time'])\n",
    "coordinator_packets = coordinator_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee','FCF Zigbee', 'Payload Length', 'Action']]\n",
    "coordinator_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "prev_timestamp = None\n",
    "for index, row in coordinator_packets.iterrows():\n",
    "    date_string = row['Time']\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    timestamp = datetime.strptime(date_string, date_format)\n",
    "    if prev_timestamp is not None:\n",
    "        delta_time = timestamp - prev_timestamp\n",
    "        if delta_time < threshold:\n",
    "            train_of_packets.append(row)\n",
    "        else:\n",
    "            if len(train_of_packets) >= 2:\n",
    "                all_trains.append(train_of_packets)\n",
    "            train_of_packets = [row]\n",
    "\n",
    "    prev_timestamp = timestamp\n",
    "\n",
    "if len(train_of_packets) >= 2:\n",
    "    all_trains.append(train_of_packets)\n",
    "\n",
    "coordinator_trains = pd.DataFrame(all_trains)\n",
    "\n",
    "del all_trains\n",
    "\n",
    "# REMOVE DUPLICATES\n",
    "coordinator_cleaned_df = []\n",
    "for i in range(len(coordinator_trains)):\n",
    "    train = coordinator_trains.loc[i]\n",
    "    seen_packets = set()\n",
    "    result = []\n",
    "    for packet in train:\n",
    "        if packet is not None:\n",
    "            packet_tuple = (\n",
    "                packet['Length'],\n",
    "                packet['FCF IEEE'],\n",
    "                packet['Sequence Number'],\n",
    "                packet['Source Zigbee'],\n",
    "                packet['Destination Zigbee'],\n",
    "                packet['Payload Length']\n",
    "                )\n",
    "            if packet_tuple not in seen_packets:\n",
    "                seen_packets.add(packet_tuple)\n",
    "                result.append(packet)\n",
    "    coordinator_cleaned_df.append(result)\n",
    "\n",
    "coordinator_cleaned_df = pd.DataFrame(coordinator_cleaned_df)\n",
    "\n",
    "# REMOVE LINK STATUS\n",
    "for i in range(len(coordinator_cleaned_df)):\n",
    "    train = coordinator_cleaned_df.loc[i]\n",
    "    result = []\n",
    "    for packet in train:\n",
    "        if packet is not None and packet['FCF IEEE'] != \"0x8841\":\n",
    "            result.append(packet)\n",
    "    if(len(result) > 0 and result[0]['Source Zigbee'] in devices):\n",
    "        coordinator_final_df.append(result)\n",
    "\n",
    "coordinator_final_df = pd.DataFrame(coordinator_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ++++++++++++++ MATRIX FOR BIDIRECTIONAL NEURAL NETWORKS ++++++++++++++ ###\n",
    "\n",
    "# Counts the number of packets for each train and make mean\n",
    "not_null = coordinator_final_df.notnull().sum(axis=1)\n",
    "mean_not_null = not_null.mean()\n",
    "rounded_mean = math.ceil(mean_not_null)\n",
    "\n",
    "num_of_features = 6\n",
    "\n",
    "# Create a train matrix for bidirectional neural networks with the following features for first 14 packets in a train:\n",
    "# Delta Time, Length, Payload Length, Source Zigbee, FCF Zigbee, IAT from last ingoing or outgoing packet\n",
    "matrix_for_binn = np.zeros((len(coordinator_final_df), rounded_mean, num_of_features))\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    j = 0\n",
    "    sum_delta_1 = 0\n",
    "    sum_delta_0 = 0\n",
    "    seen_1 = False\n",
    "    seen_0 = False\n",
    "    for packet in train:\n",
    "        if j < rounded_mean and packet is not None:\n",
    "            matrix_for_binn[i][j][0] = packet['Delta Time']\n",
    "            matrix_for_binn[i][j][1] = packet['Length']\n",
    "            matrix_for_binn[i][j][2] = packet['Payload Length']\n",
    "            matrix_for_binn[i][j][3] = 1 if packet['Source Zigbee'] == '0x0000' else 0 # 1 if coordinator, 0 if not\n",
    "            matrix_for_binn[i][j][4] = 0 if pd.isna(packet['FCF Zigbee']) else 1 # 1 if zigbee, 0 if not\n",
    "\n",
    "            # iat from last ingoing or outgoing packet\n",
    "            if matrix_for_binn[i][j][3] == 1:\n",
    "                if seen_1:\n",
    "                    sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "                    sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "                    matrix_for_binn[i][j][5] = sum_delta_1\n",
    "                    sum_delta_1 = 0\n",
    "                else:\n",
    "                    matrix_for_binn[i][j][5] = 0\n",
    "                    seen_1 = True\n",
    "                    if seen_0:\n",
    "                        sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "            else:\n",
    "                if seen_0:\n",
    "                    sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "                    sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "                    matrix_for_binn[i][j][5] = sum_delta_0\n",
    "                    sum_delta_0 = 0\n",
    "                else:\n",
    "                    matrix_for_binn[i][j][5] = 0\n",
    "                    seen_0 = True\n",
    "                    if seen_1:\n",
    "                        sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "\n",
    "            j += 1\n",
    "\n",
    "\n",
    "# List to keep track of the device address of each train\n",
    "device_address_list = []\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    device_address_list.append(device_name_mapping[train[0]['Source Zigbee']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "occurrences = Counter(device_address_list)\n",
    "for occurrence in occurrences:\n",
    "    print(f\"{occurrence}: {occurrences[occurrence]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test matrix for bidirectional neural networks with same features as train matrix\n",
    "test_matrix = np.zeros((len(coordinator_final_df), rounded_mean, num_of_features))\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    j = 0\n",
    "    for packet in train:\n",
    "        if j < rounded_mean and packet is not None:\n",
    "            test_matrix[i][j][0] = packet['Delta Time']\n",
    "            test_matrix[i][j][1] = packet['Length']\n",
    "            test_matrix[i][j][2] = packet['Payload Length']\n",
    "            test_matrix[i][j][3] = 1 if packet['Source Zigbee'] == '0x0000' else 0 # 1 if coordinator, 0 if not\n",
    "            test_matrix[i][j][4] = 0 if pd.isna(packet['FCF Zigbee']) else 1 # 1 if zigbee, 0 if not\n",
    "\n",
    "            # iat from last ingoing or outgoing packet\n",
    "            if test_matrix[i][j][3] == 1:\n",
    "                if seen_1:\n",
    "                    sum_delta_0 += test_matrix[i][j][0]\n",
    "                    sum_delta_1 += test_matrix[i][j][0]\n",
    "                    test_matrix[i][j][5] = sum_delta_1\n",
    "                    sum_delta_1 = 0\n",
    "                else:\n",
    "                    test_matrix[i][j][5] = 0\n",
    "                    seen_1 = True\n",
    "                    if seen_0:\n",
    "                        sum_delta_0 += test_matrix[i][j][0]\n",
    "            else:\n",
    "                if seen_0:\n",
    "                    sum_delta_0 += test_matrix[i][j][0]\n",
    "                    sum_delta_1 += test_matrix[i][j][0]\n",
    "                    test_matrix[i][j][5] = sum_delta_0\n",
    "                    sum_delta_0 = 0\n",
    "                else:\n",
    "                    test_matrix[i][j][5] = 0\n",
    "                    seen_0 = True\n",
    "                    if seen_1:\n",
    "                        sum_delta_1 += test_matrix[i][j][0]\n",
    "            j += 1\n",
    "\n",
    "\n",
    "test_device_address_list = []\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    test_device_address_list.append(device_name_mapping[train[0]['Source Zigbee']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Encode the device address\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(device_address_list)\n",
    "encoded_Y = encoder.transform(device_address_list)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrix_for_binn, encoded_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted') # weighted average gives higher f1 score (0.84), macro gives 0.56\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Instantiate ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "# Load the best model\n",
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and testing on different dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Encode the device address\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(device_address_list)\n",
    "encoded_Y = encoder.transform(device_address_list)\n",
    "encoded_Y_test = encoder.transform(test_device_address_list)\n",
    "\n",
    "# Select features to use\n",
    "features_to_use = [0, 1, 2, 3, 4]\n",
    "num_of_features = len(features_to_use)\n",
    "\n",
    "X_train = matrix_for_binn[:, :, features_to_use]\n",
    "y_train = encoded_Y\n",
    "X_test = test_matrix[:, :, features_to_use]\n",
    "y_test = encoded_Y_test\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "sampling = True\n",
    "\n",
    "if sampling:\n",
    "    # Convert device_address_list to numpy array for easy indexing\n",
    "    labels = np.array(encoded_Y)\n",
    "\n",
    "    # Identify the unique class labels\n",
    "    unique_classes = np.unique(labels)\n",
    "\n",
    "    # Initialize empty lists to store the balanced dataset\n",
    "    balanced_features = []\n",
    "    balanced_labels = []\n",
    "\n",
    "    # Set the desired number of samples per class\n",
    "    samples_per_class = 200\n",
    "\n",
    "    # Iterate over each class and perform class-wise sampling\n",
    "    for class_label in unique_classes:\n",
    "        # Get indices of instances belonging to the current class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "        # Check if the class has enough samples for sampling\n",
    "        if len(class_indices) >= samples_per_class:\n",
    "            # Sample a fixed number of instances from the current class\n",
    "            sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "            # Append the sampled instances to the balanced dataset\n",
    "            balanced_features.extend(matrix_for_binn[sampled_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[sampled_indices])\n",
    "        else:\n",
    "            # If the class has fewer than 50 samples, include all of them\n",
    "            balanced_features.extend(matrix_for_binn[class_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "    # Convert the balanced dataset to numpy arrays\n",
    "    balanced_features = np.array(balanced_features)\n",
    "    balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "    X_train = balanced_features\n",
    "    y_train = balanced_labels\n",
    "    number_of_epochs = 50\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "#model.add(Bidirectional(LSTM(16)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Instantiate ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=number_of_epochs, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "# Load the best model\n",
    "print('Loading best model...')\n",
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "consensus = True\n",
    "occurrence_threshold = 35\n",
    "if consensus:\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "\n",
    "decoded_y_test = encoder.inverse_transform(y_test)\n",
    "decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "classes = sorted(set(test_device_address_list))\n",
    "\n",
    "print(classification_report(decoded_y_test, decoded_y_pred_classes, target_names=classes))\n",
    "\n",
    "cm = confusion_matrix(decoded_y_test, decoded_y_pred_classes, normalize='true', labels=classes)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#plt.title('Confusion Matrix first 4 features with sampling')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, f\"{cm[i, j]:.2f}\", ha='center', va='center', color='white' if cm[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"identification_consensus_v2.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Function to calculate F1 score for given occurrence_threshold\n",
    "def calculate_f1_score(occurrence_threshold):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "    # Inverse transform labels\n",
    "    decoded_y_test = encoder.inverse_transform(y_test)\n",
    "    decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(decoded_y_test, decoded_y_pred_classes, average='weighted')\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Vary occurrence_threshold values\n",
    "occurrence_thresholds = range(1, 40)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in occurrence_thresholds:\n",
    "    f1 = calculate_f1_score(threshold)\n",
    "    f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = False\n",
    "if smooth:\n",
    "    f1_interp = interp1d(occurrence_thresholds, f1_scores, kind='cubic')\n",
    "    new_thresholds = np.linspace(min(occurrence_thresholds), max(occurrence_thresholds), 100)\n",
    "    smooth_f1_scores = f1_interp(new_thresholds)\n",
    "    plt.plot(new_thresholds, smooth_f1_scores, marker='', linestyle='-')\n",
    "else:\n",
    "    plt.plot(occurrence_thresholds, f1_scores, marker='.', linestyle='-')\n",
    "\n",
    "plt.xlabel('Occurrence Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Effect of Occurrence Threshold on F1 Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"asyntotic_consensus_cl.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(encoder.inverse_transform(encoded_Y), return_counts=True)\n",
    "value_counts = dict(zip(unique, counts))\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming 'labels' is your array, DataFrame, or list of class labels\n",
    "if isinstance(device_address_list, (np.ndarray, pd.Series)):\n",
    "    class_distribution = np.unique(device_address_list, return_counts=True)\n",
    "else:\n",
    "    class_distribution = Counter(device_address_list)\n",
    "\n",
    "plt.bar(class_distribution.keys(), class_distribution.values())\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional Neural Network with balanced dataset\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert device_address_list to numpy array for easy indexing\n",
    "labels = np.array(encoded_Y)\n",
    "\n",
    "# Identify the unique class labels\n",
    "unique_classes = np.unique(labels)\n",
    "\n",
    "# Initialize empty lists to store the balanced dataset\n",
    "balanced_features = []\n",
    "balanced_labels = []\n",
    "\n",
    "# Set the desired number of samples per class\n",
    "samples_per_class = 50\n",
    "\n",
    "# Iterate over each class and perform class-wise sampling\n",
    "for class_label in unique_classes:\n",
    "    # Get indices of instances belonging to the current class\n",
    "    class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "    # Check if the class has enough samples for sampling\n",
    "    if len(class_indices) >= samples_per_class:\n",
    "        # Sample a fixed number of instances from the current class\n",
    "        sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "        # Append the sampled instances to the balanced dataset\n",
    "        balanced_features.extend(matrix_for_binn[sampled_indices])\n",
    "        balanced_labels.extend(labels[sampled_indices])\n",
    "    else:\n",
    "        # If the class has fewer than 50 samples, include all of them\n",
    "        balanced_features.extend(matrix_for_binn[class_indices])\n",
    "        balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "# Convert the balanced dataset to numpy arrays\n",
    "balanced_features = np.array(balanced_features)\n",
    "balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "# Split the balanced dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_features, balanced_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[f1_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single DataFrame with all the packets and 'id' column to identify the train of packets\n",
    "\n",
    "all_packets = list()\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    for packet in train:\n",
    "        if packet is not None:\n",
    "            all_packets.append({\n",
    "                    'id': i,\n",
    "                    'Time': packet['Time'],\n",
    "                    'Delta Time': packet['Delta Time'],\n",
    "                    'Length': packet['Length'],\n",
    "                    'Sequence Number': packet['Sequence Number'],\n",
    "                    'Payload Length': packet['Payload Length'] if not np.isnan(packet['Payload Length']) else 0,\n",
    "                    'FCF Zigbee': int(packet['FCF Zigbee'], 0),\n",
    "                    'FCF IEEE': int(packet['FCF IEEE'], 0),\n",
    "                    'Action': packet['Action']})\n",
    "\n",
    "coordinator_result_df = pd.DataFrame(all_packets)\n",
    "\n",
    "# Set Delta Time of the first packet of each train to 0\n",
    "id_changes = coordinator_result_df['id'] != coordinator_result_df['id'].shift(1)\n",
    "coordinator_result_df.loc[id_changes, 'Delta Time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tsfresh used to get features from trains of packets\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters, ComprehensiveFCParameters\n",
    "\n",
    "coordinator_result_df['Time'] = pd.to_datetime(coordinator_result_df['Time'], format=\"%b %d, %Y %H:%M:%S.%f\")\n",
    "\n",
    "# Remove the 'Action' column for the feature extraction\n",
    "column_to_exclude = 'Action'\n",
    "excluded_column = coordinator_result_df.pop(column_to_exclude)\n",
    "\n",
    "# Extract features from the dataframe\n",
    "extracted_features = extract_features(coordinator_result_df, column_id=\"id\", column_sort=\"Time\", default_fc_parameters=MinimalFCParameters())\n",
    "\n",
    "# Impute the extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Reinsert the 'Action' column\n",
    "coordinator_result_df = pd.concat([coordinator_result_df, excluded_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the device address to the dataframe of features to be able to classify the packets\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    extracted_features.loc[i, 'Source Zigbee'] = device_name_mapping[coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']]\n",
    "    # TODO insert action based on relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    if len(source_action_list) > 1:\n",
    "        print(f\"{i}) {device_name_mapping[source]}: {source_action_list} -> {highest_priority_action}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '0x3d95'\n",
    "source_action_list = ['Report Attributes (0x0a)', 'APS: Ack']\n",
    "print(device_name_mapping[source])\n",
    "\n",
    "highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "print(highest_priority_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# Set the occurrence threshold for label-wise consensus\n",
    "occurrence_threshold = 5\n",
    "occurrences_used = False\n",
    "\n",
    "# Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "for label in np.unique(y_test):\n",
    "    indices = np.where(y_test == label)[0]\n",
    "\n",
    "    # Check if the label has occurred at least occurrence_threshold times\n",
    "    if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "        \n",
    "        # Identify the most common prediction within each occurrence_threshold group\n",
    "        for i in range(0, len(indices), occurrence_threshold):\n",
    "            group_indices = indices[i:i+occurrence_threshold]\n",
    "            most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "            most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[group_indices] = most_frequent_label\n",
    "        \n",
    "    else:\n",
    "        most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "        most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "        # Replace all predictions for this label within the group with the most common value\n",
    "        y_pred[indices] = most_frequent_label\n",
    "\n",
    "\n",
    "# Calculate and store metrics for each fold after label-wise consensus\n",
    "accuracy_consensus = accuracy_score(y_test, y_pred)\n",
    "precision_consensus = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall_consensus = recall_score(y_test, y_pred, average='macro')\n",
    "f1_consensus = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('')\n",
    "print(f\"Accuracy: {accuracy_consensus}\")\n",
    "print(f\"Precision: {precision_consensus}\")\n",
    "print(f\"Recall: {recall_consensus}\")\n",
    "print(f\"F1: {f1_consensus}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "unique_mapped_labels = sorted(set(device_name_mapping.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_mapped_labels)\n",
    "disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "accuracy_scores_consensus = []\n",
    "precision_scores_consensus = []\n",
    "recall_scores_consensus = []\n",
    "f1_scores_consensus = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the RandomForestClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # Set the occurrence threshold for label-wise consensus\n",
    "    occurrence_threshold = 10\n",
    "    occurrences_used = True\n",
    "\n",
    "    # Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "            \n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "                most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred[group_indices] = most_frequent_label\n",
    "        else:\n",
    "            most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "            most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[indices] = most_frequent_label\n",
    "\n",
    "    # Calculate and store metrics for each fold after label-wise consensus\n",
    "    accuracy_scores_consensus.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores_consensus.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores_consensus.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores_consensus.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Average Metrics Across Folds after Label-wise Consensus:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores_consensus) / len(accuracy_scores_consensus)}\")\n",
    "print(f\"Precision: {sum(precision_scores_consensus) / len(precision_scores_consensus)}\")\n",
    "print(f\"Recall: {sum(recall_scores_consensus) / len(recall_scores_consensus)}\")\n",
    "print(f\"F1: {sum(f1_scores_consensus) / len(f1_scores_consensus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score before and after label-wise consensus for different occurrence thresholds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store F1 scores before and after consensus for different occurrence thresholds\n",
    "f1_scores_before_consensus = []\n",
    "f1_scores_after_consensus = []\n",
    "\n",
    "# Define a range of occurrence thresholds from 1 to 20\n",
    "occurrence_threshold_range = range(1, 21)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for occurrence_threshold in occurrence_threshold_range:\n",
    "    # Lists to store F1 scores for each fold\n",
    "    f1_scores = []\n",
    "    f1_scores_consensus = []\n",
    "\n",
    "    for train_index, test_index in stratified_kfold.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the RandomForestClassifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate F1 score before consensus\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "        # Perform label-wise consensus\n",
    "        for label in np.unique(y_test):\n",
    "            indices = np.where(y_test == label)[0]\n",
    "\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "                most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred[group_indices] = most_frequent_label\n",
    "\n",
    "        # Calculate F1 score after consensus\n",
    "        f1_scores_consensus.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # Store average F1 scores for the current occurrence threshold\n",
    "    f1_scores_before_consensus.append(sum(f1_scores) / len(f1_scores))\n",
    "    f1_scores_after_consensus.append(sum(f1_scores_consensus) / len(f1_scores_consensus))\n",
    "\n",
    "# Plot the F1 scores before and after consensus\n",
    "plt.plot(occurrence_threshold_range, f1_scores_before_consensus, label='Before Consensus')\n",
    "plt.plot(occurrence_threshold_range, f1_scores_after_consensus, label='After Consensus')\n",
    "plt.xlabel('Occurrence Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score Before and After Label-wise Consensus')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to every row of extracted_features the number of packets that fall inside the windows of threshold / 10\n",
    "\n",
    "N = 10\n",
    "for i in range(len(extracted_features)):\n",
    "    mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    deltas = coordinator_result_df[coordinator_result_df['id'] == i]['Delta Time']\n",
    "    for n in range(N):\n",
    "        lower_bound = n * (threshold_float / N)\n",
    "        upper_bound = (n + 1) * (threshold_float / N)\n",
    "        count = ((deltas >= lower_bound) & \n",
    "                 (deltas < upper_bound)).sum()\n",
    "\n",
    "        # Add the count to the extracted features\n",
    "        match n:\n",
    "            case 0:\n",
    "                extracted_features.loc[i, 'Window 1'] = count\n",
    "            case 1:\n",
    "                extracted_features.loc[i, 'Window 2'] = count\n",
    "            case 2:\n",
    "                extracted_features.loc[i, 'Window 3'] = count\n",
    "            case 3:\n",
    "                extracted_features.loc[i, 'Window 4'] = count\n",
    "            case 4:\n",
    "                extracted_features.loc[i, 'Window 5'] = count\n",
    "            case 5:\n",
    "                extracted_features.loc[i, 'Window 6'] = count\n",
    "            case 6:\n",
    "                extracted_features.loc[i, 'Window 7'] = count\n",
    "            case 7:\n",
    "                extracted_features.loc[i, 'Window 8'] = count\n",
    "            case 8:\n",
    "                extracted_features.loc[i, 'Window 9'] = count\n",
    "            case 9:\n",
    "                extracted_features.loc[i, 'Window 10'] = count\n",
    "            case _:\n",
    "                print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering on window counts\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "window_features = extracted_features.drop(columns=['Source Zigbee'])\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=16).fit(window_features)\n",
    "clustering.labels_\n",
    "\n",
    "# Adding the cluster labels to the extracted features\n",
    "extracted_features['Cluster'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Window 1', y='Window 2', hue='Cluster', data=extracted_features, palette='viridis', legend='full')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Window 1')\n",
    "plt.ylabel('Window 2')\n",
    "plt.title('Hierarchical Clustering with Source Zigbee Information')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Cluster')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pairwise relationships between the features\n",
    "features_to_plot = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                       'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10', 'Cluster']]\n",
    "\n",
    "# Create a pairplot\n",
    "sns.pairplot(features_to_plot, hue='Cluster', palette='viridis')\n",
    "plt.suptitle('Pairplot of Hierarchical Clustering with Source Zigbee Information', y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between windows and average of windows\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "window_features = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                      'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10']]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(window_features)\n",
    "\n",
    "# Calculate the average cosine similarity for each window\n",
    "average_cosine_similarity = np.mean(cosine_sim, axis=1)\n",
    "\n",
    "# Add the average cosine similarity as a new feature\n",
    "extracted_features['Average Cosine Similarity'] = average_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between windows and array of 1s\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Select features for cosine similarity calculation\n",
    "window_features = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                      'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10']]\n",
    "\n",
    "# Create a vector full of 1s\n",
    "vector_of_ones = np.ones((1, window_features.shape[1]))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim_with_ones = cosine_similarity(window_features, vector_of_ones)\n",
    "\n",
    "# Add the cosine similarity with diagonal as a new feature\n",
    "extracted_features['Cosine Similarity With Diagonal'] = cosine_sim_with_ones[:, 0]  # Assuming you want to use the first column of the cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_feature_importances = sorted(zip(X.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Range of top features to explore (e.g., from 1 to the total number of features)\n",
    "num_features_to_explore = range(1, len(sorted_feature_importances) + 1)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {'num_features': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "# Perform k-fold cross-validation for each number of top features\n",
    "for num_features in num_features_to_explore:\n",
    "    print(f\"Trying with {num_features} top features...\")\n",
    "\n",
    "    # Choose the top N features\n",
    "    top_n_features = [feature for feature, importance in sorted_feature_importances[:num_features]]\n",
    "\n",
    "    # Subset the data with the top features\n",
    "    X_selected = X[top_n_features]\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    y_pred = cross_val_predict(clf, X_selected, y, cv=stratified_kfold)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y, y_pred, average='macro')\n",
    "    f1 = f1_score(y, y_pred, average='macro')\n",
    "\n",
    "    # Store results\n",
    "    results['num_features'].append(num_features)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['precision'].append(precision)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    \n",
    "\n",
    "# Print results\n",
    "for i in range(len(results['num_features'])):\n",
    "    print(f\"Num Features: {results['num_features'][i]}, Accuracy: {results['accuracy'][i]}, Precision: {results['precision'][i]}, Recall: {results['recall'][i]}, F1: {results['f1'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(results['num_features'], results['accuracy'], label='Accuracy', marker='.')\n",
    "\n",
    "# Plot precision\n",
    "plt.plot(results['num_features'], results['precision'], label='Precision', marker='.')\n",
    "\n",
    "# Plot recall\n",
    "plt.plot(results['num_features'], results['recall'], label='Recall', marker='.')\n",
    "\n",
    "# Plot F1 score\n",
    "plt.plot(results['num_features'], results['f1'], label='F1 Score', marker='.')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Number of Top Features')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Metrics vs. Number of Top Features')\n",
    "plt.legend()\n",
    "\n",
    "# Find the index where each metric is maximized\n",
    "idx_max_accuracy = results['accuracy'].index(max(results['accuracy']))\n",
    "idx_max_precision = results['precision'].index(max(results['precision']))\n",
    "idx_max_recall = results['recall'].index(max(results['recall']))\n",
    "idx_max_f1 = results['f1'].index(max(results['f1']))\n",
    "\n",
    "# Print the number of features and value where each metric is maximized\n",
    "print(f\"Max Accuracy at {results['num_features'][idx_max_accuracy]} features with value {max(results['accuracy'])}\")\n",
    "print(f\"Max Precision at {results['num_features'][idx_max_precision]} features with value {max(results['precision'])}\")\n",
    "print(f\"Max Recall at {results['num_features'][idx_max_recall]} features with value {max(results['recall'])}\")\n",
    "print(f\"Max F1 Score at {results['num_features'][idx_max_f1]} features with value {max(results['f1'])}\")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by importance in descending order\n",
    "sorted_feature_importances = sorted(zip(X.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the names of the top features and their importance scores\n",
    "for feature, importance in sorted_feature_importances:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- ACTION CLASSIFICATION -------------------------------------------------- ###\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_counts = df.groupby('Source Zigbee')['Action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in grouped.index:\n",
    "    if device in devices:\n",
    "        first_value = True\n",
    "        for value in grouped[device]:\n",
    "            if first_value:\n",
    "                print(f\"{device}: {value}\")\n",
    "                first_value = False\n",
    "            else:\n",
    "                print(f\"\\t{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of actions for a certain class and select the rows of the train matrix that correspond to the class\n",
    "\n",
    "# List to keep track of the device class of each train source\n",
    "device_address_list = []\n",
    "class_considered = 'Motion'\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    device_address_list.append(device_type_mapping[train[0]['Source Zigbee']])\n",
    "\n",
    "device_address_series = pd.Series(device_address_list)\n",
    "mask = (device_address_series == class_considered)\n",
    "\n",
    "mask_index = [int(i) for i in mask.index[mask]]\n",
    "\n",
    "action_list = []\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    action_list.append(highest_priority_action)\n",
    "\n",
    "filtered_action_list = [action_list[i] for i in mask.index[mask]]\n",
    "filtered_device_list = [device_address_list[i] for i in mask.index[mask]]\n",
    "\n",
    "matrix_action = matrix_for_binn[mask_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_action_series = pd.Series(filtered_action_list)\n",
    "\n",
    "# Use value_counts() on the Series\n",
    "value_counts_result = filtered_action_series.value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(value_counts_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of actions for a certain class and select the rows of the test matrix that correspond to the class\n",
    "\n",
    "# List to keep track of the device class of each train source\n",
    "test_device_address_list = []\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    test_device_address_list.append(device_type_mapping[train[0]['Source Zigbee']])\n",
    "\n",
    "test_device_address_series = pd.Series(test_device_address_list)\n",
    "mask = (test_device_address_series == class_considered)\n",
    "\n",
    "mask_index = [int(i) for i in mask.index[mask]]\n",
    "\n",
    "test_action_list = []\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    test_action_list.append(highest_priority_action)\n",
    "\n",
    "test_filtered_action_list = [test_action_list[i] for i in mask.index[mask]]\n",
    "test_filtered_device_list = [test_device_address_list[i] for i in mask.index[mask]]\n",
    "\n",
    "test_matrix_action = test_matrix[mask_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_action_series = pd.Series(test_filtered_action_list)\n",
    "\n",
    "# Use value_counts() on the Series\n",
    "value_counts_result = filtered_action_series.value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(value_counts_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and testing on different dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Encode the device address\n",
    "encoder = LabelEncoder()\n",
    "all_action_list = np.concatenate([filtered_action_list, test_filtered_action_list])\n",
    "encoder.fit(all_action_list)\n",
    "encoded_actions = encoder.transform(all_action_list)\n",
    "encoded_Y = encoder.transform(filtered_action_list)\n",
    "encoded_Y_test = encoder.transform(test_filtered_action_list)\n",
    "\n",
    "# Select features to use\n",
    "features_to_use = [0, 1, 2, 3]\n",
    "num_of_features = len(features_to_use)\n",
    "\n",
    "X_train = matrix_action[:, :, features_to_use]\n",
    "y_train = encoded_Y\n",
    "X_test = test_matrix_action[:, :, features_to_use]\n",
    "y_test = encoded_Y_test\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "sampling = False\n",
    "\n",
    "if sampling:\n",
    "    # Convert device_address_list to numpy array for easy indexing\n",
    "    labels = np.array(encoded_Y)\n",
    "\n",
    "    # Identify the unique class labels\n",
    "    unique_classes = np.unique(labels)\n",
    "\n",
    "    # Initialize empty lists to store the balanced dataset\n",
    "    balanced_features = []\n",
    "    balanced_labels = []\n",
    "\n",
    "    # Set the desired number of samples per class\n",
    "    samples_per_class = 200\n",
    "\n",
    "    # Iterate over each class and perform class-wise sampling\n",
    "    for class_label in unique_classes:\n",
    "        # Get indices of instances belonging to the current class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "        # Check if the class has enough samples for sampling\n",
    "        if len(class_indices) >= samples_per_class:\n",
    "            # Sample a fixed number of instances from the current class\n",
    "            sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "            # Append the sampled instances to the balanced dataset\n",
    "            balanced_features.extend(matrix_for_binn[sampled_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[sampled_indices])\n",
    "        else:\n",
    "            # If the class has fewer than 50 samples, include all of them\n",
    "            balanced_features.extend(matrix_for_binn[class_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "    # Convert the balanced dataset to numpy arrays\n",
    "    balanced_features = np.array(balanced_features)\n",
    "    balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "    X_train = balanced_features\n",
    "    y_train = balanced_labels\n",
    "    number_of_epochs = 50\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "#model.add(Bidirectional(LSTM(16)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_actions)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Instantiate ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=number_of_epochs, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "# Load the best model\n",
    "print('Loading best model...')\n",
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "consensus = False\n",
    "occurrence_threshold = 35\n",
    "if consensus:\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "decoded_y_test = encoder.inverse_transform(y_test)\n",
    "decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "classes = sorted(set(test_filtered_action_list))\n",
    "\n",
    "print(classification_report(decoded_y_test, decoded_y_pred_classes, target_names=classes))\n",
    "\n",
    "cm = confusion_matrix(decoded_y_test, decoded_y_pred_classes, normalize='true', labels=classes)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#plt.title('Confusion Matrix first 4 features with sampling')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, f\"{cm[i, j]:.2f}\", ha='center', va='center', color='white' if cm[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"events_bulb.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- TESTING ON DIFFERENT DATASET -------------------------------------------------- ###\n",
    "\n",
    "training_extracted_features = extracted_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X_train = training_extracted_features.drop(columns=['Source Zigbee'])\n",
    "y_train = training_extracted_features['Source Zigbee']\n",
    "\n",
    "X_test = test_extracted_features.drop(columns=['Source Zigbee'])\n",
    "y_test = test_extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# Set the occurrence threshold for label-wise consensus\n",
    "occurrence_threshold = 5\n",
    "occurrences_used = True\n",
    "\n",
    "# Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "for label in np.unique(y_test):\n",
    "    indices = np.where(y_test == label)[0]\n",
    "\n",
    "    # Check if the label has occurred at least occurrence_threshold times\n",
    "    if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "        \n",
    "        # Identify the most common prediction within each occurrence_threshold group\n",
    "        for i in range(0, len(indices), occurrence_threshold):\n",
    "            group_indices = indices[i:i+occurrence_threshold]\n",
    "            most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "            most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[group_indices] = most_frequent_label\n",
    "    else:\n",
    "        most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "        most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "        # Replace all predictions for this label within the group with the most common value\n",
    "        y_pred[indices] = most_frequent_label\n",
    "\n",
    "\n",
    "# Calculate and store metrics for each fold after label-wise consensus\n",
    "accuracy_consensus = accuracy_score(y_test, y_pred)\n",
    "precision_consensus = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall_consensus = recall_score(y_test, y_pred, average='macro')\n",
    "f1_consensus = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('')\n",
    "print(f\"Accuracy: {accuracy_consensus}\")\n",
    "print(f\"Precision: {precision_consensus}\")\n",
    "print(f\"Recall: {recall_consensus}\")\n",
    "print(f\"F1: {f1_consensus}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "unique_mapped_labels = sorted(set(device_name_mapping.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_mapped_labels)\n",
    "disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
