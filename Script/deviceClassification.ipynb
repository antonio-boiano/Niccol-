{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and plotlib kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import warnings\n",
    "import Levenshtein as lev\n",
    "import tsfresh\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import mpldatacursor\n",
    "mpldatacursor.datacursor()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.2'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tsfresh\n",
    "tsfresh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tikzplotlib\n",
    "\n",
    "def save_tikzplotlib(fig,path):\n",
    "    def tikzplotlib_fix_ncols(obj):\n",
    "        \"\"\"\n",
    "        workaround for matplotlib 3.6 renamed legend's _ncol to _ncols, which breaks tikzplotlib\n",
    "        \"\"\"\n",
    "        if hasattr(obj, \"_ncols\"):\n",
    "            obj._ncol = obj._ncols\n",
    "        for child in obj.get_children():\n",
    "            tikzplotlib_fix_ncols(child)\n",
    "    tikzplotlib_fix_ncols(fig)\n",
    "    tikzplotlib.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping(version):\n",
    "    global device_name_mapping_simple\n",
    "    global device_name_mapping \n",
    "    global device_type_mapping \n",
    "    global devices_name    \n",
    "    global devices\n",
    "    \n",
    "    if version == 2:\n",
    "\n",
    "        devices = ['0x0000', '0x265e', '0x3181', '0x0a79', '0xb815', '0x4e52', '0x9989', '0x5db6', '0x772c', '0x61af', '0xe011', '0xe694','0x6e5f', '0xc8f0', '0x09ac', '0x82eb', '0x054f', '0xebe5',  '0xe5c4', '0x482d', '0xa209', '0x2cae'];\n",
    "\n",
    "        devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', \n",
    "                        'Aqara Button', 'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb',  'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "\n",
    "        device_type_mapping = {\n",
    "            '0x0000' : 'Coordinator',\n",
    "            '0x09ac' : 'Socket',\n",
    "            '0x5db6' : 'Door',\n",
    "            '0xe694' : 'Socket',\n",
    "            '0x772c' : 'Door',\n",
    "            '0x61af' : 'Vibration',\n",
    "            '0xe011' : 'Button',\n",
    "            '0x9989' : 'Motion',\n",
    "            '0x265e' : 'Temperature',\n",
    "            '0xb815' : 'Motion',\n",
    "            '0x4e52' : 'Motion',\n",
    "            '0x3181' : 'Door',\n",
    "            '0x0a79' : 'Door',\n",
    "            '0x82eb' : 'Socket',\n",
    "            '0x6e5f' : 'Socket',\n",
    "            '0xc8f0' : 'Socket',\n",
    "            '0xebe5' : 'Bulb',\n",
    "            '0x054f' : 'Bulb',\n",
    "            '0x482d' : 'Bulb',\n",
    "            '0xa209' : 'Bulb',\n",
    "            '0x2cae' : 'Motion',\n",
    "            '0xe5c4' : 'Bulb'\n",
    "        }\n",
    "\n",
    "        device_name_mapping = {\n",
    "            '0x0000' : 'Coordinator',\n",
    "            '0x09ac' : 'Ledvance Z3 Plug',\n",
    "            '0x5db6' : 'Aqara Door 1',\n",
    "            '0xe694' : 'Smart Socket',\n",
    "            '0x772c' : 'Aqara Door 2',\n",
    "            '0x61af' : 'Aqara Vibration',\n",
    "            '0xe011' : 'Aqara Button',\n",
    "            '0x9989' : 'Aqara Motion',\n",
    "            '0x265e' : 'Sonoff Temperature',\n",
    "            '0xb815' : 'Sonoff Motion 1',\n",
    "            '0x4e52' : 'Sonoff Motion 2',\n",
    "            '0x3181' : 'Sonoff Door 1',\n",
    "            '0x0a79' : 'Sonoff Door 2',\n",
    "            '0x82eb' : 'Ledvance Smart+ Plug',\n",
    "            '0x6e5f' : 'Power Plug 1',\n",
    "            '0xc8f0' : 'Power Plug 2',\n",
    "            '0xebe5' : 'Moes Bulb',\n",
    "            '0x054f' : 'Ledvance Bulb',\n",
    "            '0x482d' : 'Philips Lamp 1',\n",
    "            '0xa209' : 'Philips Lamp 2',\n",
    "            '0x2cae' : 'Philips Lamp 3',\n",
    "            '0xe5c4' : 'Philips Motion'\n",
    "        }\n",
    "\n",
    "        device_name_mapping_simple = {\n",
    "            '0x0000' : 'Coordinator',\n",
    "            '0x09ac' : 'Ledvance Z3 Plug',\n",
    "            '0x5db6' : 'Aqara Door',\n",
    "            '0xe694' : 'Smart Socket',\n",
    "            '0x772c' : 'Aqara Door',\n",
    "            '0x61af' : 'Aqara Vibration',\n",
    "            '0xe011' : 'Aqara Button',\n",
    "            '0x9989' : 'Aqara Motion',\n",
    "            '0x265e' : 'Sonoff Temperature',\n",
    "            '0xb815' : 'Sonoff Motion',\n",
    "            '0x4e52' : 'Sonoff Motion',\n",
    "            '0x3181' : 'Sonoff Door',\n",
    "            '0x0a79' : 'Sonoff Door',\n",
    "            '0x82eb' : 'Ledvance Smart+ Plug',\n",
    "            '0x6e5f' : 'Power Plug',\n",
    "            '0xc8f0' : 'Power Plug',\n",
    "            '0xebe5' : 'Moes Bulb',\n",
    "            '0x054f' : 'Ledvance Bulb',\n",
    "            '0x482d' : 'Philips Lamp',\n",
    "            '0xa209' : 'Philips Lamp',\n",
    "            '0x2cae' : 'Philips Lamp',\n",
    "            '0xe5c4' : 'Philips Motion'\n",
    "        }\n",
    "    else:\n",
    "        devices = ['0x0000', '0x4615', '0x946e', '0x7b10', '0xd0bb', '0x1f29', '0x27d7', '0x907b', '0xe01d', '0x187a', '0xc31c',\n",
    "        '0xe1a6', '0x3d95', '0xa706', '0x4e11', '0x0112', '0xec7f', '0x1e15', '0x1cd8', '0x5bb9', '0x711c', '0x059b']\n",
    "\n",
    "        devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', 'Aqara Button',\n",
    "                        'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb', 'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "        device_type_mapping = {\n",
    "            '0x0000': 'Coordinator',\n",
    "            '0x4615': 'Temperature',\n",
    "            '0x946e': 'Door',\n",
    "            '0x7b10': 'Door',\n",
    "            '0xd0bb': 'Motion',\n",
    "            '0x1f29': 'Motion',\n",
    "            '0x27d7': 'Motion',\n",
    "            '0x907b': 'Door',\n",
    "            '0xe01d': 'Door',\n",
    "            '0x187a': 'Vibration',\n",
    "            '0xc31c': 'Button',\n",
    "            '0xe1a6': 'Socket',\n",
    "            '0x3d95': 'Socket',\n",
    "            '0xa706': 'Socket',\n",
    "            '0x4e11': 'Socket',\n",
    "            '0x0112': 'Socket',\n",
    "            '0xec7f': 'Bulb',\n",
    "            '0x1e15': 'Bulb',\n",
    "            '0x1cd8': 'Bulb',\n",
    "            '0x5bb9': 'Bulb',\n",
    "            '0x711c': 'Bulb',\n",
    "            '0x059b': 'Motion'    \n",
    "        }\n",
    "\n",
    "        device_name_mapping = {\n",
    "            '0x0000': 'Coordinator',\n",
    "            '0x4615': 'Sonoff Temperature',\n",
    "            '0x946e': 'Sonoff Door 1',\n",
    "            '0x7b10': 'Sonoff Door 2',\n",
    "            '0xd0bb': 'Sonoff Motion 1',\n",
    "            '0x1f29': 'Sonoff Motion 2',\n",
    "            '0x27d7': 'Aqara Motion',\n",
    "            '0x907b': 'Aqara Door 1',\n",
    "            '0xe01d': 'Aqara Door 2',\n",
    "            '0x187a': 'Aqara Vibration',\n",
    "            '0xc31c': 'Aqara Button',\n",
    "            '0xe1a6': 'Smart Socket',\n",
    "            '0x3d95': 'Power Plug 1',\n",
    "            '0xa706': 'Power Plug 2',\n",
    "            '0x4e11': 'Ledvance Z3 Plug',\n",
    "            '0x0112': 'Ledvance Smart+ Plug',\n",
    "            '0xec7f': 'Ledvance Bulb',\n",
    "            '0x1e15': 'Moes Bulb',\n",
    "            '0x1cd8': 'Philips Lamp 1',\n",
    "            '0x5bb9': 'Philips Lamp 2',\n",
    "            '0x711c': 'Philips Lamp 3',\n",
    "            '0x059b': 'Philips Motion' \n",
    "        }\n",
    "\n",
    "        device_name_mapping_simple = {\n",
    "            '0x0000': 'Coordinator',\n",
    "            '0x4615': 'Sonoff Temperature',\n",
    "            '0x946e': 'Sonoff Door',\n",
    "            '0x7b10': 'Sonoff Door',\n",
    "            '0xd0bb': 'Sonoff Motion',\n",
    "            '0x1f29': 'Sonoff Motion',\n",
    "            '0x27d7': 'Aqara Motion',\n",
    "            '0x907b': 'Aqara Door',\n",
    "            '0xe01d': 'Aqara Door',\n",
    "            '0x187a': 'Aqara Vibration',\n",
    "            '0xc31c': 'Aqara Button',\n",
    "            '0xe1a6': 'Smart Socket',\n",
    "            '0x3d95': 'Power Plug',\n",
    "            '0xa706': 'Power Plug',\n",
    "            '0x4e11': 'Ledvance Z3 Plug',\n",
    "            '0x0112': 'Ledvance Smart+ Plug',\n",
    "            '0xec7f': 'Ledvance Bulb',\n",
    "            '0x1e15': 'Moes Bulb',\n",
    "            '0x1cd8': 'Philips Lamp',\n",
    "            '0x5bb9': 'Philips Lamp',\n",
    "            '0x711c': 'Philips Lamp',\n",
    "            '0x059b': 'Philips Motion' \n",
    "        }\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority ranking for commands\n",
    "\n",
    "command_priorities = {\n",
    "    'Sonoff Temperature': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Sonoff Door': {\n",
    "        'Zone Status Change Notification (0x00)': 3,\n",
    "        'Report Attributes (0x0a)': 2,\n",
    "    },\n",
    "\n",
    "    'Sonoff Motion': {\n",
    "        'Zone Status Change Notification (0x00)': 7,\n",
    "        'Report Attributes (0x0a)': 6,\n",
    "        'Zone Enroll Request (0x01)': 5,\n",
    "        'Rejoin Request (0x06)': 4,\n",
    "        'zdp': 3,\n",
    "        'Ack': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Aqara Motion': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Aqara Door': {\n",
    "        'Report Attributes (0x0a)': 3,\n",
    "    },\n",
    "\n",
    "    'Aqara Vibration': {\n",
    "        'Report Attributes (0x0a)': 5,\n",
    "        'Rejoin Request (0x06)': 4,\n",
    "        'zdp': 3,\n",
    "        'Ack': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Aqara Button': {\n",
    "        'Report Attributes (0x0a)': 2,\n",
    "        'Route Record (0x05)': 1,\n",
    "    },\n",
    "\n",
    "    'Smart Socket': {\n",
    "        'Report Attributes (0x0a)': 9,\n",
    "        'Read Attributes Response (0x01)': 9,\n",
    "        'Read Attributes (0x00)': 8,\n",
    "        'Default Response (0x0b)': 7,\n",
    "        'APS: Ack': 6,\n",
    "        'Ack': 5,\n",
    "        'Rejoin Response (0x07)': 4,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'zdp': 2,\n",
    "        'Link Status': 1,\n",
    "    },\n",
    "\n",
    "    'Power Plug': {\n",
    "        'Read Attribute Response (0x01)': 9,\n",
    "        'Report Attributes (0x0a)': 9,\n",
    "        'Read Attributes (0x00)': 8,\n",
    "        'Default Response (0x0b)': 7,\n",
    "        'APS: Ack': 6,\n",
    "        'Ack': 5,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'zdp': 2,\n",
    "        'Link Status': 1,\n",
    "    },\n",
    "\n",
    "    'Ledvance Z3 Plug': {\n",
    "        'Link Status': 1,\n",
    "        'Route Record (0x05)': 6,\n",
    "        'Ack': 7,\n",
    "        'Report Attributes (0x0a)': 10,\n",
    "        'APS: Ack': 8,\n",
    "        'zdp': 2,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Get Group Membership Response (0x02)': 5,\n",
    "    },\n",
    "\n",
    "    'Ledvance Smart+ Plug': {\n",
    "        'Link Status': 2,\n",
    "        'Report Attributes (0x0a)': 10,\n",
    "        'Ack': 7,\n",
    "        'zdp': 3,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 4,\n",
    "        '---': 1,\n",
    "        'Get Group Membership Response (0x02)': 6,\n",
    "        'APS: Ack': 8,\n",
    "        'Rejoin Response (0x07)': 5,\n",
    "    },\n",
    "\n",
    "    'Ledvance Bulb': {\n",
    "        'Link Status': 1,\n",
    "        'Ack': 7,\n",
    "        'Route Record (0x05)': 6,\n",
    "        'APS: Ack': 8,\n",
    "        'zdp': 2,\n",
    "        'Report Attributes (0x0a)': 11,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Network Status (0x03)': 4,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Get Group Membership Response (0x02)': 5,\n",
    "    },\n",
    "\n",
    "    'Moes Bulb': {\n",
    "        'Link Status': 1,\n",
    "        'Ack': 4,\n",
    "        'Default Response (0x0b)': 8,\n",
    "        'Read Attribute Response (0x01)': 6,\n",
    "        'Report Attributes (0x0a)': 7,\n",
    "        'APS: Ack': 5,\n",
    "        'zdp': 2,\n",
    "        'Route Reply (0x02)': 3,\n",
    "    },\n",
    "\n",
    "    'Philips Lamp': {\n",
    "        'Link Status': 2,\n",
    "        'Route Record (0x05)': 7,\n",
    "        'zdp': 3,\n",
    "        'Ack': 8,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Report Attributes (0x0a)': 11,\n",
    "        'APS: Ack': 9,\n",
    "        'Default Response (0x0b)': 12,\n",
    "        'Get Group Membership Response (0x02)': 6,\n",
    "        'Route Reply (0x02)': 5,\n",
    "        'Rejoin Response (0x07)': 4,\n",
    "        '---': 1,\n",
    "    },\n",
    "\n",
    "    'Philips Motion': {\n",
    "        'Report Attributes (0x0a)': 6,\n",
    "        'APS: Ack': 5,\n",
    "        'Route Record (0x05)': 2,\n",
    "        'Ack': 4,\n",
    "        'zdp': 1,\n",
    "        'Rejoin Request (0x06)': 3,\n",
    "    },\n",
    "\n",
    "    'Coordinator': {\n",
    "        'Read Attributes (0x00)': 11,\n",
    "        'Ack': 7,\n",
    "        'APS: Ack': 8,\n",
    "        'Link Status': 1,\n",
    "        'Default Response (0x0b)': 9,\n",
    "        'zdp': 2,\n",
    "        'On (0x01)': 12,\n",
    "        'Move To Level with OnOff (0x04)': 14,\n",
    "        'Off (0x00)': 12,\n",
    "        'Route Reply (0x02)': 3,\n",
    "        'Move To Color (0x07)': 13,\n",
    "        'Get Group Membership (0x02)': 4,\n",
    "        'Move To Color Temperature (0x0a)': 14,\n",
    "        'Read Attributes Response (0x01)': 10,\n",
    "        'Configure Reporting (0x06)': 5,\n",
    "        'Color Loop Set (0x44)': 15,\n",
    "        'Rejoin Response (0x07)': 6,\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_highest_priority_action(device_name, action_list):\n",
    "    if device_name in command_priorities:\n",
    "        # Get the priorities for the specified device\n",
    "        device_priorities = command_priorities[device_name]\n",
    "\n",
    "        # Filter the action list to include only those present in the priorities\n",
    "        valid_actions = [action for action in action_list if action in device_priorities]\n",
    "\n",
    "        if valid_actions:\n",
    "            # Find the action with the highest priority\n",
    "            highest_priority_action = max(valid_actions, key=lambda action: device_priorities[action])\n",
    "            return highest_priority_action\n",
    "        else:\n",
    "            return '---'  # No valid actions found in the priorities\n",
    "    else:\n",
    "        return None  # Device not found in the command_priorities dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load acquisition 2\n",
    "\n",
    "df = pd.read_csv(r'../final_dataFrame_v2_anto.csv')\n",
    "\n",
    "ground_truth_df = pd.read_csv(r'../ground_truth_v2.csv')\n",
    "ground_truth_df = ground_truth_df.rename(columns={'0': 'Action'})\n",
    "if 'Unnamed: 0' in ground_truth_df.columns:\n",
    "    ground_truth_df = ground_truth_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)\n",
    "\n",
    "devices = ['0x0000', '0x265e', '0x3181', '0x0a79', '0xb815', '0x4e52', '0x9989', '0x5db6', '0x772c', '0x61af', '0xe011', '0xe694',\n",
    "            '0x6e5f', '0xc8f0', '0x09ac', '0x82eb', '0x054f', '0xebe5',  '0xe5c4', '0x482d', '0xa209', '0x2cae']\n",
    "\n",
    "devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', \n",
    "                'Aqara Button', 'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb',  'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "device_type_mapping = {\n",
    "    '0x0000' : 'Coordinator',\n",
    "    '0x09ac' : 'Socket',\n",
    "    '0x5db6' : 'Door',\n",
    "    '0xe694' : 'Socket',\n",
    "    '0x772c' : 'Door',\n",
    "    '0x61af' : 'Vibration',\n",
    "    '0xe011' : 'Button',\n",
    "    '0x9989' : 'Motion',\n",
    "    '0x265e' : 'Temperature',\n",
    "    '0xb815' : 'Motion',\n",
    "    '0x4e52' : 'Motion',\n",
    "    '0x3181' : 'Door',\n",
    "    '0x0a79' : 'Door',\n",
    "    '0x82eb' : 'Socket',\n",
    "    '0x6e5f' : 'Socket',\n",
    "    '0xc8f0' : 'Socket',\n",
    "    '0xebe5' : 'Bulb',\n",
    "    '0x054f' : 'Bulb',\n",
    "    '0x482d' : 'Bulb',\n",
    "    '0xa209' : 'Bulb',\n",
    "    '0x2cae' : 'Motion',\n",
    "    '0xe5c4' : 'Bulb'\n",
    "}\n",
    "\n",
    "device_name_mapping = {\n",
    "    '0x0000' : 'Coordinator',\n",
    "    '0x09ac' : 'Ledvance Z3 Plug',\n",
    "    '0x5db6' : 'Aqara Door',\n",
    "    '0xe694' : 'Smart Socket',\n",
    "    '0x772c' : 'Aqara Door',\n",
    "    '0x61af' : 'Aqara Vibration',\n",
    "    '0xe011' : 'Aqara Button',\n",
    "    '0x9989' : 'Aqara Motion',\n",
    "    '0x265e' : 'Sonoff Temperature',\n",
    "    '0xb815' : 'Sonoff Motion',\n",
    "    '0x4e52' : 'Sonoff Motion',\n",
    "    '0x3181' : 'Sonoff Door',\n",
    "    '0x0a79' : 'Sonoff Door',\n",
    "    '0x82eb' : 'Ledvance Smart+ Plug',\n",
    "    '0x6e5f' : 'Power Plug',\n",
    "    '0xc8f0' : 'Power Plug',\n",
    "    '0xebe5' : 'Moes Bulb',\n",
    "    '0x054f' : 'Ledvance Bulb',\n",
    "    '0x482d' : 'Philips Lamp',\n",
    "    '0xa209' : 'Philips Lamp',\n",
    "    '0x2cae' : 'Philips Lamp',\n",
    "    '0xe5c4' : 'Philips Motion'\n",
    "}\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=71482)\n",
    "threshold_float = 0.071482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Extended Source'])\n",
    "y = df['Extended Source']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.to_csv('train.csv', index=False)\n",
    "X_test.to_csv('test.csv', index=False)\n",
    "y_train.to_csv('train_labels.csv', index=False)\n",
    "y_test.to_csv('test_labels.csv', index=False)\n",
    "\n",
    "X_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "y_test = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m seq_no \u001b[38;5;241m=\u001b[39m seq_no1\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(np\u001b[38;5;241m.\u001b[39misnan(df\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPayload Length\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPayload Length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexing.py:2035\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2033\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[1;32m   2034\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[0;32m-> 2035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexing.py:2164\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39misetitem(loc, value)\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2162\u001b[0m     \u001b[38;5;66;03m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;66;03m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[0;32m-> 2164\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241m.\u001b[39miloc[loc]\n\u001b[1;32m   2165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mvoid:\n\u001b[1;32m   2166\u001b[0m         \u001b[38;5;66;03m# This means we're expanding, with multiple columns, e.g.\u001b[39;00m\n\u001b[1;32m   2167\u001b[0m         \u001b[38;5;66;03m#     df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]})\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[38;5;66;03m# Here, we replace those temporary `np.void` columns with\u001b[39;00m\n\u001b[1;32m   2171\u001b[0m         \u001b[38;5;66;03m# columns of the appropriate dtype, based on `value`.\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[:, loc] \u001b[38;5;241m=\u001b[39m construct_1d_array_from_inferred_fill_value(\n\u001b[1;32m   2173\u001b[0m             value, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   2174\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/generic.py:6461\u001b[0m, in \u001b[0;36mNDFrame.dtypes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6434\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6435\u001b[0m \u001b[38;5;124;03mReturn the dtypes in the DataFrame.\u001b[39;00m\n\u001b[1;32m   6436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6458\u001b[0m \u001b[38;5;124;03mdtype: object\u001b[39;00m\n\u001b[1;32m   6459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6460\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mget_dtypes()\n\u001b[0;32m-> 6461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_sliced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/series.py:588\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    586\u001b[0m manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 588\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mSingleBlockManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    590\u001b[0m     data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1870\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[0;34m(cls, array, index, refs)\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_array\u001b[39m(\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;28mcls\u001b[39m, array: ArrayLike, index: Index, refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SingleBlockManager:\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;124;03m    Constructor for if we have an array that is not yet a Block.\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1870\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_coerce_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m     bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(index)))\n\u001b[1;32m   1872\u001b[0m     block \u001b[38;5;241m=\u001b[39m new_block(array, placement\u001b[38;5;241m=\u001b[39mbp, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2662\u001b[0m, in \u001b[0;36mmaybe_coerce_values\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2659\u001b[0m \u001b[38;5;66;03m# Caller is responsible for ensuring NumpyExtensionArray is already extracted.\u001b[39;00m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 2662\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mensure_wrapped_if_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2664\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2665\u001b[0m         values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Add source zigbee and destination zigbee to ack packets\n",
    "seq_no = df.loc[0,'Sequence Number']\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    seq_no1 = df.loc[i,'Sequence Number']\n",
    "    if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "        df.loc[i,'Source Zigbee'] = df.loc[i-1,'Destination Zigbee']\n",
    "        df.loc[i,'Destination Zigbee'] = df.loc[i-1,'Source Zigbee']\n",
    "    seq_no = seq_no1\n",
    "    if(np.isnan(df.loc[i, 'Payload Length'])):\n",
    "        df.loc[i, 'Payload Length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_dataFrame_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframes 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load acquisition 4\n",
    "\n",
    "df = pd.read_csv(r'../final_dataFrame_v4_anto.csv')\n",
    "\n",
    "ground_truth_df = pd.read_csv(r'../ground_truth_v4.csv')\n",
    "ground_truth_df = ground_truth_df.rename(columns={'0': 'Action'})\n",
    "if 'Unnamed: 0' in ground_truth_df.columns:\n",
    "    ground_truth_df = ground_truth_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)\n",
    "\n",
    "devices = ['0x0000', '0x4615', '0x946e', '0x7b10', '0xd0bb', '0x1f29', '0x27d7', '0x907b', '0xe01d', '0x187a', '0xc31c',\n",
    "           '0xe1a6', '0x3d95', '0xa706', '0x4e11', '0x0112', '0xec7f', '0x1e15', '0x1cd8', '0x5bb9', '0x711c', '0x059b']\n",
    "\n",
    "devices_name = ['Coordinator', 'Sonoff Temperature', 'Sonoff Door', 'Sonoff Door', 'Sonoff Motion', 'Sonoff Motion', 'Aqara Motion', 'Aqara Door', 'Aqara Door', 'Aqara Vibration', 'Aqara Button',\n",
    "                'Smart Socket', 'Power Plug', 'Power Plug', 'Ledvance Z3 Plug', 'Ledvance Smart+ Plug', 'Ledvance Bulb', 'Moes Bulb', 'Philips Lamp', 'Philips Lamp', 'Philips Lamp', 'Philips Motion']\n",
    "\n",
    "device_type_mapping = {\n",
    "    '0x0000': 'Coordinator',\n",
    "    '0x4615': 'Temperature',\n",
    "    '0x946e': 'Door',\n",
    "    '0x7b10': 'Door',\n",
    "    '0xd0bb': 'Motion',\n",
    "    '0x1f29': 'Motion',\n",
    "    '0x27d7': 'Motion',\n",
    "    '0x907b': 'Door',\n",
    "    '0xe01d': 'Door',\n",
    "    '0x187a': 'Vibration',\n",
    "    '0xc31c': 'Button',\n",
    "    '0xe1a6': 'Socket',\n",
    "    '0x3d95': 'Socket',\n",
    "    '0xa706': 'Socket',\n",
    "    '0x4e11': 'Socket',\n",
    "    '0x0112': 'Socket',\n",
    "    '0xec7f': 'Bulb',\n",
    "    '0x1e15': 'Bulb',\n",
    "    '0x1cd8': 'Bulb',\n",
    "    '0x5bb9': 'Bulb',\n",
    "    '0x711c': 'Bulb',\n",
    "    '0x059b': 'Motion'    \n",
    "}\n",
    "\n",
    "device_name_mapping = {\n",
    "    '0x0000': 'Coordinator',\n",
    "    '0x4615': 'Sonoff Temperature',\n",
    "    '0x946e': 'Sonoff Door',\n",
    "    '0x7b10': 'Sonoff Door',\n",
    "    '0xd0bb': 'Sonoff Motion',\n",
    "    '0x1f29': 'Sonoff Motion',\n",
    "    '0x27d7': 'Aqara Motion',\n",
    "    '0x907b': 'Aqara Door',\n",
    "    '0xe01d': 'Aqara Door',\n",
    "    '0x187a': 'Aqara Vibration',\n",
    "    '0xc31c': 'Aqara Button',\n",
    "    '0xe1a6': 'Smart Socket',\n",
    "    '0x3d95': 'Power Plug',\n",
    "    '0xa706': 'Power Plug',\n",
    "    '0x4e11': 'Ledvance Z3 Plug',\n",
    "    '0x0112': 'Ledvance Smart+ Plug',\n",
    "    '0xec7f': 'Ledvance Bulb',\n",
    "    '0x1e15': 'Moes Bulb',\n",
    "    '0x1cd8': 'Philips Lamp',\n",
    "    '0x5bb9': 'Philips Lamp',\n",
    "    '0x711c': 'Philips Lamp',\n",
    "    '0x059b': 'Philips Motion' \n",
    "}\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=63446)\n",
    "threshold_float = 0.063446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['Extended Source'])\n",
    "y = df['Extended Source']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.to_csv('train.csv', index=False)\n",
    "X_test.to_csv('test.csv', index=False)\n",
    "y_train.to_csv('train_labels.csv', index=False)\n",
    "y_test.to_csv('test_labels.csv', index=False)\n",
    "\n",
    "X_train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "y_test = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add source zigbee and destination zigbee to ack packets\n",
    "seq_no = df.loc[0,'Sequence Number']\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    seq_no1 = df.loc[i,'Sequence Number']\n",
    "    if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "        df.loc[i,'Source Zigbee'] = df.loc[i-1,'Destination Zigbee']\n",
    "        df.loc[i,'Destination Zigbee'] = df.loc[i-1,'Source Zigbee']\n",
    "    seq_no = seq_no1\n",
    "    if(np.isnan(df.loc[i, 'Payload Length'])):\n",
    "        df.loc[i, 'Payload Length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_dataFrame_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAT Density Plot V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>Delta Time</th>\n",
       "      <th>Length</th>\n",
       "      <th>Source IEEE</th>\n",
       "      <th>Destination IEEE</th>\n",
       "      <th>FCF IEEE</th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Source Zigbee</th>\n",
       "      <th>Destination Zigbee</th>\n",
       "      <th>FCF Zigbee</th>\n",
       "      <th>Payload Length</th>\n",
       "      <th>Extended Source</th>\n",
       "      <th>File</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34068</td>\n",
       "      <td>Oct 12, 2023 17:16:36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77</td>\n",
       "      <td>0x482d</td>\n",
       "      <td>0xffff</td>\n",
       "      <td>0x8841</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0x482d</td>\n",
       "      <td>0xfffc</td>\n",
       "      <td>0x1209</td>\n",
       "      <td>32.0</td>\n",
       "      <td>00:17:88:01:06:e3:0a:f3</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Ack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34069</td>\n",
       "      <td>Oct 12, 2023 17:16:38.715568</td>\n",
       "      <td>2.715568</td>\n",
       "      <td>12</td>\n",
       "      <td>0x2cae</td>\n",
       "      <td>0x482d</td>\n",
       "      <td>0x8863</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Read Attributes (0x00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34070</td>\n",
       "      <td>Oct 12, 2023 17:16:38.716335</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0x0002</td>\n",
       "      <td>225.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Ack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34071</td>\n",
       "      <td>Oct 12, 2023 17:16:40.084308</td>\n",
       "      <td>1.367973</td>\n",
       "      <td>68</td>\n",
       "      <td>0xebe5</td>\n",
       "      <td>0xffff</td>\n",
       "      <td>0x8841</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0xebe5</td>\n",
       "      <td>0xfffc</td>\n",
       "      <td>0x1209</td>\n",
       "      <td>23.0</td>\n",
       "      <td>a4:c1:38:86:85:3b:80:7a</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Read Attributes Response (0x01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34072</td>\n",
       "      <td>Oct 12, 2023 17:16:41.091075</td>\n",
       "      <td>1.006767</td>\n",
       "      <td>77</td>\n",
       "      <td>0x82eb</td>\n",
       "      <td>0xffff</td>\n",
       "      <td>0x8841</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0x82eb</td>\n",
       "      <td>0xfffc</td>\n",
       "      <td>0x1209</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7c:b0:3e:aa:0a:08:56:75</td>\n",
       "      <td>idle2.pcapng</td>\n",
       "      <td>Ack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          Time  Delta Time  Length Source IEEE  \\\n",
       "0       34068  Oct 12, 2023 17:16:36.000000    0.000000      77      0x482d   \n",
       "1       34069  Oct 12, 2023 17:16:38.715568    2.715568      12      0x2cae   \n",
       "2       34070  Oct 12, 2023 17:16:38.716335    0.000767       5         NaN   \n",
       "3       34071  Oct 12, 2023 17:16:40.084308    1.367973      68      0xebe5   \n",
       "4       34072  Oct 12, 2023 17:16:41.091075    1.006767      77      0x82eb   \n",
       "\n",
       "  Destination IEEE FCF IEEE  Sequence Number Source Zigbee Destination Zigbee  \\\n",
       "0           0xffff   0x8841             79.0        0x482d             0xfffc   \n",
       "1           0x482d   0x8863            225.0           NaN                NaN   \n",
       "2              NaN   0x0002            225.0           NaN                NaN   \n",
       "3           0xffff   0x8841             79.0        0xebe5             0xfffc   \n",
       "4           0xffff   0x8841             37.0        0x82eb             0xfffc   \n",
       "\n",
       "  FCF Zigbee  Payload Length          Extended Source          File  \\\n",
       "0     0x1209            32.0  00:17:88:01:06:e3:0a:f3  idle2.pcapng   \n",
       "1        NaN             0.0                      NaN  idle2.pcapng   \n",
       "2        NaN             0.0                      NaN  idle2.pcapng   \n",
       "3     0x1209            23.0  a4:c1:38:86:85:3b:80:7a  idle2.pcapng   \n",
       "4     0x1209            32.0  7c:b0:3e:aa:0a:08:56:75  idle2.pcapng   \n",
       "\n",
       "                            Action  \n",
       "0                              Ack  \n",
       "1           Read Attributes (0x00)  \n",
       "2                              Ack  \n",
       "3  Read Attributes Response (0x01)  \n",
       "4                              Ack  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = True\n",
    "MICROS = 17.6*1000000 #100000\n",
    "df = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "SOURCE = 'Source IEEE' #Source IEEE Zigbee\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df = df[df['File'].str.contains('idle')]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Calculate the time delta between packets with the same sequence number (packet and corresponding ack)\n",
    "# df.sort_values(by=['Time'])\n",
    "# seq_no = df.loc[0,'Sequence Number']\n",
    "# date_string = df.loc[0,'Time']\n",
    "# date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "# date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "# list_of_timedelta = []\n",
    "\n",
    "# for i in range(1, len(df)):\n",
    "#     seq_no1 = df.loc[i,'Sequence Number']\n",
    "#     date_string = df.loc[i,'Time']\n",
    "#     date_object1 = datetime.strptime(date_string, date_format)\n",
    "#     if seq_no1 == seq_no and df.loc[i,'FCF IEEE'] == '0x0002':\n",
    "#         list_of_timedelta.append(date_object1 - date_object)\n",
    "#     date_object = date_object1\n",
    "#     seq_no = seq_no1\n",
    "\n",
    "# threshold = timedelta(days=0, hours=0, seconds=0, microseconds=20000)\n",
    "# second_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=0)\n",
    "# filtered_timedelta = [td for td in list_of_timedelta if td < threshold]\n",
    "# filtered_timedelta = [td for td in filtered_timedelta if td > second_threshold]\n",
    "# filtered_timedelta_micro = [td.microseconds for td in filtered_timedelta]\n",
    "\n",
    "# plt.hist(filtered_timedelta_micro, bins=100, edgecolor='k')\n",
    "# plt.xlabel('Time in Microseconds')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Timedelta Values for Ack Packets')\n",
    "# plt.grid(True)\n",
    "\n",
    "# hist, bin_edges = np.histogram(filtered_timedelta_micro, bins=100)\n",
    "# cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "# percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "# percentile_value = bin_edges[percentile_bin + 1]\n",
    "# print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "# number_of_ack = len(df[df['FCF IEEE'] == '0x0002'])\n",
    "# print(f\"The total number of Ack in the DataFrame is {number_of_ack}\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 95th percentile value is 76711.66313333334 Milliseconds\n",
      "Mean: 8262.730636601005, Std: 78476.8103939541\n"
     ]
    }
   ],
   "source": [
    "## Calculate the time delta between packets\n",
    "df.sort_values(by=['Time'])\n",
    "date_string = df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "\n",
    "for dev in df[SOURCE].unique():\n",
    "#    print(dev)\n",
    "    df_dev = df[df[SOURCE] == dev]\n",
    "    if len(df_dev) >0:\n",
    "        df_dev.reset_index(drop=True, inplace=True)\n",
    "        date_string = df_dev.loc[0]['Time']\n",
    "        date_object = datetime.strptime(date_string, date_format)\n",
    "        for i in range(1, len(df_dev)):\n",
    "            date_string = df_dev.iloc[i]['Time']\n",
    "            date_object1 = datetime.strptime(date_string, date_format)\n",
    "            list_of_timedelta.append(date_object1 - date_object)\n",
    "            date_object = date_object1\n",
    "\n",
    "threshold = timedelta(days=0, hours=10, seconds=40, microseconds=150000)\n",
    "second_threshold = timedelta(days=0)\n",
    "\n",
    "filtered_timedelta = [td for td in list_of_timedelta if td < threshold and td > second_threshold]\n",
    "\n",
    "filtered_timedelta_milli = [td.total_seconds()*1000 for td in filtered_timedelta]\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "plt.hist(filtered_timedelta_milli, bins=60, edgecolor='k')\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values')\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_milli, bins=60)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "percentile_bin = np.where(cdf > 0.97)[0][0]\n",
    "\n",
    "percentile_value = bin_edges[percentile_bin + 1]\n",
    "print(f\"The 95th percentile value is {percentile_value} Milliseconds\")\n",
    "\n",
    "import statistics\n",
    "mean = statistics.mean(filtered_timedelta_milli)\n",
    "std = statistics.stdev(filtered_timedelta_milli)\n",
    "\n",
    "print (f\"Mean: {mean}, Std: {std}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(3)\n",
    "\n",
    "## Density plot\n",
    "\n",
    "sns.kdeplot(filtered_timedelta_milli, bw_method=0.1)\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Timedelta Values')\n",
    "#save_tikzplotlib(plt.gcf(), \"threshold.tex\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 packets for device 0x265e\n",
      "The 95th percentile value is 10776.6 microseconds\n"
     ]
    }
   ],
   "source": [
    "## Calculate the time delta between packets ingoing or outgoing for a specific device\n",
    "\n",
    "device_address = '0x265e'\n",
    "\n",
    "device_df = df[(df[SOURCE] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "print(f\"{len(device_df)} packets for device {device_address}\")\n",
    "\n",
    "device_df.sort_values(by=['Time'])\n",
    "device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "date_string = device_df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(device_df)):\n",
    "    date_string = device_df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=150000)\n",
    "second_threshold = timedelta(days=0)\n",
    "filtered_timedelta_ledvance = [td for td in list_of_timedelta if td < threshold]\n",
    "filtered_timedelta_ledvance = [td for td in filtered_timedelta_ledvance if td > second_threshold]\n",
    "filtered_timedelta_micro_ledvance = [td.microseconds for td in filtered_timedelta_ledvance]\n",
    "\n",
    "plt.hist(filtered_timedelta_micro_ledvance, bins=60, edgecolor='k')\n",
    "plt.xlabel('Time in Microseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values')\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_micro_ledvance, bins=60)\n",
    "cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "percentile_value = bin_edges[percentile_bin + 1]\n",
    "print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x0000\n",
      "The dataframe is made up by 7547 packets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataframe is made up by 7532 packets\n",
      "17499.443 microseconds\n",
      "\n",
      "0x265e\n",
      "The dataframe is made up by 50 packets\n",
      "Filtered dataframe is made up by 26 packets\n",
      "554.311 microseconds\n",
      "\n",
      "0x3181\n",
      "The dataframe is made up by 3 packets\n",
      "Filtered dataframe is made up by 2 packets\n",
      "508.735 microseconds\n",
      "\n",
      "0x0a79\n",
      "The dataframe is made up by 7 packets\n",
      "Filtered dataframe is made up by 6 packets\n",
      "507.991 microseconds\n",
      "\n",
      "0xb815\n",
      "The dataframe is made up by 7 packets\n",
      "Filtered dataframe is made up by 5 packets\n",
      "507.07 microseconds\n",
      "\n",
      "0x4e52\n",
      "The dataframe is made up by 13 packets\n",
      "Filtered dataframe is made up by 7 packets\n",
      "511.137 microseconds\n",
      "\n",
      "0x9989\n",
      "The dataframe is made up by 23 packets\n",
      "Filtered dataframe is made up by 14 packets\n",
      "1008.847 microseconds\n",
      "\n",
      "0x5db6\n",
      "The dataframe is made up by 14 packets\n",
      "Filtered dataframe is made up by 9 packets\n",
      "5166.26 microseconds\n",
      "\n",
      "0x772c\n",
      "The dataframe is made up by 14 packets\n",
      "Filtered dataframe is made up by 9 packets\n",
      "8937.398 microseconds\n",
      "\n",
      "0x61af\n",
      "The dataframe is made up by 31 packets\n",
      "Filtered dataframe is made up by 12 packets\n",
      "1224.689 microseconds\n",
      "\n",
      "       Unnamed: 0                          Time  Delta Time  Length  \\\n",
      "7477        41545  Oct 12, 2023 17:48:43.629088    0.675263      80   \n",
      "7481        41549  Oct 12, 2023 17:48:44.138432    0.499502      12   \n",
      "19857       54178  Oct 12, 2023 18:38:53.448040    2.921897      80   \n",
      "19874       54195  Oct 12, 2023 18:38:53.957783    0.114662      12   \n",
      "19875       54196  Oct 12, 2023 18:38:53.960927    0.003144      12   \n",
      "37846       75228  Oct 12, 2023 16:58:37.186016    0.652240      80   \n",
      "37850       75232  Oct 12, 2023 16:58:37.695972    0.499890      12   \n",
      "\n",
      "      Source IEEE Destination IEEE FCF IEEE  Sequence Number Source Zigbee  \\\n",
      "7477       0xe011           0xe694   0x8861             92.0        0xe011   \n",
      "7481       0xe011           0xe694   0x8863             93.0           NaN   \n",
      "19857      0xe011           0xe694   0x8861             94.0        0xe011   \n",
      "19874      0xe011           0xe694   0x8863             95.0           NaN   \n",
      "19875      0xe011           0xe694   0x8863             95.0           NaN   \n",
      "37846      0xe011           0xe694   0x8861             90.0        0xe011   \n",
      "37850      0xe011           0xe694   0x8863             91.0           NaN   \n",
      "\n",
      "      Destination Zigbee FCF Zigbee  Payload Length          Extended Source  \\\n",
      "7477              0x0000     0x0248            43.0  00:15:8d:00:09:76:41:64   \n",
      "7481                 NaN        NaN             0.0                      NaN   \n",
      "19857             0x0000     0x0248            43.0  00:15:8d:00:09:76:41:64   \n",
      "19874                NaN        NaN             0.0                      NaN   \n",
      "19875                NaN        NaN             0.0                      NaN   \n",
      "37846             0x0000     0x0248            43.0  00:15:8d:00:09:76:41:64   \n",
      "37850                NaN        NaN             0.0                      NaN   \n",
      "\n",
      "               File    Action  \n",
      "7477   idle2.pcapng       Ack  \n",
      "7481   idle2.pcapng  APS: Ack  \n",
      "19857  idle3.pcapng       Ack  \n",
      "19874  idle3.pcapng  APS: Ack  \n",
      "19875  idle3.pcapng       Ack  \n",
      "37846  idle1.pcapng       Ack  \n",
      "37850  idle1.pcapng       Ack  \n",
      "0xe011\n",
      "The dataframe is made up by 7 packets\n",
      "Filtered dataframe is made up by 4 packets\n",
      "509.956 microseconds\n",
      "\n",
      "0xe694\n",
      "The dataframe is made up by 2423 packets\n",
      "Filtered dataframe is made up by 2414 packets\n",
      "17118.335 microseconds\n",
      "\n",
      "0x6e5f\n",
      "The dataframe is made up by 2098 packets\n",
      "Filtered dataframe is made up by 2059 packets\n",
      "17469.646 microseconds\n",
      "\n",
      "0xc8f0\n",
      "The dataframe is made up by 1987 packets\n",
      "Filtered dataframe is made up by 1897 packets\n",
      "17590.704 microseconds\n",
      "\n",
      "0x09ac\n",
      "The dataframe is made up by 714 packets\n",
      "Filtered dataframe is made up by 631 packets\n",
      "17593.891 microseconds\n",
      "\n",
      "0x82eb\n",
      "The dataframe is made up by 801 packets\n",
      "Filtered dataframe is made up by 775 packets\n",
      "17071.698 microseconds\n",
      "\n",
      "0x054f\n",
      "The dataframe is made up by 1568 packets\n",
      "Filtered dataframe is made up by 1510 packets\n",
      "17598.799 microseconds\n",
      "\n",
      "0xebe5\n",
      "The dataframe is made up by 873 packets\n",
      "Filtered dataframe is made up by 835 packets\n",
      "17133.71 microseconds\n",
      "\n",
      "0xe5c4\n",
      "The dataframe is made up by 1042 packets\n",
      "Filtered dataframe is made up by 942 packets\n",
      "17595.31 microseconds\n",
      "\n",
      "0x482d\n",
      "The dataframe is made up by 1944 packets\n",
      "Filtered dataframe is made up by 1833 packets\n",
      "17599.541 microseconds\n",
      "\n",
      "0xa209\n",
      "The dataframe is made up by 2474 packets\n",
      "Filtered dataframe is made up by 2372 packets\n",
      "17559.21 microseconds\n",
      "\n",
      "0x2cae\n",
      "The dataframe is made up by 2402 packets\n",
      "Filtered dataframe is made up by 2395 packets\n",
      "15206.676 microseconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Density plot based on inter arrival time\n",
    "\n",
    "colormap = plt.cm.get_cmap('tab20', len(devices))\n",
    "\n",
    "devices_hist_v2 = []\n",
    "for device_address in devices:\n",
    "    if device_address == '0xe011':\n",
    "        device_df = df[(df[SOURCE] == device_address)]\n",
    "        print(device_df)\n",
    "    \n",
    "\n",
    "    #device_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_df = df[(df[SOURCE] == device_address)]\n",
    "    device_df = device_df.sort_values(by=['Time'])\n",
    "    print(device_address)\n",
    "    print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "    device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    date_string = device_df.loc[0,'Time']\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(device_df)):\n",
    "        date_string = device_df.loc[i,'Time']\n",
    "        date_object1 = datetime.strptime(date_string, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=0, microseconds=MICROS) and td > timedelta(days=0)]\n",
    "    print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "    filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "    if len(filtered_timedelta_micro_device) > 0:\n",
    "        print(f\"{max(filtered_timedelta_micro_device)} microseconds\")\n",
    "\n",
    "\n",
    "    #plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "    sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address, color=colormap(devices.index(device_address)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time in Milliseconds')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f\"Histogram of Timedelta Values\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=300, range=(0,100))\n",
    "    devices_hist_v2.append(hist)\n",
    "    '''\n",
    "    cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "    percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "    percentile_value = bin_edges[percentile_bin + 1]\n",
    "    print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "    '''\n",
    "    print('')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"device_density.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Bulb\n",
      "Device address: 0x482d, Number of packets filtered: 1803, Number of packets: 1943, MEAN: 4180.055318162839\n",
      "Device address: 0xebe5, Number of packets filtered: 835, Number of packets: 872, MEAN: 4180.055318162839\n",
      "Device address: 0x054f, Number of packets filtered: 1462, Number of packets: 1567, MEAN: 4180.055318162839\n",
      "Device address: 0xa209, Number of packets filtered: 2353, Number of packets: 2473, MEAN: 4180.055318162839\n",
      "Device address: 0xe5c4, Number of packets filtered: 889, Number of packets: 1041, MEAN: 4180.055318162839\n",
      "Number of packets: 7342\n",
      "16.172709100000002\n",
      "Device type: Motion\n",
      "Device address: 0x2cae, Number of packets filtered: 2395, Number of packets: 2401, MEAN: 4180.055318162839\n",
      "Device address: 0xb815, Number of packets filtered: 5, Number of packets: 6, MEAN: 4180.055318162839\n",
      "Device address: 0x9989, Number of packets filtered: 14, Number of packets: 22, MEAN: 4180.055318162839\n",
      "Device address: 0x4e52, Number of packets filtered: 7, Number of packets: 12, MEAN: 4180.055318162839\n",
      "Number of packets: 2421\n",
      "5.064384\n",
      "Device type: nan\n",
      "Number of packets: 0\n",
      "nan\n",
      "Device type: Socket\n",
      "Device address: 0x82eb, Number of packets filtered: 775, Number of packets: 800, MEAN: 4180.055318162839\n",
      "Device address: 0x6e5f, Number of packets filtered: 2058, Number of packets: 2097, MEAN: 4180.055318162839\n",
      "Device address: 0xe694, Number of packets filtered: 2414, Number of packets: 2422, MEAN: 4180.055318162839\n",
      "Device address: 0xc8f0, Number of packets filtered: 1885, Number of packets: 1986, MEAN: 4180.055318162839\n",
      "Device address: 0x09ac, Number of packets filtered: 567, Number of packets: 713, MEAN: 4180.055318162839\n",
      "Number of packets: 7699\n",
      "15.074496400000001\n",
      "Device type: Coordinator\n",
      "Device address: 0x0000, Number of packets filtered: 7522, Number of packets: 7546, MEAN: 4180.055318162839\n",
      "Number of packets: 7522\n",
      "10.200804799999998\n",
      "Device type: Door\n",
      "Device address: 0x772c, Number of packets filtered: 9, Number of packets: 13, MEAN: 4180.055318162839\n",
      "Device address: 0x3181, Number of packets filtered: 2, Number of packets: 2, MEAN: 4180.055318162839\n",
      "Device address: 0x5db6, Number of packets filtered: 9, Number of packets: 13, MEAN: 4180.055318162839\n",
      "Device address: 0x0a79, Number of packets filtered: 6, Number of packets: 6, MEAN: 4180.055318162839\n",
      "Number of packets: 26\n",
      "4.05840875\n",
      "Device type: Vibration\n",
      "Device address: 0x61af, Number of packets filtered: 12, Number of packets: 30, MEAN: 4180.055318162839\n",
      "Number of packets: 12\n",
      "1.2242083\n",
      "Device type: Temperature\n",
      "Device address: 0x265e, Number of packets filtered: 26, Number of packets: 49, MEAN: 4180.055318162839\n",
      "Number of packets: 26\n",
      "0.5106805\n",
      "Device type: Button\n",
      "Device address: 0xe011, Number of packets filtered: 4, Number of packets: 6, MEAN: 4180.055318162839\n",
      "Number of packets: 4\n",
      "0.50992405\n",
      "Median 95 Q 4.561396375\n",
      "Mean 95 Q 6.601951987500001\n",
      "Qraw 95 Q 15.095806449999998\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df['Device Type'] = df[SOURCE].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab10', len(df['Device Type'].unique()))\n",
    "\n",
    "quant_list = list()\n",
    "quant_raw_list = list()\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    # if device_type == 'Coordinator' or device_type == 'Button' or device_type == 'Vibration': \n",
    "    #     continue\n",
    "\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_milli_device = []\n",
    "    \n",
    "    print(f\"Device type: {device_type}\")\n",
    "\n",
    "    for device_address in subset_df[SOURCE].unique():\n",
    "        device_df = subset_df[subset_df[SOURCE] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=17.160, microseconds=0) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_milli_device = [td.total_seconds()*1 for td in filtered_timedelta_device]\n",
    "        \n",
    "        print(f\"Device address: {device_address}, Number of packets filtered: {len(filtered_timedelta_milli_device)}, Number of packets: {len(list_of_timedelta)}, MEAN: {statistics.mean(filtered_timedelta_micro_device)}\")\n",
    "        \n",
    "\n",
    "        \n",
    "        all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "\n",
    "\n",
    "    # if len(all_timedelta_milli_device) < 10:\n",
    "    #     continue\n",
    "    \n",
    "    print(f\"Number of packets: {len(all_timedelta_milli_device)}\")\n",
    "    \n",
    "    quant_raw_list.extend(all_timedelta_milli_device)\n",
    "    all_timedelta_micro_device_pd = pd.Series(all_timedelta_milli_device)\n",
    "    quant = all_timedelta_micro_device_pd.quantile(0.95)\n",
    "    if quant is not None and not np.isnan(quant):\n",
    "        quant_list.append(quant)\n",
    "        \n",
    "    print(all_timedelta_micro_device_pd.quantile(0.95))\n",
    "    \n",
    "    \n",
    "    \n",
    "    sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "    #sns.histplot(all_timedelta_micro_device, bins='auto',stat='count', kde=True, color=color, label=device_type,alpha=0.5)\n",
    "    #sns.displot(all_timedelta_micro_device, kind=\"kde\",color=color, label=device_type)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Median 95 Q {statistics.median(quant_list)}\")\n",
    "print(f\"Mean 95 Q {statistics.mean(quant_list)}\")\n",
    "q_raw = pd.Series(quant_raw_list).quantile(0.95)\n",
    "print(f\"Qraw 95 Q {q_raw}\")\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Seconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "save_tikzplotlib(plt.gcf(), \"category_density_idle_17.16s_no_coordinator_ieee_and_zb.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAT Density Plot V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = True\n",
    "SOURCE = 'Source IEEE' #Source IEEE Zigbee\n",
    "MICROS = 17.6*1000000 #100000\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Calculate the time delta between packets\n",
    "\n",
    "# df = df4\n",
    "# load_mapping(4)\n",
    "\n",
    "# df.sort_values(by=['Time'])\n",
    "# date_string = df.loc[0,'Time']\n",
    "# date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "# date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "# list_of_timedelta = []\n",
    "\n",
    "# for i in range(1, len(df)):\n",
    "#     date_string = df.loc[i,'Time']\n",
    "#     date_object1 = datetime.strptime(date_string, date_format)\n",
    "#     if True:\n",
    "#         list_of_timedelta.append(date_object1 - date_object)\n",
    "#     date_object = date_object1\n",
    "\n",
    "# threshold = timedelta(days=0, hours=0, seconds=0, microseconds=200000)\n",
    "# second_threshold = timedelta(days=0)\n",
    "# filtered_timedelta_no_ack = [td for td in list_of_timedelta if td < threshold]\n",
    "# filtered_timedelta_no_ack = [td for td in filtered_timedelta_no_ack if td > second_threshold]\n",
    "# filtered_timedelta_micro_no_ack = [td.microseconds for td in filtered_timedelta_no_ack]\n",
    "\n",
    "# # plt.hist(filtered_timedelta_micro_no_ack, bins=60, edgecolor='k')\n",
    "# # plt.xlabel('Time in Microseconds')\n",
    "# # plt.ylabel('Frequency')\n",
    "# # plt.title('Histogram of Timedelta Values')\n",
    "# # plt.grid(True)\n",
    "\n",
    "# # hist, bin_edges = np.histogram(filtered_timedelta_micro_no_ack, bins=60)\n",
    "# # cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "# # percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "# # percentile_value = bin_edges[percentile_bin + 1]\n",
    "# # print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# ## Density plot\n",
    "\n",
    "# sns.kdeplot(filtered_timedelta_micro_no_ack, bw_method=0.1)\n",
    "# plt.xlabel('Time in Microseconds')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Density Plot of Timedelta Values')\n",
    "# #save_tikzplotlib(plt.gcf(), \"threshold.tex\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x0000\n",
      "The dataframe is made up by 9047 packets\n",
      "Filtered dataframe is made up by 9042 packets\n",
      "17364.527 microseconds\n",
      "\n",
      "0x4615\n",
      "The dataframe is made up by 48 packets\n",
      "Filtered dataframe is made up by 24 packets\n",
      "513.137 microseconds\n",
      "\n",
      "0x946e\n",
      "The dataframe is made up by 17 packets\n",
      "Filtered dataframe is made up by 10 packets\n",
      "508.51 microseconds\n",
      "\n",
      "0x7b10\n",
      "The dataframe is made up by 3 packets\n",
      "Filtered dataframe is made up by 2 packets\n",
      "507.605 microseconds\n",
      "\n",
      "0xd0bb\n",
      "The dataframe is made up by 8 packets\n",
      "Filtered dataframe is made up by 5 packets\n",
      "9.431 microseconds\n",
      "\n",
      "0x1f29\n",
      "The dataframe is made up by 0 packets\n",
      "0x27d7\n",
      "The dataframe is made up by 34 packets\n",
      "Filtered dataframe is made up by 10 packets\n",
      "1008.639 microseconds\n",
      "\n",
      "0x907b\n",
      "The dataframe is made up by 9 packets\n",
      "Filtered dataframe is made up by 5 packets\n",
      "512.19 microseconds\n",
      "\n",
      "0xe01d\n",
      "The dataframe is made up by 10 packets\n",
      "Filtered dataframe is made up by 6 packets\n",
      "14691.459 microseconds\n",
      "\n",
      "0x187a\n",
      "The dataframe is made up by 0 packets\n",
      "0xc31c\n",
      "The dataframe is made up by 6 packets\n",
      "Filtered dataframe is made up by 3 packets\n",
      "510.224 microseconds\n",
      "\n",
      "0xe1a6\n",
      "The dataframe is made up by 3617 packets\n",
      "Filtered dataframe is made up by 3615 packets\n",
      "15114.421 microseconds\n",
      "\n",
      "0x3d95\n",
      "The dataframe is made up by 3826 packets\n",
      "Filtered dataframe is made up by 3821 packets\n",
      "17000.798 microseconds\n",
      "\n",
      "0xa706\n",
      "The dataframe is made up by 4209 packets\n",
      "Filtered dataframe is made up by 4192 packets\n",
      "16303.028 microseconds\n",
      "\n",
      "0x4e11\n",
      "The dataframe is made up by 2333 packets\n",
      "Filtered dataframe is made up by 2318 packets\n",
      "17545.508 microseconds\n",
      "\n",
      "0x0112\n",
      "The dataframe is made up by 1659 packets\n",
      "Filtered dataframe is made up by 1591 packets\n",
      "16627.001 microseconds\n",
      "\n",
      "0xec7f\n",
      "The dataframe is made up by 5353 packets\n",
      "Filtered dataframe is made up by 5344 packets\n",
      "17544.289 microseconds\n",
      "\n",
      "0x1e15\n",
      "The dataframe is made up by 2277 packets\n",
      "Filtered dataframe is made up by 2269 packets\n",
      "17580.219 microseconds\n",
      "\n",
      "0x1cd8\n",
      "The dataframe is made up by 2681 packets\n",
      "Filtered dataframe is made up by 2650 packets\n",
      "17549.993 microseconds\n",
      "\n",
      "0x5bb9\n",
      "The dataframe is made up by 2097 packets\n",
      "Filtered dataframe is made up by 2046 packets\n",
      "17527.401 microseconds\n",
      "\n",
      "0x711c\n",
      "The dataframe is made up by 4641 packets\n",
      "Filtered dataframe is made up by 4623 packets\n",
      "17578.191 microseconds\n",
      "\n",
      "0x059b\n",
      "The dataframe is made up by 0 packets\n"
     ]
    }
   ],
   "source": [
    "## Density plot based on inter arrival time per device\n",
    "\n",
    "df = df4\n",
    "load_mapping(4)\n",
    "\n",
    "colormap = plt.cm.get_cmap('tab20', len(devices))\n",
    "\n",
    "devices_hist_v4 = []\n",
    "for device_address in devices:\n",
    "    try:\n",
    "        #device_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "        device_df = df[(df[SOURCE] == device_address)]\n",
    "        device_df = device_df.sort_values(by=['Time'])\n",
    "        print(device_address)\n",
    "        print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        date_string = device_df.loc[0,'Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        date_object = datetime.strptime(date_string, date_format)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(device_df)):\n",
    "        date_string = device_df.loc[i,'Time']\n",
    "        date_object1 = datetime.strptime(date_string, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=0, microseconds=MICROS) and td > timedelta(days=0)]\n",
    "    print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "    filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "    if len(filtered_timedelta_micro_device) > 0:\n",
    "        print(f\"{max(filtered_timedelta_micro_device)} microseconds\")\n",
    "\n",
    "\n",
    "    #plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "    sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time in Milliseconds')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f\"Histogram of Timedelta Values\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=300, range=(0,100))\n",
    "    devices_hist_v4.append(hist)\n",
    "    '''\n",
    "    cdf = np.cumsum(hist) / np.sum(hist)\n",
    "\n",
    "    percentile_bin = np.where(cdf > 0.95)[0][0]\n",
    "\n",
    "    percentile_value = bin_edges[percentile_bin + 1]\n",
    "    print(f\"The 95th percentile value is {percentile_value} microseconds\")\n",
    "    '''\n",
    "    print('')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"device_density.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df = df4\n",
    "load_mapping(4)\n",
    "\n",
    "df['Device Type'] = df[SOURCE].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_micro_device = []\n",
    "\n",
    "    for device_address in subset_df[SOURCE].unique():\n",
    "        device_df = subset_df[subset_df[SOURCE] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=20, microseconds=10000) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "\n",
    "        all_timedelta_micro_device.extend(filtered_timedelta_micro_device)\n",
    "\n",
    "    sns.kdeplot(all_timedelta_micro_device, bw_method=0.1, label=device_type, color=color)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "#save_tikzplotlib(plt.gcf(), \"category_density_updated.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CosSimilarity Distance Prob Dist Idel State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract 300 bins from the kde plot\n",
    "def extract_bins (df,device_mapping=device_type_mapping):\n",
    "\n",
    "    df['Device Type'] = df['Source IEEE'].map(device_mapping)\n",
    "    \n",
    "    areas_per_device = {}\n",
    "    kde_list = []\n",
    "    # Density plot based on inter-arrival time for each category\n",
    "    colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "    for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "        if device_type == 'Coordinator' or device_type == 'nan' or device_type is None or device_type is np.nan:\n",
    "            continue\n",
    "\n",
    "        subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "        all_timedelta_milli_device = []\n",
    "        \n",
    "        print(f\"Device type: {device_type}\")\n",
    "\n",
    "        for device_address in subset_df['Source IEEE'].unique():\n",
    "            device_df = subset_df[subset_df['Source IEEE'] == device_address]\n",
    "            device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            list_of_timedelta = []\n",
    "            for i in range(1, len(device_df)):\n",
    "                date_string = device_df.loc[i, 'Time']\n",
    "                date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "                list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "            filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=17.160, microseconds=0) and td > timedelta(days=0)]\n",
    "\n",
    "            filtered_timedelta_milli_device = [td.total_seconds()*1000 for td in filtered_timedelta_device]\n",
    "            \n",
    "            \n",
    "            all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "            \n",
    "        \n",
    "        if len(all_timedelta_milli_device) <= 5:\n",
    "            continue\n",
    "        \n",
    "        plt.figure()\n",
    "        kde_plot = sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "        kde_list.append({'device':device_type,'kde':kde_plot})\n",
    "    \n",
    "    glob_min = 0\n",
    "    glob_max = 0\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        # Extract KDE Data:\n",
    "        kde = elem['kde']\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = x_values.min(), x_values.max()\n",
    "        if glob_max < x_max:\n",
    "            glob_max = x_max\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        kde = elem['kde']\n",
    "        # Extract KDE Data:\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = glob_min,glob_max\n",
    "        bin_width = (x_max - x_min) / 300\n",
    "        bins = np.linspace(x_min, x_max, 301)  # 301 points for 300 intervals\n",
    "\n",
    "        # Area Calculation:\n",
    "        areas = []\n",
    "        for i in range(len(bins) - 1):\n",
    "            bin_start, bin_end = bins[i], bins[i + 1]\n",
    "            bin_midpoint = (bin_start + bin_end) / 2\n",
    "\n",
    "            # Use Kernel Density Estimation Function Directly (no need for interpolation):\n",
    "            density = kde_line.get_ydata()[np.argmin(np.abs(x_values - bin_midpoint))]\n",
    "            area = density * bin_width\n",
    "            areas.append(area)\n",
    "\n",
    "        areas_per_device[elem['device']] = areas\n",
    "        \n",
    "    # for device_type, areas in areas_per_device.items():\n",
    "        # plt.figure()\n",
    "        # plt.bar(bins[:-1], areas, width=bin_width, align='edge', edgecolor='k', alpha=0.5)\n",
    "        # plt.xlabel('Time Interval (ms)')\n",
    "        # plt.ylabel('Area')\n",
    "        # plt.title(f'Areas under KDE for {device_type}')\n",
    "        # plt.show()\n",
    "    \n",
    "    return areas_per_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Bulb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Motion\n",
      "Device type: Socket\n",
      "Device type: Door\n",
      "Device type: Vibration\n",
      "Device type: Temperature\n",
      "Device type: Button\n"
     ]
    }
   ],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hist2 = extract_bins(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Bulb\n",
      "Device type: Socket\n",
      "Device type: Temperature\n",
      "Device type: Motion\n",
      "Device type: Button\n",
      "Device type: Door\n"
     ]
    }
   ],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "hist4 = extract_bins(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Bulb', 'Door', 'Motion', 'Socket', 'Temperature', 'Vibration'])\n",
      "dict_keys(['Bulb', 'Door', 'Motion', 'Socket', 'Temperature'])\n"
     ]
    }
   ],
   "source": [
    "hist2 = dict(sorted(hist2.items()))\n",
    "hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "print(hist2.keys())\n",
    "print(hist4.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wasserstein_distance\n",
    "# from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "# hist2_clean = hist2.copy()\n",
    "\n",
    "# if \"Vibration\" in hist2_clean:\n",
    "#     del hist2_clean[\"Vibration\"]\n",
    "\n",
    "# # Ensure Dictionaries are Sorted for Consistent Order\n",
    "# sorted_hist2 = dict(sorted(hist2_clean.items()))\n",
    "# sorted_hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "# # Assuming hist2 and hist4 are dictionaries of histograms\n",
    "# distance_matrix_total = np.zeros((len(sorted_hist4), len(sorted_hist2)))  # Use float dtype for EMD\n",
    "\n",
    "# # Compute pairwise EMD between histograms of the same device type\n",
    "# for i, (dev_type1, hist1_val) in enumerate(sorted_hist2.items()):\n",
    "#     for j, (dev_type2, hist2_val) in enumerate(sorted_hist4.items()):\n",
    "#         distance_matrix_total[j, i] = wasserstein_distance(hist1_val, hist2_val)  # Correct indexing\n",
    "\n",
    "# print(distance_matrix_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wasserstein_distance\n",
    "\n",
    "# # Assuming hist2 and hist4 are dictionaries of histograms\n",
    "# distance_matrix_total = np.zeros((len(hist2), len(hist2)))  # Use float dtype for EMD\n",
    "\n",
    "# # Compute pairwise EMD between histograms of the same device type\n",
    "# for i, (dev_type1, hist1_val) in enumerate(hist2.items()):\n",
    "#     for j, (dev_type2, hist2_val) in enumerate(hist2.items()):\n",
    "#         distance_matrix_total[j, i] = wasserstein_distance(hist1_val, hist2_val)  # Correct indexing\n",
    "\n",
    "# print(distance_matrix_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65505659 0.62536624 0.53346158 0.60986253 0.24648811]\n",
      " [0.84266176 0.93243361 0.16117994 0.77094298 0.45978229]\n",
      " [0.35102739 0.24401501 0.08310376 0.34864799 0.11521651]\n",
      " [0.6194601  0.55361354 0.5343208  0.58570326 0.21699446]\n",
      " [0.22699794 0.27478028 0.04736678 0.21116576 0.49175979]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hist2_clean = hist2.copy()\n",
    "\n",
    "if \"Vibration\" in hist2_clean:\n",
    "    del hist2_clean[\"Vibration\"]\n",
    "\n",
    "# # Ensure Dictionaries are Sorted for Consistent Order\n",
    "# sorted_hist2 = dict(sorted(hist2_clean.items()))\n",
    "# sorted_hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist4), len(hist2_clean)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2_clean.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "        distance_matrix_total[j, i] =np.dot(hist1_val, hist2_val) / (np.linalg.norm(hist1_val) * np.linalg.norm(hist2_val))\n",
    "\n",
    "print(distance_matrix_total)\n",
    "\n",
    "sns.heatmap(distance_matrix_total, xticklabels=hist2_clean.keys(), yticklabels=hist4.keys(), linewidths=0.5, cmap=\"vlag\", square=True)\n",
    "plt.title('Cosine Similarity for Acquisition 2 vs 4')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2_vs_v4.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.81239481 0.20860854 0.97252136 0.35967035]\n",
      " [0.81239481 1.         0.2279186  0.72406403 0.46331132]\n",
      " [0.20860854 0.2279186  1.         0.17046839 0.07314906]\n",
      " [0.97252136 0.72406403 0.17046839 1.         0.32352925]\n",
      " [0.35967035 0.46331132 0.07314906 0.32352925 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist2_clean), len(hist2_clean)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2_clean.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist2_clean.items()):\n",
    "        distance_matrix_total[j, i] =np.dot(hist1_val, hist2_val) / (np.linalg.norm(hist1_val) * np.linalg.norm(hist2_val))\n",
    "\n",
    "print(distance_matrix_total)\n",
    "\n",
    "sns.heatmap(distance_matrix_total, xticklabels=hist2_clean.keys(), yticklabels=hist2_clean.keys(), linewidths=0.5, cmap=\"vlag\", square=True)\n",
    "plt.title('Cosine Similarity for Acquisition 2')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import wasserstein_distance\n",
    "# from scipy.stats import entropy\n",
    "\n",
    "# # Assuming hist2 and hist4 are dictionaries of histograms\n",
    "# distance_matrix_total = np.zeros((len(hist4), len(hist2)))  # Use float dtype for EMD\n",
    "\n",
    "# # Compute pairwise EMD between histograms of the same device type\n",
    "# for i, (dev_type1, hist1_val) in enumerate(hist2.items()):\n",
    "#     for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "#         kde_data1_normalized = hist1_val/np.sum(hist1_val)\n",
    "#         kde_data2_normalized = hist2_val/np.sum(hist2_val)\n",
    "        \n",
    "#         distance_matrix_total[j, i] = entropy(kde_data1_normalized, kde_data2_normalized)\n",
    "\n",
    "# print(distance_matrix_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity per Device full, idle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract 300 bins from the kde plot\n",
    "def extract_bins (df,device_mapping,seconds=17.6,microseconds=0):\n",
    "\n",
    "    df['Device Type'] = df['Source IEEE'].map(device_mapping)\n",
    "    \n",
    "    areas_per_device = {}\n",
    "    kde_list = []\n",
    "    # Density plot based on inter-arrival time for each category\n",
    "    colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "    for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "        if device_type == 'Coordinator' or device_type == 'nan' or device_type is None or device_type is np.nan:\n",
    "            continue\n",
    "\n",
    "        subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "        all_timedelta_milli_device = []\n",
    "        \n",
    "        print(f\"Device type: {device_type}\")\n",
    "\n",
    "        for device_address in subset_df['Source IEEE'].unique():\n",
    "            device_df = subset_df[subset_df['Source IEEE'] == device_address]\n",
    "            device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            list_of_timedelta = []\n",
    "            for i in range(1, len(device_df)):\n",
    "                date_string = device_df.loc[i, 'Time']\n",
    "                date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "                list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "            filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=seconds, microseconds=microseconds) and td > timedelta(days=0)]\n",
    "\n",
    "            filtered_timedelta_milli_device = [td.total_seconds()*1000 for td in filtered_timedelta_device]\n",
    "            \n",
    "            \n",
    "            all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "            \n",
    "        \n",
    "        if len(all_timedelta_milli_device) <= 5:\n",
    "            continue\n",
    "        \n",
    "        plt.figure()\n",
    "        kde_plot = sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "        kde_list.append({'device':device_type,'kde':kde_plot})\n",
    "    \n",
    "    glob_min = 0\n",
    "    glob_max = 0\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        # Extract KDE Data:\n",
    "        kde = elem['kde']\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = x_values.min(), x_values.max()\n",
    "        if glob_max < x_max:\n",
    "            glob_max = x_max\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        kde = elem['kde']\n",
    "        # Extract KDE Data:\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = glob_min,glob_max\n",
    "        bin_width = (x_max - x_min) / 300\n",
    "        bins = np.linspace(x_min, x_max, 301)  # 301 points for 300 intervals\n",
    "\n",
    "        # Area Calculation:\n",
    "        areas = []\n",
    "        for i in range(len(bins) - 1):\n",
    "            bin_start, bin_end = bins[i], bins[i + 1]\n",
    "            bin_midpoint = (bin_start + bin_end) / 2\n",
    "\n",
    "            # Use Kernel Density Estimation Function Directly (no need for interpolation):\n",
    "            density = kde_line.get_ydata()[np.argmin(np.abs(x_values - bin_midpoint))]\n",
    "            area = density * bin_width\n",
    "            areas.append(area)\n",
    "\n",
    "        areas_per_device[elem['device']] = areas\n",
    "        \n",
    "    # for device_type, areas in areas_per_device.items():\n",
    "        # plt.figure()\n",
    "        # plt.bar(bins[:-1], areas, width=bin_width, align='edge', edgecolor='k', alpha=0.5)\n",
    "        # plt.xlabel('Time Interval (ms)')\n",
    "        # plt.ylabel('Area')\n",
    "        # plt.title(f'Areas under KDE for {device_type}')\n",
    "        # plt.show()\n",
    "    \n",
    "    return areas_per_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Philips Lamp 1\n",
      "Device type: Philips Lamp 3\n",
      "Device type: Moes Bulb\n",
      "Device type: Ledvance Smart+ Plug\n",
      "Device type: Power Plug 1\n",
      "Device type: Ledvance Bulb\n",
      "Device type: Smart Socket\n",
      "Device type: Power Plug 2\n",
      "Device type: Philips Lamp 2\n",
      "Device type: Ledvance Z3 Plug\n",
      "Device type: Philips Motion\n",
      "Device type: Aqara Door 2\n",
      "Device type: Aqara Vibration\n",
      "Device type: Sonoff Temperature\n",
      "Device type: Sonoff Motion 1\n",
      "Device type: Aqara Button\n",
      "Device type: Sonoff Door 1\n",
      "Device type: Aqara Motion\n",
      "Device type: Aqara Door 1\n",
      "Device type: Sonoff Motion 2\n",
      "Device type: Sonoff Door 2\n"
     ]
    }
   ],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hist2 = extract_bins(df2,device_name_mapping,0,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Ledvance Bulb\n",
      "Device type: Philips Lamp 2\n",
      "Device type: Philips Lamp 1\n",
      "Device type: Philips Lamp 3\n",
      "Device type: Moes Bulb\n",
      "Device type: Ledvance Smart+ Plug\n",
      "Device type: Ledvance Z3 Plug\n",
      "Device type: Smart Socket\n",
      "Device type: Power Plug 2\n",
      "Device type: Power Plug 1\n",
      "Device type: Sonoff Temperature\n",
      "Device type: Aqara Motion\n",
      "Device type: Aqara Button\n",
      "Device type: Aqara Door 2\n",
      "Device type: Aqara Door 1\n",
      "Device type: Sonoff Door 1\n",
      "Device type: Sonoff Motion 1\n",
      "Device type: Sonoff Door 2\n"
     ]
    }
   ],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = True\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "hist4 = extract_bins(df4,device_name_mapping,0,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Ledvance Bulb', 'Ledvance Smart+ Plug', 'Ledvance Z3 Plug', 'Moes Bulb', 'Philips Lamp 1', 'Philips Lamp 2', 'Philips Lamp 3', 'Philips Motion', 'Power Plug 1', 'Power Plug 2', 'Smart Socket'])\n",
      "dict_keys(['Ledvance Bulb', 'Ledvance Smart+ Plug', 'Ledvance Z3 Plug', 'Moes Bulb', 'Philips Lamp 1', 'Philips Lamp 2', 'Philips Lamp 3', 'Power Plug 1', 'Power Plug 2', 'Smart Socket'])\n"
     ]
    }
   ],
   "source": [
    "hist2 = dict(sorted(hist2.items()))\n",
    "hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "print(hist2.keys())\n",
    "print(hist4.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89169952 0.86436426 0.38867293 0.47497199 0.96115805 0.93101077\n",
      "  0.49795036 0.76852343 0.8568067  0.81959552]\n",
      " [0.66232577 0.74157904 0.1983085  0.49817088 0.76131108 0.78141174\n",
      "  0.58107091 0.69779295 0.69642428 0.64585685]\n",
      " [0.56064965 0.45528585 0.22720197 0.20160503 0.64365281 0.53276749\n",
      "  0.07686647 0.28957048 0.31658156 0.39213713]\n",
      " [0.88234088 0.65825301 0.68823758 0.62788192 0.85107095 0.73063424\n",
      "  0.35581364 0.83821664 0.72434556 0.87228388]\n",
      " [0.57819411 0.40474892 0.2540393  0.2617371  0.66641844 0.57490836\n",
      "  0.09806673 0.31202133 0.35670931 0.40067192]\n",
      " [0.61515999 0.41639895 0.29306705 0.33040165 0.69722479 0.6077652\n",
      "  0.12787688 0.36528405 0.39399565 0.44918918]\n",
      " [0.81323644 0.8344256  0.24633536 0.63798087 0.88581799 0.95669408\n",
      "  0.70879674 0.82168645 0.96364554 0.7123118 ]\n",
      " [0.8993856  0.75124802 0.58345553 0.78681611 0.8356435  0.79342909\n",
      "  0.65322869 0.97910421 0.87591384 0.87544408]\n",
      " [0.67177879 0.61342732 0.29499029 0.84427056 0.68479803 0.75317263\n",
      "  0.66198949 0.84567373 0.89788396 0.65442586]\n",
      " [0.84158665 0.66069452 0.68960363 0.45360384 0.7638638  0.55780837\n",
      "  0.21424619 0.76472486 0.53914154 0.90365749]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hist2_clean = hist2.copy()\n",
    "\n",
    "# Filter hist2 to contain only keys that are also in hist4\n",
    "hist2_clean = {key: hist2[key] for key in hist4.keys() if key in hist2}\n",
    "\n",
    "# # Ensure Dictionaries are Sorted for Consistent Order\n",
    "# sorted_hist2 = dict(sorted(hist2_clean.items()))\n",
    "# sorted_hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist4), len(hist2_clean)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2_clean.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "        distance_matrix_total[j, i] =np.dot(hist1_val, hist2_val) / (np.linalg.norm(hist1_val) * np.linalg.norm(hist2_val))\n",
    "\n",
    "print(distance_matrix_total)\n",
    "\n",
    "\n",
    "sns.heatmap(distance_matrix_total, xticklabels=hist2_clean.keys(), yticklabels=hist4.keys(), linewidths=0.5, cmap=\"vlag\", square=True)\n",
    "plt.title('Cosine Similarity for Acquisition 2 vs 4')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2_vs_v4_xdev.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cossim per devname, no idle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Sonoff Motion 2\n",
      "Device type: Philips Motion\n",
      "Device type: Ledvance Smart+ Plug\n",
      "Device type: Sonoff Temperature\n",
      "Device type: Moes Bulb\n",
      "Device type: Aqara Button\n",
      "Device type: Aqara Vibration\n",
      "Device type: Aqara Door 2\n",
      "Device type: Smart Socket\n",
      "Device type: Ledvance Z3 Plug\n",
      "Device type: Power Plug 1\n",
      "Device type: Ledvance Bulb\n",
      "Device type: Philips Lamp 1\n",
      "Device type: Power Plug 2\n",
      "Device type: Sonoff Door 1\n",
      "Device type: Aqara Motion\n",
      "Device type: Aqara Door 1\n",
      "Device type: Sonoff Motion 1\n",
      "Device type: Sonoff Door 2\n",
      "Device type: Philips Lamp 3\n",
      "Device type: Philips Lamp 2\n",
      "Device type: Philips Lamp 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50835/2533953701.py:41: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device type: Aqara Button\n",
      "Device type: Aqara Motion\n",
      "Device type: Sonoff Temperature\n",
      "Device type: Smart Socket\n",
      "Device type: Ledvance Bulb\n",
      "Device type: Power Plug 1\n",
      "Device type: Philips Lamp 3\n",
      "Device type: Ledvance Z3 Plug\n",
      "Device type: Moes Bulb\n",
      "Device type: Ledvance Smart+ Plug\n",
      "Device type: Sonoff Motion 2\n",
      "Device type: Sonoff Motion 1\n",
      "Device type: Aqara Vibration\n",
      "Device type: Sonoff Door 2\n",
      "Device type: Aqara Door 1\n",
      "Device type: Power Plug 2\n",
      "Device type: Aqara Door 2\n",
      "Device type: Philips Motion\n",
      "Device type: Sonoff Door 1\n",
      "Device type: Philips Lamp 2\n",
      "dict_keys(['Aqara Button', 'Aqara Door 1', 'Aqara Door 2', 'Aqara Motion', 'Aqara Vibration', 'Ledvance Bulb', 'Ledvance Smart+ Plug', 'Ledvance Z3 Plug', 'Moes Bulb', 'Philips Lamp 1', 'Philips Lamp 2', 'Philips Lamp 3', 'Philips Motion', 'Power Plug 1', 'Power Plug 2', 'Smart Socket', 'Sonoff Door 2', 'Sonoff Motion 1', 'Sonoff Motion 2', 'Sonoff Temperature'])\n",
      "dict_keys(['Aqara Button', 'Aqara Door 2', 'Aqara Vibration', 'Ledvance Bulb', 'Ledvance Smart+ Plug', 'Ledvance Z3 Plug', 'Moes Bulb', 'Philips Lamp 1', 'Philips Lamp 2', 'Philips Lamp 3', 'Philips Motion', 'Power Plug 1', 'Power Plug 2', 'Smart Socket', 'Sonoff Door 1', 'Sonoff Door 2', 'Sonoff Motion 1', 'Sonoff Temperature'])\n",
      "[[0.09198818 0.52050842 0.46100375 0.27980388 0.26251743 0.37861457\n",
      "  0.43823959 0.40620852 0.36630889 0.19211787 0.43422471 0.41603139\n",
      "  0.44173872 0.37438423 0.53051305 0.39760859 0.50312386]\n",
      " [0.7021064  0.82549468 0.75881015 0.17198456 0.07768853 0.29437362\n",
      "  0.48375293 0.33797949 0.27507196 0.13741339 0.43342489 0.36058015\n",
      "  0.42541655 0.26248384 0.73423991 0.72216216 0.76028233]\n",
      " [0.45264541 0.79026898 0.97684412 0.28656891 0.1389582  0.49557047\n",
      "  0.79177313 0.55860471 0.46082849 0.28060986 0.70951582 0.61363612\n",
      "  0.71502024 0.44979021 0.94994666 0.89913057 0.93738045]\n",
      " [0.21405799 0.37605875 0.49734512 0.92980085 0.84034525 0.92371376\n",
      "  0.84514868 0.98077511 0.95548766 0.59244748 0.93437719 0.89297319\n",
      "  0.88813366 0.89316134 0.53151992 0.5879177  0.58290326]\n",
      " [0.05588272 0.10778639 0.1423847  0.8252978  0.97284391 0.76679898\n",
      "  0.53549829 0.85198242 0.87503273 0.45659265 0.67026888 0.73025872\n",
      "  0.67824905 0.83397919 0.1948954  0.2603088  0.23579063]\n",
      " [0.25737629 0.45316733 0.60374146 0.81441005 0.6235545  0.90619506\n",
      "  0.85400279 0.85613488 0.81025349 0.43151613 0.92591327 0.7649944\n",
      "  0.76249303 0.72386222 0.6057495  0.61510772 0.6368542 ]\n",
      " [0.1498615  0.26750102 0.34492116 0.64321175 0.62537562 0.8310751\n",
      "  0.7252209  0.68890407 0.63932896 0.30025288 0.62382298 0.77522419\n",
      "  0.62824533 0.82799778 0.36356457 0.36485996 0.38278679]\n",
      " [0.15467552 0.27890465 0.35937272 0.91339926 0.7681679  0.85200006\n",
      "  0.64992331 0.86601113 0.8794606  0.39889977 0.86010985 0.6550597\n",
      "  0.67687843 0.70716256 0.38905621 0.40933048 0.41373347]\n",
      " [0.20879471 0.37288849 0.49216544 0.89622959 0.72895126 0.92400797\n",
      "  0.79568557 0.91506041 0.91222677 0.47106259 0.94434686 0.76151893\n",
      "  0.77105172 0.76914664 0.51386228 0.53516738 0.54807223]\n",
      " [0.20586206 0.36330742 0.47487412 0.87676624 0.80467209 0.81790476\n",
      "  0.77448729 0.9643331  0.97432132 0.71092807 0.9190679  0.86086243\n",
      "  0.92089878 0.82546971 0.53784883 0.57046505 0.58471848]\n",
      " [0.06761119 0.13495332 0.19117677 0.51685198 0.33740796 0.43939223\n",
      "  0.50514625 0.55517674 0.57308019 0.97969907 0.5539271  0.64959165\n",
      "  0.67518683 0.49391854 0.28400201 0.30557993 0.3799265 ]\n",
      " [0.23831417 0.42733708 0.56561494 0.73630726 0.63010099 0.89075446\n",
      "  0.92103194 0.8544699  0.77245942 0.61402918 0.83308822 0.97354172\n",
      "  0.86965188 0.91898408 0.59742015 0.62236793 0.64448362]\n",
      " [0.32042125 0.57617534 0.73390503 0.67771135 0.55653316 0.75434581\n",
      "  0.91354078 0.8863071  0.83801289 0.7373008  0.91966459 0.91871056\n",
      "  0.98978271 0.78221522 0.78568128 0.78108584 0.82293548]\n",
      " [0.13610887 0.24386459 0.32167224 0.8003366  0.86003855 0.89373677\n",
      "  0.74040058 0.86225725 0.8238993  0.43094997 0.73464317 0.86938994\n",
      "  0.72595479 0.97623686 0.35722162 0.39339972 0.39497452]\n",
      " [0.53727374 0.88848108 0.87461947 0.27491736 0.16078032 0.41685642\n",
      "  0.62390494 0.48725243 0.41420301 0.2627281  0.59819155 0.53179227\n",
      "  0.63156126 0.39802323 0.90684812 0.71326673 0.88450977]\n",
      " [0.44125535 0.7804252  0.89938428 0.33718908 0.20076785 0.49377901\n",
      "  0.77583704 0.57856833 0.49426358 0.29873381 0.69382431 0.61578272\n",
      "  0.70859172 0.46627514 0.88473596 0.87201418 0.92158687]\n",
      " [0.55871441 0.86360392 0.93831767 0.32038822 0.17794724 0.45436497\n",
      "  0.70775469 0.53757851 0.46219254 0.3281706  0.64900561 0.57868082\n",
      "  0.66861251 0.43399852 0.92629229 0.93516341 0.92425715]\n",
      " [0.58802295 0.87614427 0.9187314  0.36690918 0.19350401 0.47424043\n",
      "  0.72225797 0.56015882 0.49030043 0.41112177 0.66745733 0.6157807\n",
      "  0.70115305 0.45695152 0.91912799 0.92570869 0.94943445]]\n"
     ]
    }
   ],
   "source": [
    "#Extract 300 bins from the kde plot\n",
    "def extract_bins (df,device_mapping,seconds=17.6,microseconds=0):\n",
    "\n",
    "    df['Device Type'] = df['Source IEEE'].map(device_mapping)\n",
    "    \n",
    "    areas_per_device = {}\n",
    "    kde_list = []\n",
    "    # Density plot based on inter-arrival time for each category\n",
    "    colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "    for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "        if device_type == 'Coordinator' or device_type == 'nan' or device_type is None or device_type is np.nan:\n",
    "            continue\n",
    "\n",
    "        subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "        all_timedelta_milli_device = []\n",
    "        \n",
    "        print(f\"Device type: {device_type}\")\n",
    "\n",
    "        for device_address in subset_df['Source IEEE'].unique():\n",
    "            device_df = subset_df[subset_df['Source IEEE'] == device_address]\n",
    "            device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            list_of_timedelta = []\n",
    "            for i in range(1, len(device_df)):\n",
    "                date_string = device_df.loc[i, 'Time']\n",
    "                date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "                list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "            filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=seconds, microseconds=microseconds) and td > timedelta(days=0)]\n",
    "\n",
    "            filtered_timedelta_milli_device = [td.total_seconds()*1000 for td in filtered_timedelta_device]\n",
    "            \n",
    "            \n",
    "            all_timedelta_milli_device.extend(filtered_timedelta_milli_device)\n",
    "            \n",
    "        \n",
    "        if len(all_timedelta_milli_device) <= 5:\n",
    "            continue\n",
    "        \n",
    "        plt.figure()\n",
    "        kde_plot = sns.kdeplot(all_timedelta_milli_device, bw_method=0.1, label=device_type, color=color)\n",
    "        kde_list.append({'device':device_type,'kde':kde_plot})\n",
    "    \n",
    "    glob_min = 0\n",
    "    glob_max = 0\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        # Extract KDE Data:\n",
    "        kde = elem['kde']\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = x_values.min(), x_values.max()\n",
    "        if glob_max < x_max:\n",
    "            glob_max = x_max\n",
    "    \n",
    "    for elem in kde_list:\n",
    "        kde = elem['kde']\n",
    "        # Extract KDE Data:\n",
    "        kde_line = kde.lines[0]\n",
    "        x_values, y_values = kde_line.get_data()\n",
    "\n",
    "        # Define Bins:\n",
    "        x_min, x_max = glob_min,glob_max\n",
    "        bin_width = (x_max - x_min) / 300\n",
    "        bins = np.linspace(x_min, x_max, 301)  # 301 points for 300 intervals\n",
    "\n",
    "        # Area Calculation:\n",
    "        areas = []\n",
    "        for i in range(len(bins) - 1):\n",
    "            bin_start, bin_end = bins[i], bins[i + 1]\n",
    "            bin_midpoint = (bin_start + bin_end) / 2\n",
    "\n",
    "            # Use Kernel Density Estimation Function Directly (no need for interpolation):\n",
    "            density = kde_line.get_ydata()[np.argmin(np.abs(x_values - bin_midpoint))]\n",
    "            area = density * bin_width\n",
    "            areas.append(area)\n",
    "\n",
    "        areas_per_device[elem['device']] = areas\n",
    "        \n",
    "    # for device_type, areas in areas_per_device.items():\n",
    "        # plt.figure()\n",
    "        # plt.bar(bins[:-1], areas, width=bin_width, align='edge', edgecolor='k', alpha=0.5)\n",
    "        # plt.xlabel('Time Interval (ms)')\n",
    "        # plt.ylabel('Area')\n",
    "        # plt.title(f'Areas under KDE for {device_type}')\n",
    "        # plt.show()\n",
    "    \n",
    "    return areas_per_device\n",
    "\n",
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "hist2 = extract_bins(df2,device_name_mapping,0,100000)\n",
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "hist4 = extract_bins(df4,device_name_mapping,0,100000)\n",
    "hist2 = dict(sorted(hist2.items()))\n",
    "hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "print(hist2.keys())\n",
    "print(hist4.keys())\n",
    "\n",
    "# hist2_clean = hist2.copy()\n",
    "\n",
    "# Filter hist2 to contain only keys that are also in hist4\n",
    "hist2_clean = {key: hist2[key] for key in hist4.keys() if key in hist2}\n",
    "\n",
    "# # Ensure Dictionaries are Sorted for Consistent Order\n",
    "# sorted_hist2 = dict(sorted(hist2_clean.items()))\n",
    "# sorted_hist4 = dict(sorted(hist4.items()))\n",
    "\n",
    "# Assuming hist2 and hist4 are dictionaries of histograms\n",
    "distance_matrix_total = np.zeros((len(hist4), len(hist2_clean)))  # Use float dtype for EMD\n",
    "\n",
    "# Compute pairwise EMD between histograms of the same device type\n",
    "for i, (dev_type1, hist1_val) in enumerate(hist2_clean.items()):\n",
    "    for j, (dev_type2, hist2_val) in enumerate(hist4.items()):\n",
    "        distance_matrix_total[j, i] =np.dot(hist1_val, hist2_val) / (np.linalg.norm(hist1_val) * np.linalg.norm(hist2_val))\n",
    "\n",
    "print(distance_matrix_total)\n",
    "\n",
    "\n",
    "sns.heatmap(distance_matrix_total, xticklabels=hist2_clean.keys(), yticklabels=hist4.keys(), linewidths=0.5, cmap=\"vlag\", square=True)\n",
    "plt.title('Cosine Similarity for Acquisition 2 vs 4')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2_vs_v4_xdev.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity per Device full, no idle _old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance_matrix_v2 = [[0 for x in range(len(devices_hist_v2))] for y in range(len(devices_hist_v2))]\n",
    "\n",
    "\n",
    "for i in range(len(devices_hist_v2)):\n",
    "    for j in range(len(devices_hist_v2)):\n",
    "        \n",
    "        distance_matrix_v2[i][j] = np.dot(devices_hist_v2[i], devices_hist_v2[j]) / (np.linalg.norm(devices_hist_v2[i]) * np.linalg.norm(devices_hist_v2[j]))\n",
    "\n",
    "\n",
    "sns.heatmap(distance_matrix_v2, xticklabels=devices_name_v2, yticklabels=devices_name_v2, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 2')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12237/2696100979.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  distance_matrix_v4[i][j] = np.dot(devices_hist_v4[i], devices_hist_v4[j]) / (np.linalg.norm(devices_hist_v4[i]) * np.linalg.norm(devices_hist_v4[j]))\n"
     ]
    }
   ],
   "source": [
    "distance_matrix_v4 = [[0 for x in range(len(devices_hist_v4))] for y in range(len(devices_hist_v4))]\n",
    "\n",
    "for i in range(len(devices_hist_v4)):\n",
    "    for j in range(len(devices_hist_v4)):\n",
    "        distance_matrix_v4[i][j] = np.dot(devices_hist_v4[i], devices_hist_v4[j]) / (np.linalg.norm(devices_hist_v4[i]) * np.linalg.norm(devices_hist_v4[j]))\n",
    "\n",
    "sns.heatmap(distance_matrix_v4, xticklabels=devices_name_v4, yticklabels=devices_name_v4, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 4')\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12237/3601635131.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  distance_matrix_tot[i][j] = np.dot(devices_hist_v2[i], devices_hist_v4[j]) / (np.linalg.norm(devices_hist_v2[i]) * np.linalg.norm(devices_hist_v4[j]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(devices_hist_v2)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(devices_hist_v4)):\n\u001b[0;32m----> 5\u001b[0m         \u001b[43mdistance_matrix_tot\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(devices_hist_v2[i], devices_hist_v4[j]) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(devices_hist_v2[i]) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(devices_hist_v4[j]))\n\u001b[1;32m      7\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(distance_matrix_tot, xticklabels\u001b[38;5;241m=\u001b[39mdevices_name_v2, yticklabels\u001b[38;5;241m=\u001b[39mdevices_name_v4, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrest\u001b[39m\u001b[38;5;124m\"\u001b[39m, square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#sns.heatmap(distance_matrix_total[0:22, 22:45], xticklabels=devices_name_v4, yticklabels=devices_name_v2, linewidths=0.5, cmap=\"crest\", square=True)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "distance_matrix_tot = [[0 for x in range(len(devices_hist_v2))] for y in range(len(devices_hist_v4))]\n",
    "\n",
    "for i in range(len(devices_hist_v2)):\n",
    "    for j in range(len(devices_hist_v4)):\n",
    "        distance_matrix_tot[i][j] = np.dot(devices_hist_v2[i], devices_hist_v4[j]) / (np.linalg.norm(devices_hist_v2[i]) * np.linalg.norm(devices_hist_v4[j]))\n",
    "\n",
    "sns.heatmap(distance_matrix_tot, xticklabels=devices_name_v2, yticklabels=devices_name_v4, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "\n",
    "#sns.heatmap(distance_matrix_total[0:22, 22:45], xticklabels=devices_name_v4, yticklabels=devices_name_v2, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix in between Acquisition 2 and 4')\n",
    "save_tikzplotlib(plt.gcf(), \"cos_sim_heat_v2_vs_v4_per_device.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEVENST Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_distance_matrix_v2 = [[0 for x in range(len(devices_hist_v2))] for y in range(len(devices_hist_v2))]\n",
    "\n",
    "for i in range(len(devices_hist_v2)):\n",
    "    for j in range(len(devices_hist_v2)):\n",
    "        lev_distance_matrix_v2[i][j] = lev.distance(str(devices_hist_v2[i]), str(devices_hist_v2[j]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_distance_matrix_v4 = [[0 for x in range(len(devices_hist_v4))] for y in range(len(devices_hist_v4))]\n",
    "\n",
    "for i in range(len(devices_hist_v4)):\n",
    "    for j in range(len(devices_hist_v4)):\n",
    "        lev_distance_matrix_v4[i][j] = lev.distance(str(devices_hist_v4[i]), str(devices_hist_v4[j]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 293 377 ... 220 222 235]\n",
      " [293   0 118 ... 180 178 166]\n",
      " [377 118   0 ... 267 271 256]\n",
      " ...\n",
      " [220 180 267 ...   0 115 119]\n",
      " [222 178 271 ... 115   0  97]\n",
      " [235 166 256 ... 119  97   0]]\n"
     ]
    }
   ],
   "source": [
    "total_devices_hist = devices_hist_v2 + devices_hist_v4\n",
    "\n",
    "lev_distance_matrix_total = np.zeros((len(total_devices_hist), len(total_devices_hist)), dtype=int)\n",
    "\n",
    "for i in range(len(total_devices_hist)):\n",
    "    for j in range(len(total_devices_hist)):\n",
    "        lev_distance_matrix_total[i][j] = lev.distance(str(total_devices_hist[i]), str(total_devices_hist[j]))\n",
    "\n",
    "print(lev_distance_matrix_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lev_distance_matrix_total[0:22, 22:45], xticklabels=devices_name_v4, yticklabels=devices_name_v2, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix in between Acquisition 2 and 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lev_distance_matrix_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(\u001b[43mlev_distance_matrix_v2\u001b[49m, xticklabels\u001b[38;5;241m=\u001b[39mdevices_name, yticklabels\u001b[38;5;241m=\u001b[39mdevices_name, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrest\u001b[39m\u001b[38;5;124m\"\u001b[39m, square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLevenshtein Distance Matrix for Acquisition 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lev_distance_matrix_v2' is not defined"
     ]
    }
   ],
   "source": [
    "sns.heatmap(lev_distance_matrix_v2, xticklabels=devices_name, yticklabels=devices_name, linewidths=0.5, cmap=\"crest\", square=True)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(lev_distance_matrix_v4, xticklabels=devices_name, yticklabels=devices_name)\n",
    "plt.title('Levenshtein Distance Matrix for Acquisition 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "idle_df = df[df['File'].str.contains('idle')]\n",
    "\n",
    "idle_df.head()\n",
    "\n",
    "sns.kdeplot(df['Length'], bw_method=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plot based on inter arrival time grouping by type of devices\n",
    "\n",
    "df['Device Type'] = df['Source Zigbee'].map(device_type_mapping)\n",
    "\n",
    "# Density plot based on inter-arrival time for each category\n",
    "colormap = plt.cm.get_cmap('tab20', len(df['Device Type'].unique()))\n",
    "\n",
    "for device_type, color in zip(df['Device Type'].unique(), colormap.colors):\n",
    "    subset_df = df[df['Device Type'] == device_type]\n",
    "\n",
    "    all_timedelta_micro_device = []\n",
    "\n",
    "    for device_address in subset_df['Source Zigbee'].unique():\n",
    "        device_df = subset_df[subset_df['Source Zigbee'] == device_address]\n",
    "        device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        list_of_timedelta = []\n",
    "        for i in range(1, len(device_df)):\n",
    "            date_string = device_df.loc[i, 'Time']\n",
    "            date_object1 = datetime.strptime(date_string, \"%b %d, %Y %H:%M:%S.%f\")\n",
    "            list_of_timedelta.append(date_object1 - datetime.strptime(device_df.loc[i-1, 'Time'], \"%b %d, %Y %H:%M:%S.%f\"))\n",
    "\n",
    "        filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=50, microseconds=10000) and td > timedelta(days=0)]\n",
    "\n",
    "        filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) * 60 / 6e4 for td in filtered_timedelta_device]\n",
    "\n",
    "        all_timedelta_micro_device.extend(filtered_timedelta_micro_device)\n",
    "\n",
    "    sns.kdeplot(all_timedelta_micro_device, bw_method=0.1, label=device_type, color=color)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Milliseconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density for Category of Devices')\n",
    "plt.grid(True)\n",
    "#save_tikzplotlib(plt.gcf(), \"category_density_updated.tex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x0a79\n",
      "The dataframe is made up by 75 packets\n",
      "Filtered dataframe is made up by 56 packets\n",
      "8.6792 seconds\n"
     ]
    }
   ],
   "source": [
    "# Density function for specific device\n",
    "\n",
    "device_address = '0x0a79'\n",
    "\n",
    "device_df = df[(df['Source Zigbee'] == device_address)]\n",
    "device_df = device_df.sort_values(by=['Time'])\n",
    "print(device_address)\n",
    "print(f\"The dataframe is made up by {len(device_df)} packets\")\n",
    "\n",
    "device_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "date_string = device_df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(device_df)):\n",
    "    date_string = device_df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1\n",
    "\n",
    "filtered_timedelta_device = [td for td in list_of_timedelta if td < timedelta(days=0, hours=0, seconds=20, microseconds=0) and td > timedelta(days=0)]\n",
    "print(f\"Filtered dataframe is made up by {len(filtered_timedelta_device)} packets\")\n",
    "\n",
    "filtered_timedelta_micro_device = [(td.microseconds + 1000000 * td.seconds) / 1e6 for td in filtered_timedelta_device]\n",
    "if len(filtered_timedelta_micro_device) > 0:\n",
    "    print(f\"{max(filtered_timedelta_micro_device)} seconds\")\n",
    "\n",
    "\n",
    "#plt.hist(filtered_timedelta_micro_device, bins=100, edgecolor='k', color=colormap(devices.index(device_address)), label=device_address)\n",
    "sns.kdeplot(filtered_timedelta_micro_device, bw_method=0.1, label=device_address)\n",
    "plt.legend()\n",
    "plt.xlabel('Time in Seconds')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f\"Histogram of Timedelta Values\")\n",
    "plt.grid(True)\n",
    "\n",
    "hist, bin_edges = np.histogram(filtered_timedelta_micro_device, bins=100, range=(0, 20))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating mean and standard deviation from the total dataframe\n",
    "inter_arrival_time = []\n",
    "df = df.sort_values(by=['Time'])\n",
    "\n",
    "date_string = df.loc[0,'Time']\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string, date_format)\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    date_string = df.loc[i,'Time']\n",
    "    date_object1 = datetime.strptime(date_string, date_format)\n",
    "    iat = date_object1 - date_object\n",
    "    if(iat > timedelta(days=0, hours=0, seconds=0, microseconds=0) and\n",
    "       iat < timedelta(days=0, hours=0, seconds=0, microseconds=150000)):\n",
    "        inter_arrival_time.append(iat)\n",
    "    date_object = date_object1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is 0:00:00.010561\n",
      "Standard deviation is 0.02030713200671125 seconds\n",
      "Threshold is 0.07148239602013376 seconds\n"
     ]
    }
   ],
   "source": [
    "iat_mean = np.mean(inter_arrival_time)\n",
    "\n",
    "micro_iat = np.array([td.total_seconds() for td in inter_arrival_time])\n",
    "iat_stdev = np.std(micro_iat)\n",
    "\n",
    "print(f\"Mean is {iat_mean}\")\n",
    "print(f\"Standard deviation is {iat_stdev} seconds\")\n",
    "\n",
    "iat_threshold = iat_mean.total_seconds() + 3 * iat_stdev\n",
    "print(f\"Threshold is {iat_threshold} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe for device 0x310d is made up by 0 packets\n"
     ]
    }
   ],
   "source": [
    "## Calculate the time delta between packets from a specific device grouped by destination zigbee\n",
    "\n",
    "device_address = '0x310d'\n",
    "\n",
    "device_flux_df = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "device_flux_df = device_flux_df.sort_values(by=['Time'])\n",
    "print(f\"The dataframe for device {device_address} is made up by {len(device_flux_df)} packets\")\n",
    "\n",
    "device_flux_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#group by destination zigbee\n",
    "device_flux_df = device_flux_df.groupby('Destination Zigbee')\n",
    "\n",
    "legend_labels = []\n",
    "\n",
    "for destination_address, group_data in device_flux_df:\n",
    "    #if destination_address != device_address:\n",
    "    group = device_flux_df.get_group(destination_address)\n",
    "    group.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    date_string = group.loc[0,'Time']\n",
    "    date_string_without_zeros1 = date_string[:-3]\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    date_object = datetime.strptime(date_string_without_zeros1, date_format)\n",
    "\n",
    "    list_of_timedelta = []\n",
    "\n",
    "    for i in range(1, len(group)):\n",
    "        date_string = group.loc[i,'Time']\n",
    "        date_string_without_zeros = date_string[:-3]\n",
    "        date_object1 = datetime.strptime(date_string_without_zeros, date_format)\n",
    "        list_of_timedelta.append(date_object1 - date_object)\n",
    "        date_object = date_object1\n",
    "\n",
    "    threshold = timedelta(days=0, hours=0, seconds=0, microseconds=100000)\n",
    "    filtered = [td for td in list_of_timedelta if td < threshold]\n",
    "    list_micro = [td.microseconds + 1000000 * td.seconds for td in filtered]\n",
    "\n",
    "    if len(list_micro) > 1:\n",
    "        mean = np.mean(list_micro)\n",
    "        std = np.std(list_micro)\n",
    "\n",
    "        print(f\"Mean for destination {destination_address} is: {mean}\")\n",
    "        print(f\"Standard deviation for destination {destination_address} is: {std}\")\n",
    "        print(\"Calculated on {} packets\".format(len(list_micro)))\n",
    "        print(\"\")\n",
    "        legend_labels.append(destination_address)\n",
    "        plt.hist(list_micro, bins=100, edgecolor='k', alpha=0.7, label=destination_address)\n",
    "    \n",
    "    plt.legend(legend_labels, title='Destination Address')\n",
    "    plt.xlabel('Time in Microseconds')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Timedelta Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the time delta between a flux of two specific devices\n",
    "\n",
    "device_address = '0x0000'\n",
    "\n",
    "device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "device_packets = device_packets.sort_values(by=['Time'])\n",
    "device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "communicating_device = device_packets['Destination Zigbee'].unique()\n",
    "mask = communicating_device != device_address\n",
    "communicating_device = communicating_device[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0xfffc' '0xe694' '0x6e5f' '0xc8f0' '0xebe5' '0x4e52' '0xa209' '0x2cae'\n",
      " '0xe5c4' '0x054f' '0x482d' '0x265e' '0x61af' '0x9989' '0x5db6' '0x772c'\n",
      " '0x82eb' '0xb815' '0x09ac' '0xe011' '0x3181' '0x0a79' '0x4a30']\n"
     ]
    }
   ],
   "source": [
    "print(communicating_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_device = '0xaea8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication = device_packets[(device_packets['Destination Zigbee'] == other_device) | (device_packets['Source Zigbee'] == other_device)]\n",
    "communication.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexes/range.py:413\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m date_string \u001b[38;5;241m=\u001b[39m \u001b[43mcommunication\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m date_string_without_zeros1 \u001b[38;5;241m=\u001b[39m date_string[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      3\u001b[0m date_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexing.py:1183\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/frame.py:4221\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4215\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[1;32m   4217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   4218\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   4219\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   4220\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[0;32m-> 4221\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[1;32m   4224\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[1;32m   4225\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/FL IoT Forensics/Niccol-/venv/lib/python3.10/site-packages/pandas/core/indexes/range.py:415\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "date_string = communication.loc[0,'Time']\n",
    "date_string_without_zeros1 = date_string[:-3]\n",
    "date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "date_object = datetime.strptime(date_string_without_zeros1, date_format)\n",
    "\n",
    "list_of_timedelta = []\n",
    "\n",
    "for i in range(1, len(communication)):\n",
    "    date_string = communication.loc[i,'Time']\n",
    "    date_string_without_zeros = date_string[:-3]\n",
    "    date_object1 = datetime.strptime(date_string_without_zeros, date_format)\n",
    "    list_of_timedelta.append(date_object1 - date_object)\n",
    "    date_object = date_object1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_of_timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=0)\n",
    "max_threshold = timedelta(days=0, hours=0, seconds=0, microseconds=50000)\n",
    "filtered = [td for td in list_of_timedelta if td > min_threshold]\n",
    "filtered = [td for td in filtered if td < max_threshold]\n",
    "list_micro = [td.microseconds + 1000000 * td.seconds for td in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_labels = []\n",
    "\n",
    "if len(list_micro) > 1:\n",
    "    mean = np.mean(list_micro)\n",
    "    std = np.std(list_micro)\n",
    "\n",
    "    print(f\"Mean for destination {other_device} is: {mean}\")\n",
    "    print(f\"Standard deviation for destination {other_device} is: {std}\")\n",
    "    print(\"Calculated on {} packets\".format(len(list_micro)))\n",
    "    print(\"\")\n",
    "\n",
    "plt.hist(list_micro, bins=500, edgecolor='k')\n",
    "legend_labels.append(other_device)\n",
    "plt.legend(legend_labels, title='Device in Communication')\n",
    "plt.xlabel('Time in Microseconds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Timedelta Values for device {}'.format(device_address))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the dataframe into trains of packets according to the threshold\n",
    "\n",
    "# for each device i will have a list of list: a list of trains and each train is a list of packets\n",
    "# there will be the same train for both the devices in communication\n",
    "# from the packets in the train i will extract the features (delta time, packet length, packet type, payload, fcf, ecc)\n",
    "\n",
    "train_of_packets = []\n",
    "all_trains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device_address in devices:\n",
    "\n",
    "    device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2:\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    trains.to_csv(f'trains_{device_address}.csv', index=False)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    train_of_packets = []\n",
    "    all_trains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bin_edges) - 1):\n",
    "    print(f\"Bin {i + 1}: {bin_edges[i]:.2f} to {bin_edges[i + 1]:.2f} --> Count: {int(hist[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Possible MAC addresses for each device\n",
    "\n",
    "mac_addresses = df.groupby('Source Zigbee')['Extended Source'].unique().reset_index()\n",
    "\n",
    "for source, extended in zip(mac_addresses['Source Zigbee'], mac_addresses['Extended Source']):\n",
    "    if source not in devices:\n",
    "        mac_addresses.drop(mac_addresses[mac_addresses['Source Zigbee'] == source].index, inplace=True)\n",
    "    else:\n",
    "        mac_addresses.loc[mac_addresses['Source Zigbee'] == source, 'Source Zigbee'] = device_name_mapping[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model IAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ---------------------------------------- RANDOM FOREST ON DENSITY FUNCTION ---------------------------------------- ###\n",
    "\n",
    "## Matrix composed by the hystograms of the time delta between packets for each device removing acks and link status\n",
    "def create_ml_dataset_tot(df,threshold,number_of_features=100):\n",
    "    total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "    train_of_packets = []\n",
    "\n",
    "\n",
    "    for device_address in devices:\n",
    "\n",
    "        \n",
    "        # CREATE THE TRAINS OF PACKETS\n",
    "        all_trains = []\n",
    "        \n",
    "        device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "        device_packets = device_packets.sort_values(by=['Time'])\n",
    "        device_packets = device_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "        device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        prev_timestamp = None\n",
    "        for index, row in device_packets.iterrows():\n",
    "            date_string = row['Time']\n",
    "            date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "            timestamp = datetime.strptime(date_string, date_format)\n",
    "            if prev_timestamp is not None:\n",
    "                delta_time = timestamp - prev_timestamp\n",
    "                if delta_time < threshold:\n",
    "                    train_of_packets.append(row)\n",
    "                else:\n",
    "                    if len(train_of_packets) >= 2 and train_of_packets[0]['Source Zigbee'] == device_address:\n",
    "                        all_trains.append(train_of_packets)\n",
    "                    train_of_packets = [row]\n",
    "\n",
    "            prev_timestamp = timestamp\n",
    "\n",
    "        if len(train_of_packets) >= 2:\n",
    "            all_trains.append(train_of_packets)\n",
    "\n",
    "        trains = pd.DataFrame(all_trains)\n",
    "\n",
    "        del all_trains\n",
    "\n",
    "        # REMOVE DUPLICATES\n",
    "        cleaned_df = []\n",
    "        for i in range(len(trains)):\n",
    "            train = trains.loc[i]\n",
    "            seen_packets = set()\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None:\n",
    "                    packet_tuple = (\n",
    "                        packet['Length'],\n",
    "                        packet['FCF IEEE'],\n",
    "                        packet['Sequence Number'],\n",
    "                        packet['Source Zigbee'],\n",
    "                        packet['Destination Zigbee'],\n",
    "                        packet['Payload Length']\n",
    "                    )\n",
    "                    if packet_tuple not in seen_packets:\n",
    "                        seen_packets.add(packet_tuple)\n",
    "                        result.append(packet)\n",
    "            cleaned_df.append(result)\n",
    "\n",
    "        cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "        # REMOVE ACK PACKETS AND LINK STATUS\n",
    "        no_ack_df = []\n",
    "        for i in range(len(cleaned_df)):\n",
    "            train = cleaned_df.loc[i]\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                    result.append(packet)\n",
    "            no_ack_df.append(result)\n",
    "\n",
    "        no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "        # CREATE THE HISTOGRAMS\n",
    "        histograms = []\n",
    "        for i in range(len(no_ack_df)):\n",
    "            train = no_ack_df.loc[i]\n",
    "            list_of_timedelta = []\n",
    "            for j in range(1, len(train)):\n",
    "                if train[j] is not None:\n",
    "                    date_string = train[j]['Time']\n",
    "                    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                    date_object = datetime.strptime(date_string, date_format)\n",
    "                    date_string1 = train[j-1]['Time']\n",
    "                    date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                    list_of_timedelta.append(date_object - date_object1)\n",
    "            list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "            hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "            histograms.append(hist)\n",
    "\n",
    "        # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "        histogram_matrix = np.array(histograms)\n",
    "        last_column = np.repeat(device_address, len(histogram_matrix))\n",
    "        matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "        # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "        total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n",
    "    \n",
    "    return total_matrix\n",
    "\n",
    "\n",
    "## Matrix composed by the hystograms of the time delta between outgoing packets for each device removing acks and link status\n",
    "\n",
    "def create_ml_dataset_out(df,threshold,number_of_features=100):\n",
    "    total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "    train_of_packets = []\n",
    "\n",
    "\n",
    "    for device_address in devices:\n",
    "        \n",
    "        # CREATE THE TRAINS OF PACKETS\n",
    "        all_trains = []\n",
    "        \n",
    "        device_packets = df[(df['Source Zigbee'] == device_address)]\n",
    "        device_packets = device_packets.sort_values(by=['Time'])\n",
    "        device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "        device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        prev_timestamp = None\n",
    "        for index, row in device_packets.iterrows():\n",
    "            date_string = row['Time']\n",
    "            date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "            timestamp = datetime.strptime(date_string, date_format)\n",
    "            if prev_timestamp is not None:\n",
    "                delta_time = timestamp - prev_timestamp\n",
    "                if delta_time < threshold:\n",
    "                    train_of_packets.append(row)\n",
    "                else:\n",
    "                    if len(train_of_packets) >= 2 or device_address == '0x907b':\n",
    "                        all_trains.append(train_of_packets)\n",
    "                    train_of_packets = [row]\n",
    "\n",
    "            prev_timestamp = timestamp\n",
    "\n",
    "        if len(train_of_packets) >= 2:\n",
    "            all_trains.append(train_of_packets)\n",
    "\n",
    "        trains = pd.DataFrame(all_trains)\n",
    "\n",
    "        del all_trains\n",
    "\n",
    "        # REMOVE DUPLICATES\n",
    "        cleaned_df = []\n",
    "        for i in range(len(trains)):\n",
    "            train = trains.loc[i]\n",
    "            seen_packets = set()\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None:\n",
    "                    packet_tuple = (\n",
    "                        packet['Length'],\n",
    "                        packet['FCF IEEE'],\n",
    "                        packet['Sequence Number'],\n",
    "                        packet['Source Zigbee'],\n",
    "                        packet['Destination Zigbee'],\n",
    "                        packet['Payload Length']\n",
    "                    )\n",
    "                    if packet_tuple not in seen_packets:\n",
    "                        seen_packets.add(packet_tuple)\n",
    "                        result.append(packet)\n",
    "            cleaned_df.append(result)\n",
    "\n",
    "        cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "        # REMOVE ACK PACKETS AND LINK STATUS\n",
    "        no_ack_df = []\n",
    "        for i in range(len(cleaned_df)):\n",
    "            train = cleaned_df.loc[i]\n",
    "            result = []\n",
    "            for packet in train:\n",
    "                if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                    result.append(packet)\n",
    "            no_ack_df.append(result)\n",
    "\n",
    "        no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "        # CREATE THE HISTOGRAMS\n",
    "        histograms = []\n",
    "        for i in range(len(no_ack_df)):\n",
    "            train = no_ack_df.loc[i]\n",
    "            list_of_timedelta = []\n",
    "            for j in range(1, len(train)):\n",
    "                if train[j] is not None:\n",
    "                    date_string = train[j]['Time']\n",
    "                    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                    date_object = datetime.strptime(date_string, date_format)\n",
    "                    date_string1 = train[j-1]['Time']\n",
    "                    date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                    list_of_timedelta.append(date_object - date_object1)\n",
    "            list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "            hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "            histograms.append(hist)\n",
    "\n",
    "        # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "        histogram_matrix = np.array(histograms)\n",
    "        last_column = np.repeat(device_address, len(histogram_matrix))\n",
    "        matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "        # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "        total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n",
    "    return total_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev classificatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = True #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=17.6, microseconds=0)\n",
    "\n",
    "tot_mtrx2 = create_ml_dataset_out(df2,threshold,30)\n",
    "\n",
    "\n",
    "last_column = pd.Series(tot_mtrx2[:, -1])\n",
    "\n",
    "if CLS:           \n",
    "    last_column_mapped = last_column.map(device_type_mapping)\n",
    "else:\n",
    "    last_column_mapped = last_column.map(device_name_mapping)\n",
    "\n",
    "tot_mtrx2[:, -1] = last_column_mapped.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = True #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=17.6, microseconds=0)\n",
    "tot_mtrx4 = create_ml_dataset_out(df4,threshold,30)\n",
    "\n",
    "last_column = pd.Series(tot_mtrx4[:, -1])\n",
    "\n",
    "if CLS:           \n",
    "    last_column_mapped = last_column.map(device_type_mapping)\n",
    "else:\n",
    "    last_column_mapped = last_column.map(device_name_mapping)\n",
    "\n",
    "\n",
    "tot_mtrx4[:, -1] = last_column_mapped.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Importance\n",
      "[0.19830717 0.10372784 0.15879428 0.14105783 0.04916855 0.03304448\n",
      " 0.02139679 0.01335526 0.01367604 0.01341185 0.01688153 0.016063\n",
      " 0.00973655 0.00874496 0.01004708 0.00800133 0.00960403 0.00759703\n",
      " 0.00645545 0.01039828 0.0111655  0.02518408 0.02139811 0.01305403\n",
      " 0.01255703 0.02003762 0.01751282 0.01362403 0.00751848 0.00847896]\n",
      "\n",
      "Accuracy: 0.5909090909090909\n",
      "Precision: 0.4579926922674479\n",
      "Recall: 0.3940052116220376\n",
      "F1: 0.4082902280415866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "X_train = tot_mtrx2[:, :-1]\n",
    "y_train = tot_mtrx2[:, -1]\n",
    "\n",
    "X_test = tot_mtrx4[:, :-1]\n",
    "y_test = tot_mtrx4[:, -1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Importance\n",
      "[0.2067688  0.09881455 0.14916022 0.13606956 0.04779896 0.02854087\n",
      " 0.02643032 0.01215158 0.01413022 0.01697184 0.01707107 0.01653794\n",
      " 0.01026192 0.01166526 0.00874397 0.00844369 0.01022636 0.00789334\n",
      " 0.00554285 0.00839743 0.011895   0.02560328 0.02312285 0.01454583\n",
      " 0.01350608 0.01836216 0.01730961 0.01536116 0.0077569  0.01091639]\n",
      "\n",
      "Accuracy: 0.6482758620689655\n",
      "Precision: 0.6473005176036488\n",
      "Recall: 0.5151602334303927\n",
      "F1: 0.4839761990644816\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Classifier on the total matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "X = tot_mtrx2[:, :-1]\n",
    "y = tot_mtrx2[:, -1]\n",
    "\n",
    "# X_test = tot_mtrx4[:, :-1]\n",
    "# y_test = tot_mtrx4[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "\n",
    "VERSION = 2 #4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = False #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "ID_SIMP = False #If True it will be used the simple mapping for device identification\n",
    "\n",
    "df2 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v2 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df2 = df2[df2['File'].str.contains('idle')]\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=100000)\n",
    "\n",
    "tot_mtrx2 = create_ml_dataset_out(df2,threshold,30)\n",
    "\n",
    "\n",
    "last_column = pd.Series(tot_mtrx2[:, -1])\n",
    "\n",
    "if CLS:\n",
    "    last_column_mapped = last_column.map(device_type_mapping)\n",
    "    \n",
    "else:\n",
    "    if ID_SIMP:\n",
    "        last_column_mapped = last_column.map(device_name_mapping_simple)\n",
    "    else:\n",
    "        last_column_mapped = last_column.map(device_name_mapping)\n",
    "\n",
    "tot_mtrx2[:, -1] = last_column_mapped.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load df from csv\n",
    "VERSION = 4\n",
    "IDLE_FILTER = False\n",
    "\n",
    "CLS = False #If True it will be profrmed Device Classification, otherwise Device Identification\n",
    "ID_SIMP = False #If True it will be used the simple mapping for device identification\n",
    "\n",
    "df4 = pd.read_csv(f'final_dataFrame_v{VERSION}.csv')\n",
    "load_mapping(VERSION)\n",
    "devices_name_v4 = devices_name.copy()\n",
    "\n",
    "if IDLE_FILTER:\n",
    "    df4 = df4[df4['File'].str.contains('idle')]\n",
    "\n",
    "threshold = timedelta(days=0, hours=0, seconds=0, microseconds=100000)\n",
    "tot_mtrx4 = create_ml_dataset_out(df4,threshold,30)\n",
    "\n",
    "last_column = pd.Series(tot_mtrx4[:, -1])\n",
    "\n",
    "if CLS:           \n",
    "    last_column_mapped = last_column.map(device_type_mapping)\n",
    "else:\n",
    "    if ID_SIMP:\n",
    "        last_column_mapped = last_column.map(device_name_mapping_simple)\n",
    "    else:\n",
    "        last_column_mapped = last_column.map(device_name_mapping)\n",
    "    \n",
    "\n",
    "tot_mtrx4[:, -1] = last_column_mapped.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Importance\n",
      "[0.02423178 0.12062075 0.0606063  0.04558478 0.03937231 0.02843564\n",
      " 0.02547081 0.02373033 0.02361537 0.02618015 0.02897114 0.04149232\n",
      " 0.03736395 0.05153788 0.07164105 0.04642249 0.04474787 0.04775897\n",
      " 0.04659814 0.04784253 0.02576634 0.02294963 0.01841412 0.01587695\n",
      " 0.00872579 0.00839109 0.00352827 0.00370953 0.00514859 0.00526514]\n",
      "\n",
      "Accuracy: 0.5266887539112829\n",
      "Precision: 0.14007209993269912\n",
      "Recall: 0.11513114489982122\n",
      "F1: 0.11797041495750023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "X_train = tot_mtrx2[:, :-1]\n",
    "y_train = tot_mtrx2[:, -1]\n",
    "\n",
    "X_test = tot_mtrx4[:, :-1]\n",
    "y_test = tot_mtrx4[:, -1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Importance\n",
      "[0.02258282 0.11629325 0.05795128 0.04711291 0.04221216 0.02790107\n",
      " 0.02698825 0.02285519 0.02302664 0.02964301 0.02894268 0.0395065\n",
      " 0.03663724 0.05232381 0.070627   0.04882762 0.04312297 0.05123671\n",
      " 0.04996291 0.04353506 0.02465368 0.0225985  0.0187382  0.01573755\n",
      " 0.01040687 0.00690844 0.00399908 0.00451422 0.00607185 0.00508253]\n",
      "\n",
      "Accuracy: 0.5656868626274745\n",
      "Precision: 0.20156346971454273\n",
      "Recall: 0.15881827328980813\n",
      "F1: 0.1634774430784928\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Classifier on the total matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = tot_mtrx2[:, :-1]\n",
    "y = tot_mtrx2[:, -1]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "max_depths = range(15, 25)\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "\n",
    "# for max_depth in max_depths:\n",
    "#     clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth)\n",
    "#     f1score = []\n",
    "#     accuracy = []\n",
    "#     for train_index, test_index in cv.split(X, y):\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index] \n",
    "#         clf.fit(X_train, y_train)\n",
    "#         y_pred = clf.predict(X_test)\n",
    "#         #f1score.append(f1_score(y_test, y_pred, average='macro'))\n",
    "#         accuracy.append(accuracy_score(y_test, y_pred))\n",
    "#     #f1_scores.append(np.mean(f1score))\n",
    "#     accuracies.append(np.mean(accuracy))\n",
    "\n",
    "# plt.plot(max_depths, accuracies)\n",
    "# plt.xlabel('Max Depth')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy vs Max Depth')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X = tot_mtrx2[:, :-1]\n",
    "y = tot_mtrx2[:, -1]\n",
    "\n",
    "# X_test = tot_mtrx4[:, :-1]\n",
    "# y_test = tot_mtrx4[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=21)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix composed by the hystograms of the time delta between outgoing packets for each device removing acks and link status\n",
    "\n",
    "number_of_features = 100\n",
    "total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "train_of_packets = []\n",
    "\n",
    "\n",
    "for device_address in devices:\n",
    "    \n",
    "    # CREATE THE TRAINS OF PACKETS\n",
    "    all_trains = []\n",
    "    \n",
    "    device_packets = df[(df['Source Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2 or device_address == '0x907b':\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    # REMOVE DUPLICATES\n",
    "    cleaned_df = []\n",
    "    for i in range(len(trains)):\n",
    "        train = trains.loc[i]\n",
    "        seen_packets = set()\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                packet_tuple = (\n",
    "                    packet['Length'],\n",
    "                    packet['FCF IEEE'],\n",
    "                    packet['Sequence Number'],\n",
    "                    packet['Source Zigbee'],\n",
    "                    packet['Destination Zigbee'],\n",
    "                    packet['Payload Length']\n",
    "                )\n",
    "                if packet_tuple not in seen_packets:\n",
    "                    seen_packets.add(packet_tuple)\n",
    "                    result.append(packet)\n",
    "        cleaned_df.append(result)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "    # REMOVE ACK PACKETS AND LINK STATUS\n",
    "    no_ack_df = []\n",
    "    for i in range(len(cleaned_df)):\n",
    "        train = cleaned_df.loc[i]\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                result.append(packet)\n",
    "        no_ack_df.append(result)\n",
    "\n",
    "    no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "    # CREATE THE HISTOGRAMS\n",
    "    histograms = []\n",
    "    for i in range(len(no_ack_df)):\n",
    "        train = no_ack_df.loc[i]\n",
    "        list_of_timedelta = []\n",
    "        for j in range(1, len(train)):\n",
    "            if train[j] is not None:\n",
    "                date_string = train[j]['Time']\n",
    "                date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                date_object = datetime.strptime(date_string, date_format)\n",
    "                date_string1 = train[j-1]['Time']\n",
    "                date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                list_of_timedelta.append(date_object - date_object1)\n",
    "        list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "        hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "        histograms.append(hist)\n",
    "\n",
    "    # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "    histogram_matrix = np.array(histograms)\n",
    "    last_column = np.repeat(device_address, len(histogram_matrix))\n",
    "    matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "    # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "    total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix composed by the hystograms of the time delta between outgoing packets for each type of device removing acks and link status\n",
    "\n",
    "number_of_features = 30\n",
    "total_matrix = np.empty((0, number_of_features + 1), int)\n",
    "\n",
    "train_of_packets = []\n",
    "\n",
    "\n",
    "for device_address in devices:\n",
    "    \n",
    "    # CREATE THE TRAINS OF PACKETS\n",
    "    all_trains = []\n",
    "    \n",
    "    device_packets = df[(df['Source Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2 or device_address == '0x907b':\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    # REMOVE DUPLICATES\n",
    "    cleaned_df = []\n",
    "    for i in range(len(trains)):\n",
    "        train = trains.loc[i]\n",
    "        seen_packets = set()\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                packet_tuple = (\n",
    "                    packet['Length'],\n",
    "                    packet['FCF IEEE'],\n",
    "                    packet['Sequence Number'],\n",
    "                    packet['Source Zigbee'],\n",
    "                    packet['Destination Zigbee'],\n",
    "                    packet['Payload Length']\n",
    "                )\n",
    "                if packet_tuple not in seen_packets:\n",
    "                    seen_packets.add(packet_tuple)\n",
    "                    result.append(packet)\n",
    "        cleaned_df.append(result)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "    # REMOVE ACK PACKETS AND LINK STATUS\n",
    "    no_ack_df = []\n",
    "    for i in range(len(cleaned_df)):\n",
    "        train = cleaned_df.loc[i]\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                result.append(packet)\n",
    "        no_ack_df.append(result)\n",
    "\n",
    "    no_ack_df = pd.DataFrame(no_ack_df)\n",
    "\n",
    "    # CREATE THE HISTOGRAMS\n",
    "    histograms = []\n",
    "    for i in range(len(no_ack_df)):\n",
    "        train = no_ack_df.loc[i]\n",
    "        list_of_timedelta = []\n",
    "        for j in range(1, len(train)):\n",
    "            if train[j] is not None:\n",
    "                date_string = train[j]['Time']\n",
    "                date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "                date_object = datetime.strptime(date_string, date_format)\n",
    "                date_string1 = train[j-1]['Time']\n",
    "                date_object1 = datetime.strptime(date_string1, date_format)\n",
    "                list_of_timedelta.append(date_object - date_object1)\n",
    "        list_of_timedelta_micro = [td.microseconds for td in list_of_timedelta]\n",
    "        hist, bin_edges = np.histogram(list_of_timedelta_micro, bins=number_of_features, range=(0, threshold.microseconds))\n",
    "        histograms.append(hist)\n",
    "\n",
    "    # CREATE THE MATRIX WITH THE DEVICE ADDRESS\n",
    "    histogram_matrix = np.array(histograms)\n",
    "    last_column = np.repeat(device_type_mapping[device_address], len(histogram_matrix))\n",
    "    matrix_with_device = np.column_stack((histogram_matrix, last_column))\n",
    "\n",
    "    # APPEND THE MATRIX TO THE TOTAL MATRIX\n",
    "    total_matrix = np.append(total_matrix, matrix_with_device, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- TSFRESH -------------------------------------------------- ###\n",
    "\n",
    "# Dataframe with train of packets without acks and link status\n",
    "\n",
    "train_of_packets = []\n",
    "no_ack_df = []\n",
    "\n",
    "for device_address in devices:\n",
    "    \n",
    "    # CREATE THE TRAINS OF PACKETS\n",
    "    all_trains = []\n",
    "    \n",
    "    device_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "    device_packets = device_packets.sort_values(by=['Time'])\n",
    "    device_packets = device_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee', 'Payload Length']]\n",
    "    device_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    prev_timestamp = None\n",
    "    for index, row in device_packets.iterrows():\n",
    "        date_string = row['Time']\n",
    "        date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "        timestamp = datetime.strptime(date_string, date_format)\n",
    "        if prev_timestamp is not None:\n",
    "            delta_time = timestamp - prev_timestamp\n",
    "            if delta_time < threshold:\n",
    "                train_of_packets.append(row)\n",
    "            else:\n",
    "                if len(train_of_packets) >= 2:\n",
    "                    all_trains.append(train_of_packets)\n",
    "                train_of_packets = [row]\n",
    "\n",
    "        prev_timestamp = timestamp\n",
    "\n",
    "    if len(train_of_packets) >= 2:\n",
    "        all_trains.append(train_of_packets)\n",
    "\n",
    "    trains = pd.DataFrame(all_trains)\n",
    "\n",
    "    del all_trains\n",
    "\n",
    "    # REMOVE DUPLICATES\n",
    "    cleaned_df = []\n",
    "    for i in range(len(trains)):\n",
    "        train = trains.loc[i]\n",
    "        seen_packets = set()\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None:\n",
    "                packet_tuple = (\n",
    "                    packet['Length'],\n",
    "                    packet['FCF IEEE'],\n",
    "                    packet['Sequence Number'],\n",
    "                    packet['Source Zigbee'],\n",
    "                    packet['Destination Zigbee'],\n",
    "                    packet['Payload Length']\n",
    "                )\n",
    "                if packet_tuple not in seen_packets:\n",
    "                    seen_packets.add(packet_tuple)\n",
    "                    result.append(packet)\n",
    "        cleaned_df.append(result)\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_df)\n",
    "\n",
    "    # REMOVE ACK PACKETS AND LINK STATUS\n",
    "    for i in range(len(cleaned_df)):\n",
    "        train = cleaned_df.loc[i]\n",
    "        result = []\n",
    "        for packet in train:\n",
    "            if packet is not None and packet['FCF IEEE'] != \"0x0002\" and packet['FCF IEEE'] != \"0x8841\":\n",
    "                result.append(packet)\n",
    "        if(len(result) > 0 and result[0]['Source Zigbee'] in devices):\n",
    "            no_ack_df.append(result)\n",
    "\n",
    "no_ack_df = pd.DataFrame(no_ack_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single DataFrame with all the packets and 'id' column to identify the train of packets\n",
    "\n",
    "all_packets = list()\n",
    "\n",
    "for i in range(len(no_ack_df)):\n",
    "    train = no_ack_df.loc[i]\n",
    "    for packet in train:\n",
    "        if packet is not None:\n",
    "            all_packets.append({\n",
    "                    'id': i,\n",
    "                    'Time': packet['Time'],\n",
    "                    'Delta Time': packet['Delta Time'],\n",
    "                    'Length': packet['Length'],\n",
    "                    'Sequence Number': packet['Sequence Number'],\n",
    "                    'Payload Length': packet['Payload Length'] if not np.isnan(packet['Payload Length']) else 0})\n",
    "\n",
    "result_df = pd.DataFrame(all_packets)\n",
    "\n",
    "# Set Delta Time of the first packet of each train to 0\n",
    "id_changes = result_df['id'] != result_df['id'].shift(1)\n",
    "result_df.loc[id_changes, 'Delta Time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tsfresh used to get features from trains of packets\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters, ComprehensiveFCParameters\n",
    "\n",
    "result_df['Time'] = pd.to_datetime(result_df['Time'], format=\"%b %d, %Y %H:%M:%S.%f\")\n",
    "\n",
    "# Extract features from the dataframe\n",
    "extracted_features = extract_features(result_df, column_id=\"id\", column_sort=\"Time\", default_fc_parameters=MinimalFCParameters())\n",
    "\n",
    "# Impute the extracted features\n",
    "impute(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the device address to the dataframe of features to be able to classify the packets\n",
    "for i in range(len(no_ack_df)):\n",
    "    extracted_features.loc[i, 'Source Zigbee'] = no_ack_df.loc[i].loc[0].loc['Source Zigbee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to every row of extracted_features the number of packets that fall inside the windows of threshold / 10\n",
    "\n",
    "N = 10\n",
    "for i in range(len(extracted_features)):\n",
    "    mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    deltas = result_df[result_df['id'] == i]['Delta Time']\n",
    "    for n in range(N):\n",
    "        lower_bound = n * (threshold_float / N)\n",
    "        upper_bound = (n + 1) * (threshold_float / N)\n",
    "        count = ((deltas >= lower_bound) & \n",
    "                 (deltas < upper_bound)).sum()\n",
    "\n",
    "        # Add the count to the extracted features\n",
    "        match n:\n",
    "            case 0:\n",
    "                extracted_features.loc[i, 'Window 1'] = count\n",
    "            case 1:\n",
    "                extracted_features.loc[i, 'Window 2'] = count\n",
    "            case 2:\n",
    "                extracted_features.loc[i, 'Window 3'] = count\n",
    "            case 3:\n",
    "                extracted_features.loc[i, 'Window 4'] = count\n",
    "            case 4:\n",
    "                extracted_features.loc[i, 'Window 5'] = count\n",
    "            case 5:\n",
    "                extracted_features.loc[i, 'Window 6'] = count\n",
    "            case 6:\n",
    "                extracted_features.loc[i, 'Window 7'] = count\n",
    "            case 7:\n",
    "                extracted_features.loc[i, 'Window 8'] = count\n",
    "            case 8:\n",
    "                extracted_features.loc[i, 'Window 9'] = count\n",
    "            case 9:\n",
    "                extracted_features.loc[i, 'Window 10'] = count\n",
    "            case _:\n",
    "                print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "#plot_confusion_matrix(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=devices)\n",
    "disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "plt.show()\n",
    "#plt.figure(figsize=(10, 10))\n",
    "#sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.ylabel('Actual')\n",
    "#plt.title('Confusion Matrix')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the RandomForestClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the features with low importance\n",
    "feature_threshold = 0.00000\n",
    "selected_features = np.where(clf.feature_importances_ > feature_threshold)[0]\n",
    "selected_features = X_train.columns[selected_features]\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to every row of extracted_features the number of packets that fall inside the window {-2, -2 + i(4/N)}\n",
    "\n",
    "N = 5\n",
    "max_batches = 0\n",
    "id_max_batches = 0\n",
    "\n",
    "for i in range(len(extracted_features)):\n",
    "    batches = 0\n",
    "    mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    maximum = extracted_features.loc[i, 'Delta Time__maximum']\n",
    "\n",
    "    print(f\"Batches for train {i}\")\n",
    "    print(f\"Mean: {mu}, std dev: {sigma}\")\n",
    "    print(f\"Maximum: {maximum}\")\n",
    "    if(sigma == 0):\n",
    "        continue\n",
    "    for n in range(0,10):\n",
    "        batch = mu - 2 * sigma + n * (4 * sigma / N)\n",
    "        if(batch < 0):\n",
    "            continue\n",
    "        if(batch > maximum):\n",
    "            break\n",
    "        batches += 1\n",
    "        print(batch)\n",
    "    if(batches > max_batches):\n",
    "        max_batches = batches\n",
    "        id_max_batches = i\n",
    "    print('')\n",
    "\n",
    "print(f\"Max batches: {max_batches} for train {id_max_batches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(N):\n",
    "    lower_bound = n * (threshold_float / N)\n",
    "    upper_bound = (n + 1) * (threshold_float / N)\n",
    "    print(f\"{lower_bound} -> {upper_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = result_df.loc[12694,'Delta Time']\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a / threshold_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x_points = np.linspace(mu - 2 * sigma, mu + 2 * sigma, 100)\n",
    "y_points = norm.pdf(x_points, mu, sigma)\n",
    "\n",
    "plt.plot(x_points, y_points, color='black', label=f\"Mean: {mu:.2f}, Std Dev: {sigma:.2f}\")\n",
    "plt.title('Graph of the Probability Density Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost classifiers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "class_mapping = {label:idx for idx,label in enumerate(np.unique(y))}\n",
    "target = y.map(class_mapping)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2)\n",
    "\n",
    "# Instantiate the XGBoostClassifier\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=22, random_state=42)\n",
    "\n",
    "# Train the XGBoostClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the XGBoostClassifier on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBClassifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'extracted_features' is your DataFrame\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "target = y.map(class_mapping)\n",
    "\n",
    "# Instantiate the XGBoostClassifier\n",
    "clf = xgb.XGBClassifier(objective='multi:softmax', num_class=22, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, target):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # Train the XGBoostClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- COORDINATOR DATAFRAME -------------------------------------------------- ###\n",
    "\n",
    "# Dataframe with train of packets without acks and link status\n",
    "\n",
    "train_of_packets = []\n",
    "coordinator_final_df = []\n",
    "\n",
    "device_address = '0x0000'\n",
    "    \n",
    "# CREATE THE TRAINS OF PACKETS\n",
    "all_trains = []\n",
    "\n",
    "coordinator_packets = df[(df['Source Zigbee'] == device_address) | (df['Destination Zigbee'] == device_address)]\n",
    "coordinator_packets = coordinator_packets.sort_values(by=['Time'])\n",
    "coordinator_packets = coordinator_packets[['Time', 'Delta Time', 'Length', 'FCF IEEE', 'Sequence Number', 'Source Zigbee', 'Destination Zigbee','FCF Zigbee', 'Payload Length', 'Action']]\n",
    "coordinator_packets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "prev_timestamp = None\n",
    "for index, row in coordinator_packets.iterrows():\n",
    "    date_string = row['Time']\n",
    "    date_format = \"%b %d, %Y %H:%M:%S.%f\"\n",
    "    timestamp = datetime.strptime(date_string, date_format)\n",
    "    if prev_timestamp is not None:\n",
    "        delta_time = timestamp - prev_timestamp\n",
    "        if delta_time < threshold:\n",
    "            train_of_packets.append(row)\n",
    "        else:\n",
    "            if len(train_of_packets) >= 2:\n",
    "                all_trains.append(train_of_packets)\n",
    "            train_of_packets = [row]\n",
    "\n",
    "    prev_timestamp = timestamp\n",
    "\n",
    "if len(train_of_packets) >= 2:\n",
    "    all_trains.append(train_of_packets)\n",
    "\n",
    "coordinator_trains = pd.DataFrame(all_trains)\n",
    "\n",
    "del all_trains\n",
    "\n",
    "# REMOVE DUPLICATES\n",
    "coordinator_cleaned_df = []\n",
    "for i in range(len(coordinator_trains)):\n",
    "    train = coordinator_trains.loc[i]\n",
    "    seen_packets = set()\n",
    "    result = []\n",
    "    for packet in train:\n",
    "        if packet is not None:\n",
    "            packet_tuple = (\n",
    "                packet['Length'],\n",
    "                packet['FCF IEEE'],\n",
    "                packet['Sequence Number'],\n",
    "                packet['Source Zigbee'],\n",
    "                packet['Destination Zigbee'],\n",
    "                packet['Payload Length']\n",
    "                )\n",
    "            if packet_tuple not in seen_packets:\n",
    "                seen_packets.add(packet_tuple)\n",
    "                result.append(packet)\n",
    "    coordinator_cleaned_df.append(result)\n",
    "\n",
    "coordinator_cleaned_df = pd.DataFrame(coordinator_cleaned_df)\n",
    "\n",
    "# REMOVE LINK STATUS\n",
    "for i in range(len(coordinator_cleaned_df)):\n",
    "    train = coordinator_cleaned_df.loc[i]\n",
    "    result = []\n",
    "    for packet in train:\n",
    "        if packet is not None and packet['FCF IEEE'] != \"0x8841\":\n",
    "            result.append(packet)\n",
    "    if(len(result) > 0 and result[0]['Source Zigbee'] in devices):\n",
    "        coordinator_final_df.append(result)\n",
    "\n",
    "coordinator_final_df = pd.DataFrame(coordinator_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ++++++++++++++ MATRIX FOR BIDIRECTIONAL NEURAL NETWORKS ++++++++++++++ ###\n",
    "\n",
    "# Counts the number of packets for each train and make mean\n",
    "not_null = coordinator_final_df.notnull().sum(axis=1)\n",
    "mean_not_null = not_null.mean()\n",
    "rounded_mean = math.ceil(mean_not_null)\n",
    "\n",
    "num_of_features = 6\n",
    "\n",
    "# Create a train matrix for bidirectional neural networks with the following features for first 14 packets in a train:\n",
    "# Delta Time, Length, Payload Length, Source Zigbee, FCF Zigbee, IAT from last ingoing or outgoing packet\n",
    "matrix_for_binn = np.zeros((len(coordinator_final_df), rounded_mean, num_of_features))\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    j = 0\n",
    "    sum_delta_1 = 0\n",
    "    sum_delta_0 = 0\n",
    "    seen_1 = False\n",
    "    seen_0 = False\n",
    "    for packet in train:\n",
    "        if j < rounded_mean and packet is not None:\n",
    "            matrix_for_binn[i][j][0] = packet['Delta Time']\n",
    "            matrix_for_binn[i][j][1] = packet['Length']\n",
    "            matrix_for_binn[i][j][2] = packet['Payload Length']\n",
    "            matrix_for_binn[i][j][3] = 1 if packet['Source Zigbee'] == '0x0000' else 0 # 1 if coordinator, 0 if not\n",
    "            matrix_for_binn[i][j][4] = 0 if pd.isna(packet['FCF Zigbee']) else 1 # 1 if zigbee, 0 if not\n",
    "\n",
    "            # iat from last ingoing or outgoing packet\n",
    "            if matrix_for_binn[i][j][3] == 1:\n",
    "                if seen_1:\n",
    "                    sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "                    sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "                    matrix_for_binn[i][j][5] = sum_delta_1\n",
    "                    sum_delta_1 = 0\n",
    "                else:\n",
    "                    matrix_for_binn[i][j][5] = 0\n",
    "                    seen_1 = True\n",
    "                    if seen_0:\n",
    "                        sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "            else:\n",
    "                if seen_0:\n",
    "                    sum_delta_0 += matrix_for_binn[i][j][0]\n",
    "                    sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "                    matrix_for_binn[i][j][5] = sum_delta_0\n",
    "                    sum_delta_0 = 0\n",
    "                else:\n",
    "                    matrix_for_binn[i][j][5] = 0\n",
    "                    seen_0 = True\n",
    "                    if seen_1:\n",
    "                        sum_delta_1 += matrix_for_binn[i][j][0]\n",
    "\n",
    "            j += 1\n",
    "\n",
    "\n",
    "# List to keep track of the device address of each train\n",
    "device_address_list = []\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    device_address_list.append(device_name_mapping[train[0]['Source Zigbee']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "occurrences = Counter(device_address_list)\n",
    "for occurrence in occurrences:\n",
    "    print(f\"{occurrence}: {occurrences[occurrence]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test matrix for bidirectional neural networks with same features as train matrix\n",
    "test_matrix = np.zeros((len(coordinator_final_df), rounded_mean, num_of_features))\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    j = 0\n",
    "    for packet in train:\n",
    "        if j < rounded_mean and packet is not None:\n",
    "            test_matrix[i][j][0] = packet['Delta Time']\n",
    "            test_matrix[i][j][1] = packet['Length']\n",
    "            test_matrix[i][j][2] = packet['Payload Length']\n",
    "            test_matrix[i][j][3] = 1 if packet['Source Zigbee'] == '0x0000' else 0 # 1 if coordinator, 0 if not\n",
    "            test_matrix[i][j][4] = 0 if pd.isna(packet['FCF Zigbee']) else 1 # 1 if zigbee, 0 if not\n",
    "\n",
    "            # iat from last ingoing or outgoing packet\n",
    "            if test_matrix[i][j][3] == 1:\n",
    "                if seen_1:\n",
    "                    sum_delta_0 += test_matrix[i][j][0]\n",
    "                    sum_delta_1 += test_matrix[i][j][0]\n",
    "                    test_matrix[i][j][5] = sum_delta_1\n",
    "                    sum_delta_1 = 0\n",
    "                else:\n",
    "                    test_matrix[i][j][5] = 0\n",
    "                    seen_1 = True\n",
    "                    if seen_0:\n",
    "                        sum_delta_0 += test_matrix[i][j][0]\n",
    "            else:\n",
    "                if seen_0:\n",
    "                    sum_delta_0 += test_matrix[i][j][0]\n",
    "                    sum_delta_1 += test_matrix[i][j][0]\n",
    "                    test_matrix[i][j][5] = sum_delta_0\n",
    "                    sum_delta_0 = 0\n",
    "                else:\n",
    "                    test_matrix[i][j][5] = 0\n",
    "                    seen_0 = True\n",
    "                    if seen_1:\n",
    "                        sum_delta_1 += test_matrix[i][j][0]\n",
    "            j += 1\n",
    "\n",
    "\n",
    "test_device_address_list = []\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    test_device_address_list.append(device_name_mapping[train[0]['Source Zigbee']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Encode the device address\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(device_address_list)\n",
    "encoded_Y = encoder.transform(device_address_list)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrix_for_binn, encoded_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted') # weighted average gives higher f1 score (0.84), macro gives 0.56\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Instantiate ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "# Load the best model\n",
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and testing on different dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Encode the device address\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(device_address_list)\n",
    "encoded_Y = encoder.transform(device_address_list)\n",
    "encoded_Y_test = encoder.transform(test_device_address_list)\n",
    "\n",
    "# Select features to use\n",
    "features_to_use = [0, 1, 2, 3, 4]\n",
    "num_of_features = len(features_to_use)\n",
    "\n",
    "X_train = matrix_for_binn[:, :, features_to_use]\n",
    "y_train = encoded_Y\n",
    "X_test = test_matrix[:, :, features_to_use]\n",
    "y_test = encoded_Y_test\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "sampling = True\n",
    "\n",
    "if sampling:\n",
    "    # Convert device_address_list to numpy array for easy indexing\n",
    "    labels = np.array(encoded_Y)\n",
    "\n",
    "    # Identify the unique class labels\n",
    "    unique_classes = np.unique(labels)\n",
    "\n",
    "    # Initialize empty lists to store the balanced dataset\n",
    "    balanced_features = []\n",
    "    balanced_labels = []\n",
    "\n",
    "    # Set the desired number of samples per class\n",
    "    samples_per_class = 200\n",
    "\n",
    "    # Iterate over each class and perform class-wise sampling\n",
    "    for class_label in unique_classes:\n",
    "        # Get indices of instances belonging to the current class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "        # Check if the class has enough samples for sampling\n",
    "        if len(class_indices) >= samples_per_class:\n",
    "            # Sample a fixed number of instances from the current class\n",
    "            sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "            # Append the sampled instances to the balanced dataset\n",
    "            balanced_features.extend(matrix_for_binn[sampled_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[sampled_indices])\n",
    "        else:\n",
    "            # If the class has fewer than 50 samples, include all of them\n",
    "            balanced_features.extend(matrix_for_binn[class_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "    # Convert the balanced dataset to numpy arrays\n",
    "    balanced_features = np.array(balanced_features)\n",
    "    balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "    X_train = balanced_features\n",
    "    y_train = balanced_labels\n",
    "    number_of_epochs = 50\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "#model.add(Bidirectional(LSTM(16)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Instantiate ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=number_of_epochs, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "# Load the best model\n",
    "print('Loading best model...')\n",
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "consensus = True\n",
    "occurrence_threshold = 35\n",
    "if consensus:\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "\n",
    "decoded_y_test = encoder.inverse_transform(y_test)\n",
    "decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "classes = sorted(set(test_device_address_list))\n",
    "\n",
    "print(classification_report(decoded_y_test, decoded_y_pred_classes, target_names=classes))\n",
    "\n",
    "cm = confusion_matrix(decoded_y_test, decoded_y_pred_classes, normalize='true', labels=classes)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#plt.title('Confusion Matrix first 4 features with sampling')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, f\"{cm[i, j]:.2f}\", ha='center', va='center', color='white' if cm[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"identification_consensus_v2.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Function to calculate F1 score for given occurrence_threshold\n",
    "def calculate_f1_score(occurrence_threshold):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "    # Inverse transform labels\n",
    "    decoded_y_test = encoder.inverse_transform(y_test)\n",
    "    decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(decoded_y_test, decoded_y_pred_classes, average='weighted')\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Vary occurrence_threshold values\n",
    "occurrence_thresholds = range(1, 40)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in occurrence_thresholds:\n",
    "    f1 = calculate_f1_score(threshold)\n",
    "    f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = False\n",
    "if smooth:\n",
    "    f1_interp = interp1d(occurrence_thresholds, f1_scores, kind='cubic')\n",
    "    new_thresholds = np.linspace(min(occurrence_thresholds), max(occurrence_thresholds), 100)\n",
    "    smooth_f1_scores = f1_interp(new_thresholds)\n",
    "    plt.plot(new_thresholds, smooth_f1_scores, marker='', linestyle='-')\n",
    "else:\n",
    "    plt.plot(occurrence_thresholds, f1_scores, marker='.', linestyle='-')\n",
    "\n",
    "plt.xlabel('Occurrence Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Effect of Occurrence Threshold on F1 Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"asyntotic_consensus_cl.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(encoder.inverse_transform(encoded_Y), return_counts=True)\n",
    "value_counts = dict(zip(unique, counts))\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming 'labels' is your array, DataFrame, or list of class labels\n",
    "if isinstance(device_address_list, (np.ndarray, pd.Series)):\n",
    "    class_distribution = np.unique(device_address_list, return_counts=True)\n",
    "else:\n",
    "    class_distribution = Counter(device_address_list)\n",
    "\n",
    "plt.bar(class_distribution.keys(), class_distribution.values())\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional Neural Network with balanced dataset\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert device_address_list to numpy array for easy indexing\n",
    "labels = np.array(encoded_Y)\n",
    "\n",
    "# Identify the unique class labels\n",
    "unique_classes = np.unique(labels)\n",
    "\n",
    "# Initialize empty lists to store the balanced dataset\n",
    "balanced_features = []\n",
    "balanced_labels = []\n",
    "\n",
    "# Set the desired number of samples per class\n",
    "samples_per_class = 50\n",
    "\n",
    "# Iterate over each class and perform class-wise sampling\n",
    "for class_label in unique_classes:\n",
    "    # Get indices of instances belonging to the current class\n",
    "    class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "    # Check if the class has enough samples for sampling\n",
    "    if len(class_indices) >= samples_per_class:\n",
    "        # Sample a fixed number of instances from the current class\n",
    "        sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "        # Append the sampled instances to the balanced dataset\n",
    "        balanced_features.extend(matrix_for_binn[sampled_indices])\n",
    "        balanced_labels.extend(labels[sampled_indices])\n",
    "    else:\n",
    "        # If the class has fewer than 50 samples, include all of them\n",
    "        balanced_features.extend(matrix_for_binn[class_indices])\n",
    "        balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "# Convert the balanced dataset to numpy arrays\n",
    "balanced_features = np.array(balanced_features)\n",
    "balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "# Split the balanced dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_features, balanced_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_Y)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[f1_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single DataFrame with all the packets and 'id' column to identify the train of packets\n",
    "\n",
    "all_packets = list()\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    for packet in train:\n",
    "        if packet is not None:\n",
    "            all_packets.append({\n",
    "                    'id': i,\n",
    "                    'Time': packet['Time'],\n",
    "                    'Delta Time': packet['Delta Time'],\n",
    "                    'Length': packet['Length'],\n",
    "                    'Sequence Number': packet['Sequence Number'],\n",
    "                    'Payload Length': packet['Payload Length'] if not np.isnan(packet['Payload Length']) else 0,\n",
    "                    'FCF Zigbee': int(packet['FCF Zigbee'], 0),\n",
    "                    'FCF IEEE': int(packet['FCF IEEE'], 0),\n",
    "                    'Action': packet['Action']})\n",
    "\n",
    "coordinator_result_df = pd.DataFrame(all_packets)\n",
    "\n",
    "# Set Delta Time of the first packet of each train to 0\n",
    "id_changes = coordinator_result_df['id'] != coordinator_result_df['id'].shift(1)\n",
    "coordinator_result_df.loc[id_changes, 'Delta Time'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tsfresh used to get features from trains of packets\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters, ComprehensiveFCParameters\n",
    "\n",
    "coordinator_result_df['Time'] = pd.to_datetime(coordinator_result_df['Time'], format=\"%b %d, %Y %H:%M:%S.%f\")\n",
    "\n",
    "# Remove the 'Action' column for the feature extraction\n",
    "column_to_exclude = 'Action'\n",
    "excluded_column = coordinator_result_df.pop(column_to_exclude)\n",
    "\n",
    "# Extract features from the dataframe\n",
    "extracted_features = extract_features(coordinator_result_df, column_id=\"id\", column_sort=\"Time\", default_fc_parameters=MinimalFCParameters())\n",
    "\n",
    "# Impute the extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Reinsert the 'Action' column\n",
    "coordinator_result_df = pd.concat([coordinator_result_df, excluded_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the device address to the dataframe of features to be able to classify the packets\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    extracted_features.loc[i, 'Source Zigbee'] = device_name_mapping[coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']]\n",
    "    # TODO insert action based on relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    if len(source_action_list) > 1:\n",
    "        print(f\"{i}) {device_name_mapping[source]}: {source_action_list} -> {highest_priority_action}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '0x3d95'\n",
    "source_action_list = ['Report Attributes (0x0a)', 'APS: Ack']\n",
    "print(device_name_mapping[source])\n",
    "\n",
    "highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "print(highest_priority_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "print('Features Importance')\n",
    "print(clf.feature_importances_)\n",
    "print('')\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro') \n",
    "class_prob = clf.predict_proba(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# Set the occurrence threshold for label-wise consensus\n",
    "occurrence_threshold = 5\n",
    "occurrences_used = False\n",
    "\n",
    "# Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "for label in np.unique(y_test):\n",
    "    indices = np.where(y_test == label)[0]\n",
    "\n",
    "    # Check if the label has occurred at least occurrence_threshold times\n",
    "    if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "        \n",
    "        # Identify the most common prediction within each occurrence_threshold group\n",
    "        for i in range(0, len(indices), occurrence_threshold):\n",
    "            group_indices = indices[i:i+occurrence_threshold]\n",
    "            most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "            most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[group_indices] = most_frequent_label\n",
    "        \n",
    "    else:\n",
    "        most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "        most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "        # Replace all predictions for this label within the group with the most common value\n",
    "        y_pred[indices] = most_frequent_label\n",
    "\n",
    "\n",
    "# Calculate and store metrics for each fold after label-wise consensus\n",
    "accuracy_consensus = accuracy_score(y_test, y_pred)\n",
    "precision_consensus = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall_consensus = recall_score(y_test, y_pred, average='macro')\n",
    "f1_consensus = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('')\n",
    "print(f\"Accuracy: {accuracy_consensus}\")\n",
    "print(f\"Precision: {precision_consensus}\")\n",
    "print(f\"Recall: {recall_consensus}\")\n",
    "print(f\"F1: {f1_consensus}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "unique_mapped_labels = sorted(set(device_name_mapping.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_mapped_labels)\n",
    "disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier with 10-fold cross validation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "accuracy_scores_consensus = []\n",
    "precision_scores_consensus = []\n",
    "recall_scores_consensus = []\n",
    "f1_scores_consensus = []\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for train_index, test_index in stratified_kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Train the RandomForestClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # Set the occurrence threshold for label-wise consensus\n",
    "    occurrence_threshold = 10\n",
    "    occurrences_used = True\n",
    "\n",
    "    # Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "            \n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "                most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred[group_indices] = most_frequent_label\n",
    "        else:\n",
    "            most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "            most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[indices] = most_frequent_label\n",
    "\n",
    "    # Calculate and store metrics for each fold after label-wise consensus\n",
    "    accuracy_scores_consensus.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores_consensus.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "    recall_scores_consensus.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores_consensus.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"Average Metrics Across Folds:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores) / len(accuracy_scores)}\")\n",
    "print(f\"Precision: {sum(precision_scores) / len(precision_scores)}\")\n",
    "print(f\"Recall: {sum(recall_scores) / len(recall_scores)}\")\n",
    "print(f\"F1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Average Metrics Across Folds after Label-wise Consensus:\")\n",
    "print(f\"Accuracy: {sum(accuracy_scores_consensus) / len(accuracy_scores_consensus)}\")\n",
    "print(f\"Precision: {sum(precision_scores_consensus) / len(precision_scores_consensus)}\")\n",
    "print(f\"Recall: {sum(recall_scores_consensus) / len(recall_scores_consensus)}\")\n",
    "print(f\"F1: {sum(f1_scores_consensus) / len(f1_scores_consensus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score before and after label-wise consensus for different occurrence thresholds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = extracted_features.drop(columns=['Source Zigbee'])\n",
    "y = extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create StratifiedKFold object with 10 folds\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store F1 scores before and after consensus for different occurrence thresholds\n",
    "f1_scores_before_consensus = []\n",
    "f1_scores_after_consensus = []\n",
    "\n",
    "# Define a range of occurrence thresholds from 1 to 20\n",
    "occurrence_threshold_range = range(1, 21)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for occurrence_threshold in occurrence_threshold_range:\n",
    "    # Lists to store F1 scores for each fold\n",
    "    f1_scores = []\n",
    "    f1_scores_consensus = []\n",
    "\n",
    "    for train_index, test_index in stratified_kfold.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the RandomForestClassifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate F1 score before consensus\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "        # Perform label-wise consensus\n",
    "        for label in np.unique(y_test):\n",
    "            indices = np.where(y_test == label)[0]\n",
    "\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "                most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred[group_indices] = most_frequent_label\n",
    "\n",
    "        # Calculate F1 score after consensus\n",
    "        f1_scores_consensus.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # Store average F1 scores for the current occurrence threshold\n",
    "    f1_scores_before_consensus.append(sum(f1_scores) / len(f1_scores))\n",
    "    f1_scores_after_consensus.append(sum(f1_scores_consensus) / len(f1_scores_consensus))\n",
    "\n",
    "# Plot the F1 scores before and after consensus\n",
    "plt.plot(occurrence_threshold_range, f1_scores_before_consensus, label='Before Consensus')\n",
    "plt.plot(occurrence_threshold_range, f1_scores_after_consensus, label='After Consensus')\n",
    "plt.xlabel('Occurrence Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score Before and After Label-wise Consensus')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to every row of extracted_features the number of packets that fall inside the windows of threshold / 10\n",
    "\n",
    "N = 10\n",
    "for i in range(len(extracted_features)):\n",
    "    mu = extracted_features.loc[i, 'Delta Time__mean']\n",
    "    sigma = extracted_features.loc[i, 'Delta Time__standard_deviation']\n",
    "    deltas = coordinator_result_df[coordinator_result_df['id'] == i]['Delta Time']\n",
    "    for n in range(N):\n",
    "        lower_bound = n * (threshold_float / N)\n",
    "        upper_bound = (n + 1) * (threshold_float / N)\n",
    "        count = ((deltas >= lower_bound) & \n",
    "                 (deltas < upper_bound)).sum()\n",
    "\n",
    "        # Add the count to the extracted features\n",
    "        match n:\n",
    "            case 0:\n",
    "                extracted_features.loc[i, 'Window 1'] = count\n",
    "            case 1:\n",
    "                extracted_features.loc[i, 'Window 2'] = count\n",
    "            case 2:\n",
    "                extracted_features.loc[i, 'Window 3'] = count\n",
    "            case 3:\n",
    "                extracted_features.loc[i, 'Window 4'] = count\n",
    "            case 4:\n",
    "                extracted_features.loc[i, 'Window 5'] = count\n",
    "            case 5:\n",
    "                extracted_features.loc[i, 'Window 6'] = count\n",
    "            case 6:\n",
    "                extracted_features.loc[i, 'Window 7'] = count\n",
    "            case 7:\n",
    "                extracted_features.loc[i, 'Window 8'] = count\n",
    "            case 8:\n",
    "                extracted_features.loc[i, 'Window 9'] = count\n",
    "            case 9:\n",
    "                extracted_features.loc[i, 'Window 10'] = count\n",
    "            case _:\n",
    "                print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering on window counts\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "window_features = extracted_features.drop(columns=['Source Zigbee'])\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=16).fit(window_features)\n",
    "clustering.labels_\n",
    "\n",
    "# Adding the cluster labels to the extracted features\n",
    "extracted_features['Cluster'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Window 1', y='Window 2', hue='Cluster', data=extracted_features, palette='viridis', legend='full')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Window 1')\n",
    "plt.ylabel('Window 2')\n",
    "plt.title('Hierarchical Clustering with Source Zigbee Information')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend(title='Cluster')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pairwise relationships between the features\n",
    "features_to_plot = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                       'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10', 'Cluster']]\n",
    "\n",
    "# Create a pairplot\n",
    "sns.pairplot(features_to_plot, hue='Cluster', palette='viridis')\n",
    "plt.suptitle('Pairplot of Hierarchical Clustering with Source Zigbee Information', y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between windows and average of windows\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "window_features = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                      'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10']]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(window_features)\n",
    "\n",
    "# Calculate the average cosine similarity for each window\n",
    "average_cosine_similarity = np.mean(cosine_sim, axis=1)\n",
    "\n",
    "# Add the average cosine similarity as a new feature\n",
    "extracted_features['Average Cosine Similarity'] = average_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between windows and array of 1s\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Select features for cosine similarity calculation\n",
    "window_features = extracted_features[['Window 1', 'Window 2', 'Window 3', 'Window 4', 'Window 5',\n",
    "                                      'Window 6', 'Window 7', 'Window 8', 'Window 9', 'Window 10']]\n",
    "\n",
    "# Create a vector full of 1s\n",
    "vector_of_ones = np.ones((1, window_features.shape[1]))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim_with_ones = cosine_similarity(window_features, vector_of_ones)\n",
    "\n",
    "# Add the cosine similarity with diagonal as a new feature\n",
    "extracted_features['Cosine Similarity With Diagonal'] = cosine_sim_with_ones[:, 0]  # Assuming you want to use the first column of the cosine similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "sorted_feature_importances = sorted(zip(X.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Range of top features to explore (e.g., from 1 to the total number of features)\n",
    "num_features_to_explore = range(1, len(sorted_feature_importances) + 1)\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {'num_features': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "# Perform k-fold cross-validation for each number of top features\n",
    "for num_features in num_features_to_explore:\n",
    "    print(f\"Trying with {num_features} top features...\")\n",
    "\n",
    "    # Choose the top N features\n",
    "    top_n_features = [feature for feature, importance in sorted_feature_importances[:num_features]]\n",
    "\n",
    "    # Subset the data with the top features\n",
    "    X_selected = X[top_n_features]\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    y_pred = cross_val_predict(clf, X_selected, y, cv=stratified_kfold)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y, y_pred, average='macro')\n",
    "    f1 = f1_score(y, y_pred, average='macro')\n",
    "\n",
    "    # Store results\n",
    "    results['num_features'].append(num_features)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['precision'].append(precision)\n",
    "    results['recall'].append(recall)\n",
    "    results['f1'].append(f1)\n",
    "    \n",
    "\n",
    "# Print results\n",
    "for i in range(len(results['num_features'])):\n",
    "    print(f\"Num Features: {results['num_features'][i]}, Accuracy: {results['accuracy'][i]}, Precision: {results['precision'][i]}, Recall: {results['recall'][i]}, F1: {results['f1'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(results['num_features'], results['accuracy'], label='Accuracy', marker='.')\n",
    "\n",
    "# Plot precision\n",
    "plt.plot(results['num_features'], results['precision'], label='Precision', marker='.')\n",
    "\n",
    "# Plot recall\n",
    "plt.plot(results['num_features'], results['recall'], label='Recall', marker='.')\n",
    "\n",
    "# Plot F1 score\n",
    "plt.plot(results['num_features'], results['f1'], label='F1 Score', marker='.')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Number of Top Features')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Metrics vs. Number of Top Features')\n",
    "plt.legend()\n",
    "\n",
    "# Find the index where each metric is maximized\n",
    "idx_max_accuracy = results['accuracy'].index(max(results['accuracy']))\n",
    "idx_max_precision = results['precision'].index(max(results['precision']))\n",
    "idx_max_recall = results['recall'].index(max(results['recall']))\n",
    "idx_max_f1 = results['f1'].index(max(results['f1']))\n",
    "\n",
    "# Print the number of features and value where each metric is maximized\n",
    "print(f\"Max Accuracy at {results['num_features'][idx_max_accuracy]} features with value {max(results['accuracy'])}\")\n",
    "print(f\"Max Precision at {results['num_features'][idx_max_precision]} features with value {max(results['precision'])}\")\n",
    "print(f\"Max Recall at {results['num_features'][idx_max_recall]} features with value {max(results['recall'])}\")\n",
    "print(f\"Max F1 Score at {results['num_features'][idx_max_f1]} features with value {max(results['f1'])}\")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by importance in descending order\n",
    "sorted_feature_importances = sorted(zip(X.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the names of the top features and their importance scores\n",
    "for feature, importance in sorted_feature_importances:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- ACTION CLASSIFICATION -------------------------------------------------- ###\n",
    "\n",
    "df = df.merge(ground_truth_df[['Action']], left_index=True, right_index=True, how='left')\n",
    "grouped = df.groupby('Source Zigbee')['Action'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_counts = df.groupby('Source Zigbee')['Action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in grouped.index:\n",
    "    if device in devices:\n",
    "        first_value = True\n",
    "        for value in grouped[device]:\n",
    "            if first_value:\n",
    "                print(f\"{device}: {value}\")\n",
    "                first_value = False\n",
    "            else:\n",
    "                print(f\"\\t{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of actions for a certain class and select the rows of the train matrix that correspond to the class\n",
    "\n",
    "# List to keep track of the device class of each train source\n",
    "device_address_list = []\n",
    "class_considered = 'Motion'\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    device_address_list.append(device_type_mapping[train[0]['Source Zigbee']])\n",
    "\n",
    "device_address_series = pd.Series(device_address_list)\n",
    "mask = (device_address_series == class_considered)\n",
    "\n",
    "mask_index = [int(i) for i in mask.index[mask]]\n",
    "\n",
    "action_list = []\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    action_list.append(highest_priority_action)\n",
    "\n",
    "filtered_action_list = [action_list[i] for i in mask.index[mask]]\n",
    "filtered_device_list = [device_address_list[i] for i in mask.index[mask]]\n",
    "\n",
    "matrix_action = matrix_for_binn[mask_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_action_series = pd.Series(filtered_action_list)\n",
    "\n",
    "# Use value_counts() on the Series\n",
    "value_counts_result = filtered_action_series.value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(value_counts_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of actions for a certain class and select the rows of the test matrix that correspond to the class\n",
    "\n",
    "# List to keep track of the device class of each train source\n",
    "test_device_address_list = []\n",
    "\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    train = coordinator_final_df.loc[i]\n",
    "    test_device_address_list.append(device_type_mapping[train[0]['Source Zigbee']])\n",
    "\n",
    "test_device_address_series = pd.Series(test_device_address_list)\n",
    "mask = (test_device_address_series == class_considered)\n",
    "\n",
    "mask_index = [int(i) for i in mask.index[mask]]\n",
    "\n",
    "test_action_list = []\n",
    "for i in range(len(coordinator_final_df)):\n",
    "    source_action_list = []\n",
    "    source = coordinator_final_df.loc[i].loc[0].loc['Source Zigbee']\n",
    "    for packet in coordinator_final_df.loc[i]:\n",
    "        if packet is not None and packet.loc['Source Zigbee'] == source:\n",
    "            if packet.loc['Action'] not in source_action_list:\n",
    "                source_action_list.append(packet.loc['Action'])\n",
    "\n",
    "    # Get the action with highest priority within the actions performed by the source\n",
    "    highest_priority_action = get_highest_priority_action(device_name_mapping[source], source_action_list)\n",
    "    \n",
    "    test_action_list.append(highest_priority_action)\n",
    "\n",
    "test_filtered_action_list = [test_action_list[i] for i in mask.index[mask]]\n",
    "test_filtered_device_list = [test_device_address_list[i] for i in mask.index[mask]]\n",
    "\n",
    "test_matrix_action = test_matrix[mask_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_action_series = pd.Series(test_filtered_action_list)\n",
    "\n",
    "# Use value_counts() on the Series\n",
    "value_counts_result = filtered_action_series.value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(value_counts_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and testing on different dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Encode the device address\n",
    "encoder = LabelEncoder()\n",
    "all_action_list = np.concatenate([filtered_action_list, test_filtered_action_list])\n",
    "encoder.fit(all_action_list)\n",
    "encoded_actions = encoder.transform(all_action_list)\n",
    "encoded_Y = encoder.transform(filtered_action_list)\n",
    "encoded_Y_test = encoder.transform(test_filtered_action_list)\n",
    "\n",
    "# Select features to use\n",
    "features_to_use = [0, 1, 2, 3]\n",
    "num_of_features = len(features_to_use)\n",
    "\n",
    "X_train = matrix_action[:, :, features_to_use]\n",
    "y_train = encoded_Y\n",
    "X_test = test_matrix_action[:, :, features_to_use]\n",
    "y_test = encoded_Y_test\n",
    "\n",
    "number_of_epochs = 30\n",
    "\n",
    "sampling = False\n",
    "\n",
    "if sampling:\n",
    "    # Convert device_address_list to numpy array for easy indexing\n",
    "    labels = np.array(encoded_Y)\n",
    "\n",
    "    # Identify the unique class labels\n",
    "    unique_classes = np.unique(labels)\n",
    "\n",
    "    # Initialize empty lists to store the balanced dataset\n",
    "    balanced_features = []\n",
    "    balanced_labels = []\n",
    "\n",
    "    # Set the desired number of samples per class\n",
    "    samples_per_class = 200\n",
    "\n",
    "    # Iterate over each class and perform class-wise sampling\n",
    "    for class_label in unique_classes:\n",
    "        # Get indices of instances belonging to the current class\n",
    "        class_indices = np.where(labels == class_label)[0]\n",
    "\n",
    "        # Check if the class has enough samples for sampling\n",
    "        if len(class_indices) >= samples_per_class:\n",
    "            # Sample a fixed number of instances from the current class\n",
    "            sampled_indices = resample(class_indices, n_samples=samples_per_class, replace=False, random_state=42)\n",
    "\n",
    "            # Append the sampled instances to the balanced dataset\n",
    "            balanced_features.extend(matrix_for_binn[sampled_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[sampled_indices])\n",
    "        else:\n",
    "            # If the class has fewer than 50 samples, include all of them\n",
    "            balanced_features.extend(matrix_for_binn[class_indices][:, :, features_to_use])\n",
    "            balanced_labels.extend(labels[class_indices])\n",
    "\n",
    "    # Convert the balanced dataset to numpy arrays\n",
    "    balanced_features = np.array(balanced_features)\n",
    "    balanced_labels = np.array(balanced_labels)\n",
    "\n",
    "    X_train = balanced_features\n",
    "    y_train = balanced_labels\n",
    "    number_of_epochs = 50\n",
    "\n",
    "# Custom F1ScoreCallback\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(Callback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is not None:\n",
    "            x_val, y_val = self.validation_data\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "            print(f'F1 Score: {f1}')\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(rounded_mean, num_of_features)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "#model.add(Bidirectional(LSTM(16)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(set(encoded_actions)), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Instantiate F1ScoreCallback\n",
    "f1_callback = F1ScoreCallback(validation_data=(X_test, y_test))\n",
    "\n",
    "# Instantiate ModelCheckpoint callback\n",
    "checkpoint_filepath = 'best_model.keras'\n",
    "model_checkpoint_callback = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=number_of_epochs, validation_data=(X_test, y_test), callbacks=[f1_callback, model_checkpoint_callback])\n",
    "\n",
    "# Load the best model\n",
    "print('Loading best model...')\n",
    "model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "consensus = False\n",
    "occurrence_threshold = 35\n",
    "if consensus:\n",
    "    # Apply label-wise consensus mechanism\n",
    "    for label in np.unique(y_test):\n",
    "        indices = np.where(y_test == label)[0]\n",
    "\n",
    "        # Check if the label has occurred at least occurrence_threshold times\n",
    "        if len(indices) >= occurrence_threshold:\n",
    "            # Identify the most common prediction within each occurrence_threshold group\n",
    "            for i in range(0, len(indices), occurrence_threshold):\n",
    "                group_indices = indices[i:i+occurrence_threshold]\n",
    "                most_frequent_label = np.argmax(np.bincount(y_pred_classes[group_indices]))\n",
    "\n",
    "                # Replace all predictions for this label within the group with the most common value\n",
    "                y_pred_classes[group_indices] = most_frequent_label\n",
    "\n",
    "decoded_y_test = encoder.inverse_transform(y_test)\n",
    "decoded_y_pred_classes = encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "classes = sorted(set(test_filtered_action_list))\n",
    "\n",
    "print(classification_report(decoded_y_test, decoded_y_pred_classes, target_names=classes))\n",
    "\n",
    "cm = confusion_matrix(decoded_y_test, decoded_y_pred_classes, normalize='true', labels=classes)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#plt.title('Confusion Matrix first 4 features with sampling')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, f\"{cm[i, j]:.2f}\", ha='center', va='center', color='white' if cm[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.show()\n",
    "#save_tikzplotlib(plt.gcf(), \"events_bulb.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------------- TESTING ON DIFFERENT DATASET -------------------------------------------------- ###\n",
    "\n",
    "training_extracted_features = extracted_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X_train = training_extracted_features.drop(columns=['Source Zigbee'])\n",
    "y_train = training_extracted_features['Source Zigbee']\n",
    "\n",
    "X_test = test_extracted_features.drop(columns=['Source Zigbee'])\n",
    "y_test = test_extracted_features['Source Zigbee']\n",
    "\n",
    "# Instantiate the RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the RandomForestClassifier on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "# Set the occurrence threshold for label-wise consensus\n",
    "occurrence_threshold = 5\n",
    "occurrences_used = True\n",
    "\n",
    "# Identify the most common prediction for each label after every occurrence_threshold occurrences\n",
    "for label in np.unique(y_test):\n",
    "    indices = np.where(y_test == label)[0]\n",
    "\n",
    "    # Check if the label has occurred at least occurrence_threshold times\n",
    "    if occurrences_used and len(indices) >= occurrence_threshold:\n",
    "        \n",
    "        # Identify the most common prediction within each occurrence_threshold group\n",
    "        for i in range(0, len(indices), occurrence_threshold):\n",
    "            group_indices = indices[i:i+occurrence_threshold]\n",
    "            most_frequent_label, _ = np.unique(y_pred[group_indices], return_counts=True)\n",
    "            most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "            # Replace all predictions for this label within the group with the most common value\n",
    "            y_pred[group_indices] = most_frequent_label\n",
    "    else:\n",
    "        most_frequent_label, _ = np.unique(y_pred[indices], return_counts=True)\n",
    "        most_frequent_label = most_frequent_label[np.argmax(_)]\n",
    "\n",
    "        # Replace all predictions for this label within the group with the most common value\n",
    "        y_pred[indices] = most_frequent_label\n",
    "\n",
    "\n",
    "# Calculate and store metrics for each fold after label-wise consensus\n",
    "accuracy_consensus = accuracy_score(y_test, y_pred)\n",
    "precision_consensus = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "recall_consensus = recall_score(y_test, y_pred, average='macro')\n",
    "f1_consensus = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('')\n",
    "print(f\"Accuracy: {accuracy_consensus}\")\n",
    "print(f\"Precision: {precision_consensus}\")\n",
    "print(f\"Recall: {recall_consensus}\")\n",
    "print(f\"F1: {f1_consensus}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "unique_mapped_labels = sorted(set(device_name_mapping.values()))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_mapped_labels)\n",
    "disp.plot(ax=plt.gca(), xticks_rotation='vertical', cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
